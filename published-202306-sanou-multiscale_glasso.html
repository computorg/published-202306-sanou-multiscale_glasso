<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Edmond Sanou">
<meta name="author" content="Christophe Ambroise">
<meta name="author" content="Geneviève Robin">
<meta name="dcterms.date" content="2023-06-28">
<meta name="keywords" content="Neighborhood selection, Convex hierarchical clustering, Gaussian graphical models">

<title>Inference of Multiscale Gaussian Graphical Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="published-202306-sanou-multiscale_glasso_files/libs/clipboard/clipboard.min.js"></script>
<script src="published-202306-sanou-multiscale_glasso_files/libs/quarto-html/quarto.js"></script>
<script src="published-202306-sanou-multiscale_glasso_files/libs/quarto-html/popper.min.js"></script>
<script src="published-202306-sanou-multiscale_glasso_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="published-202306-sanou-multiscale_glasso_files/libs/quarto-html/anchor.min.js"></script>
<link href="published-202306-sanou-multiscale_glasso_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="published-202306-sanou-multiscale_glasso_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="published-202306-sanou-multiscale_glasso_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="published-202306-sanou-multiscale_glasso_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="published-202306-sanou-multiscale_glasso_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="published-202306-sanou-multiscale_glasso_files/libs/quarto-contrib/pseudocode-2.4/pseudocode.min.js"></script>
<link href="published-202306-sanou-multiscale_glasso_files/libs/quarto-contrib/pseudocode-2.4/pseudocode.min.css" rel="stylesheet">
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #FFFFFF;
      }

      .quarto-title-block .quarto-title-banner {
        color: #FFFFFF;
background: #034E79;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><a href="https://computo.sfds.asso.fr">
        <img src="https://computo.sfds.asso.fr/assets/img/logo_notext_white.png" height="60px">
      </a> &nbsp; Inference of Multiscale Gaussian Graphical Models</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> source</button></div></div>
            <p><a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons BY License"></a>
ISSN 2824-7795</p>
                </div>
  </div>
    
    <div class="quarto-title-meta-author">
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-heading">Affiliation</div>
          
          <div class="quarto-title-meta-contents">
        <a href="https://desanou.github.io/">Edmond Sanou</a> 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="http://www.math-evry.cnrs.fr/">
                  Université Paris-Saclay, CNRS, Univ Evry, Laboratoire de Mathématiques et Modélisation d’Evry
                  </a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://cambroise.github.io/">Christophe Ambroise</a> 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="http://www.math-evry.cnrs.fr/">
                  Université Paris-Saclay, CNRS, Univ Evry, Laboratoire de Mathématiques et Modélisation d’Evry
                  </a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://genevieverobin.wordpress.com/">Geneviève Robin</a> 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="http://www.math-evry.cnrs.fr/">
                  Université Paris-Saclay, CNRS, Univ Evry, Laboratoire de Mathématiques et Modélisation d’Evry
                  </a>
                </p>
            </div>
        </div>
                    
  <div class="quarto-title-meta">
                                
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 28, 2023</p>
      </div>
    </div>
                                    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">July 11, 2023</p>
      </div>
    </div>
      
                  
      <div>
      <div class="quarto-title-meta-heading">Keywords</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Neighborhood selection, Convex hierarchical clustering, Gaussian graphical models</p>
      </div>
    </div>
    
    <div>
      <div class="quarto-title-meta-heading">Status</div>
      <div class="quarto-title-meta-contents">
              <a href="https://github.com/computorg/published-202306-sanou-multiscale_glasso"><img src="https://github.com/computorg/published-202306-sanou-multiscale_glasso/actions/workflows/build.yml/badge.svg" alt="build status"></a>
                    <p class="date"></p>
        <a href="https://github.com/computorg/published-202306-sanou-multiscale_glasso/issues?q=is%3Aopen+is%3Aissue+label%3Areview"><img src="https://img.shields.io/badge/reviews-reports-blue" alt="reviews"></a>
            </div>
    </div>

  </div>
                                                
  <div>
    <div class="abstract">
    <div class="abstract-title">Abstract</div>
      <p>Gaussian Graphical Models (GGMs) are widely used in high-dimensional data analysis to synthesize the interaction between variables. In many applications, such as genomics or image analysis, graphical models rely on sparsity and clustering to reduce dimensionality and improve performances. This paper explores a slightly different paradigm where clustering is not knowledge-driven but performed simultaneously with the graph inference task. We introduce a novel Multiscale Graphical Lasso (MGLasso) to improve networks interpretability by proposing graphs at different granularity levels. The method estimates clusters through a convex clustering approach — a relaxation of <span class="math inline">k</span>-means, and hierarchical clustering. The conditional independence graph is simultaneously inferred through a neighborhood selection scheme for undirected graphical models. MGLasso extends and generalizes the sparse group fused lasso problem to undirected graphical models. We use continuation with Nesterov smoothing in a shrinkage-thresholding algorithm (CONESTA) to propose a regularization path of solutions along the group fused Lasso penalty, while the Lasso penalty is kept constant. Extensive experiments on synthetic data compare the performances of our model to state-of-the-art clustering methods and network inference models. Applications to gut microbiome data and poplar’s methylation mixed with transcriptomic data are presented.</p>
    </div>
  </div>

  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#multiscale-graphical-lasso" id="toc-multiscale-graphical-lasso" class="nav-link" data-scroll-target="#multiscale-graphical-lasso"><span class="header-section-number">2</span> Multiscale Graphical Lasso</a></li>
  <li><a href="#numerical-scheme" id="toc-numerical-scheme" class="nav-link" data-scroll-target="#numerical-scheme"><span class="header-section-number">3</span> Numerical scheme</a>
  <ul class="collapse">
  <li><a href="#optimization-via-conesta-algorithm" id="toc-optimization-via-conesta-algorithm" class="nav-link" data-scroll-target="#optimization-via-conesta-algorithm"><span class="header-section-number">3.1</span> Optimization via CONESTA algorithm</a></li>
  <li><a href="#reformulation-of-mglasso-for-conesta-algorithm" id="toc-reformulation-of-mglasso-for-conesta-algorithm" class="nav-link" data-scroll-target="#reformulation-of-mglasso-for-conesta-algorithm"><span class="header-section-number">3.2</span> Reformulation of MGLasso for CONESTA algorithm</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection"><span class="header-section-number">3.3</span> Model selection</a></li>
  </ul></li>
  <li><a href="#simulation-experiments" id="toc-simulation-experiments" class="nav-link" data-scroll-target="#simulation-experiments"><span class="header-section-number">4</span> Simulation experiments</a>
  <ul class="collapse">
  <li><a href="#synthetic-data-models" id="toc-synthetic-data-models" class="nav-link" data-scroll-target="#synthetic-data-models"><span class="header-section-number">4.1</span> Synthetic data models</a>
  <ul class="collapse">
  <li><a href="#stochastic-block-model" id="toc-stochastic-block-model" class="nav-link" data-scroll-target="#stochastic-block-model"><span class="header-section-number">4.1.1</span> Stochastic Block Model</a></li>
  <li><a href="#erdös-renyi-model" id="toc-erdös-renyi-model" class="nav-link" data-scroll-target="#erdös-renyi-model"><span class="header-section-number">4.1.2</span> Erdös-Renyi Model</a></li>
  <li><a href="#scale-free-model" id="toc-scale-free-model" class="nav-link" data-scroll-target="#scale-free-model"><span class="header-section-number">4.1.3</span> Scale-free Model</a></li>
  </ul></li>
  <li><a href="#support-recovery" id="toc-support-recovery" class="nav-link" data-scroll-target="#support-recovery"><span class="header-section-number">4.2</span> Support recovery</a></li>
  <li><a href="#clustering" id="toc-clustering" class="nav-link" data-scroll-target="#clustering"><span class="header-section-number">4.3</span> Clustering</a></li>
  </ul></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications"><span class="header-section-number">5</span> Applications</a>
  <ul class="collapse">
  <li><a href="#application-to-microbial-associations-in-gut-data" id="toc-application-to-microbial-associations-in-gut-data" class="nav-link" data-scroll-target="#application-to-microbial-associations-in-gut-data"><span class="header-section-number">5.1</span> Application to microbial associations in gut data</a></li>
  <li><a href="#application-to-methylation-and-transcriptomic-genotypes-in-poplar" id="toc-application-to-methylation-and-transcriptomic-genotypes-in-poplar" class="nav-link" data-scroll-target="#application-to-methylation-and-transcriptomic-genotypes-in-poplar"><span class="header-section-number">5.2</span> Application to methylation and transcriptomic genotypes in poplar</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">6</span> Conclusion</a></li>
  
  
  
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="published-202306-sanou-multiscale_glasso.pdf"><i class="bi bi-file-pdf"></i>PDF (computo)</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Probabilistic graphical models <span class="citation" data-cites="Lauritzen1996 Koller2009">(<a href="#ref-Lauritzen1996" role="doc-biblioref">Lauritzen 1996</a>; <a href="#ref-Koller2009" role="doc-biblioref">Koller and Friedman 2009</a>)</span> are widely used in high-dimensional data analysis to synthesize the interaction between variables. In many applications, such as genomics or image analysis, graphical models reduce the number of parameters by selecting the most relevant interactions between variables. Undirected <em>Gaussian Graphical Models</em> (GGMs) are a class of graphical models used in Gaussian settings. In the context of high-dimensional statistics, graphical models are generally assumed sparse, meaning that a small number of variables interact compared to the total number of possible interactions. This assumption has been shown to provide both statistical and computational advantages by simplifying the structure of dependence between variables <span class="citation" data-cites="Dempster1972">(<a href="#ref-Dempster1972" role="doc-biblioref">Dempster 1972</a>)</span> and allowing efficient algorithms <span class="citation" data-cites="Meinshausen2006">(<a href="#ref-Meinshausen2006" role="doc-biblioref">Meinshausen and Bühlmann 2006</a>)</span>. See, for instance, <span class="citation" data-cites="Fan2016">Fan, Liao, and Liu (<a href="#ref-Fan2016" role="doc-biblioref">2016</a>)</span> for a review of sparse graphical models inference.</p>
<p>In GGMs, it is well known <span class="citation" data-cites="Lauritzen1996">(<a href="#ref-Lauritzen1996" role="doc-biblioref">Lauritzen 1996</a>)</span> that inferring the graphical model or, equivalently, the <em>conditional independence graph</em> (CIG) boils down to inferring the support of the precision matrix <span class="math inline">\mathbf{\Omega}</span> (the inverse of the variance-covariance matrix). Several <span class="math inline">\ell_1</span> penalized methods have been proposed in the literature to learn the CIG of GGMs. For instance, <em>the neighborhood selection</em> <span class="citation" data-cites="Meinshausen2006">(MB, <a href="#ref-Meinshausen2006" role="doc-biblioref">Meinshausen and Bühlmann 2006</a>)</span> based on a nodewise regression approach via the <em>least absolute shrinkage and selection operator</em> <span class="citation" data-cites="tibshirani1996">(Lasso, <a href="#ref-tibshirani1996" role="doc-biblioref">R. Tibshirani 1996</a>)</span> is a popular method. Each variable is regressed on the others, taking advantage of the link between the so-obtained regression coefficients and partial correlations. The MB method has generated a long line of work in nodewise regression methods. For instance, <span class="citation" data-cites="Rocha2008">Rocha, Zhao, and Yu (<a href="#ref-Rocha2008" role="doc-biblioref">2008</a>)</span> and <span class="citation" data-cites="Ambroise2009">Ambroise, Chiquet, and Matias (<a href="#ref-Ambroise2009" role="doc-biblioref">2009</a>)</span> showed that nodewise regression could be seen as a pseudo-likelihood approximation and <span class="citation" data-cites="Peng2009">Peng et al. (<a href="#ref-Peng2009" role="doc-biblioref">2009</a>)</span> extended the MB method to estimate sparse partial correlations using a single regression problem. Other inference methods similar to nodewise regression include a method based on the Dantzig selector <span class="citation" data-cites="Yuan2010">(<a href="#ref-Yuan2010" role="doc-biblioref">Yuan 2010</a>)</span> and the introduction of the Clime estimator <span class="citation" data-cites="Cai2011">(<a href="#ref-Cai2011" role="doc-biblioref">Cai, Liu, and Luo 2011</a>)</span>. Another family of sparse CIG inference methods directly estimates <span class="math inline">\mathbf{\Omega}</span> via direct minimization of the <span class="math inline">\ell_1</span>-penalized negative log-likelihood <span class="citation" data-cites="Banerjee2008">(<a href="#ref-Banerjee2008" role="doc-biblioref">Banerjee, El Ghaoui, and d’Aspremont 2008</a>)</span>, without resorting to the auxiliary regression problem. This method called the <em>graphical Lasso</em> <span class="citation" data-cites="Friedman2007">(GLasso, <a href="#ref-Friedman2007" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2007</a>)</span>, benefits from many optimization algorithms <span class="citation" data-cites="Yuan2007 Rothman2008 Banerjee2008 Hsieh2014">(<a href="#ref-Yuan2007" role="doc-biblioref">Yuan and Lin 2007</a>; <a href="#ref-Rothman2008" role="doc-biblioref">Rothman et al. 2008</a>; <a href="#ref-Banerjee2008" role="doc-biblioref">Banerjee, El Ghaoui, and d’Aspremont 2008</a>; <a href="#ref-Hsieh2014" role="doc-biblioref">Hsieh et al. 2014</a>)</span>.</p>
<p>Such inference methods are widely used and enjoy many favorable theoretical and empirical properties, including robustness to high-dimensional problems. However, some limitations have been observed, particularly in the presence of strongly correlated variables. Known impairments of Lasso-type regularization cause these limitations in this context <span class="citation" data-cites="Buhlmann2012 Park2007">(<a href="#ref-Buhlmann2012" role="doc-biblioref">Bühlmann et al. 2012</a>; <a href="#ref-Park2007" role="doc-biblioref">Park, Hastie, and Tibshirani 2006</a>)</span>. To overcome this, in addition to sparsity, several previous works attempt to estimate CIG by integrating clustering structures among variables for statistical sanity and interpretability. A non-exhaustive list of works that integrate a clustering structure to speed up or improve the estimation procedure includes <span class="citation" data-cites="Honorio2009">Honorio et al. (<a href="#ref-Honorio2009" role="doc-biblioref">2009</a>)</span>, <span class="citation" data-cites="Ambroise2009">Ambroise, Chiquet, and Matias (<a href="#ref-Ambroise2009" role="doc-biblioref">2009</a>)</span>, <span class="citation" data-cites="Mazumder2012">Mazumder and Hastie (<a href="#ref-Mazumder2012" role="doc-biblioref">2012</a>)</span>, <span class="citation" data-cites="Tan2013">Tan, Witten, and Shojaie (<a href="#ref-Tan2013" role="doc-biblioref">2013</a>)</span>, <span class="citation" data-cites="Devijver2018">Devijver and Gallopin (<a href="#ref-Devijver2018" role="doc-biblioref">2018</a>)</span>, <span class="citation" data-cites="Yao2019">Yao and Allen (<a href="#ref-Yao2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>The above methods exploit the group structure to simplify the graph inference problem and infer the CIG between single variables. Another question that has received less attention is the inference of the CIG between the groups of variables, i.e., between the meta-variables representative of the group structure. A recent work introducing inference of graphical models on multiple grouping levels is <span class="citation" data-cites="Cheng2017">Cheng, Shan, and Kim (<a href="#ref-Cheng2017" role="doc-biblioref">2017</a>)</span>. They proposed inferring the CIG of gene data on two levels corresponding to genes and pathways, respectively. Note that pathways are considered as groups of functionally related genes known in advance. The inference is achieved by optimizing a penalized maximum likelihood that estimates a sparse network at both gene and group levels. Our work is also part of this dynamic. We introduce a penalty term allowing parsimonious networks to be built at different clustering levels. The main difference with the procedure of <span class="citation" data-cites="Cheng2017">Cheng, Shan, and Kim (<a href="#ref-Cheng2017" role="doc-biblioref">2017</a>)</span> is that we do not require prior knowledge of the group structure, which makes the problem significantly more complex. In addition, our method has the advantage of proposing CIGs at more than two levels of granularity.</p>
<p>We introduce the Multiscale Graphical Lasso (MGLasso), a novel method to estimate simultaneously a hierarchical clustering structure and graphical models depicting the conditional independence structure between clusters of variables at each level of the hierarchy. Our approach is based on neighborhood selection <span class="citation" data-cites="Meinshausen2006">(<a href="#ref-Meinshausen2006" role="doc-biblioref">Meinshausen and Bühlmann 2006</a>)</span> and considers an additional fused-Lasso type penalty for clustering <span class="citation" data-cites="pelckmans2005convex Hocking2011 Lindsten2011">(<a href="#ref-pelckmans2005convex" role="doc-biblioref">Pelckmans et al. 2005</a>; <a href="#ref-Hocking2011" role="doc-biblioref">Hocking et al. 2011</a>; <a href="#ref-Lindsten2011" role="doc-biblioref">Lindsten, Ohlsson, and Ljung 2011</a>)</span>.</p>
<p>The use of fusion penalties in Gaussian graphical model inference is a well-studied area. Some prior works on learning sparse GGMs with a fusion penalty term have focused on penalized likelihood. Among those, a line of works <span class="citation" data-cites="danaher2014joint yang2015fused">(<a href="#ref-danaher2014joint" role="doc-biblioref">Danaher, Wang, and Witten 2014</a>; <a href="#ref-yang2015fused" role="doc-biblioref">S. Yang et al. 2015</a>)</span> infers multiple graphs across several classes while assuming the observations belong to different known clusters. Another line of research <span class="citation" data-cites="Honorio2009 Yao2019 lin2020estimation">(<a href="#ref-Honorio2009" role="doc-biblioref">Honorio et al. 2009</a>; <a href="#ref-Yao2019" role="doc-biblioref">Yao and Allen 2019</a>; <a href="#ref-lin2020estimation" role="doc-biblioref">Lin et al. 2020</a>)</span> investigates fusion penalties for enforcing local constancy in the nodes of the inferred network. Variables belonging to the same clusters are thus more likely to share the same neighborhood. These ordinary likelihood-based models are computationally challenging compared to pseudo-likelihood approximations. The unpublished manuscript of <span class="citation" data-cites="ganguly2014">Ganguly and Polonik (<a href="#ref-ganguly2014" role="doc-biblioref">2014</a>)</span> introduces a fusion-like penalty in the neighborhood selection framework. However, the problem is solved in a node-wise regression fashion where the <span class="math inline">p</span> regressions problems are not combined.</p>
<p>Fusion penalties have also been used in simple regression problems <span class="citation" data-cites="tibshirani2005sparsity">(<a href="#ref-tibshirani2005sparsity" role="doc-biblioref">Robert Tibshirani et al. 2005</a>)</span> and multivariate regression analysis (multitask learning) with multiple outcomes <span class="citation" data-cites="chen2010graph degras2021sparse dondelinger2020joint hallac2015network chu2021adaptive">(see, e.g., <a href="#ref-chen2010graph" role="doc-biblioref">Chen et al. 2010</a>; <a href="#ref-degras2021sparse" role="doc-biblioref">Degras 2021</a>; <a href="#ref-dondelinger2020joint" role="doc-biblioref">Dondelinger, Mukherjee, and Initiative 2020</a>; <a href="#ref-hallac2015network" role="doc-biblioref">Hallac, Leskovec, and Boyd 2015</a>; <a href="#ref-chu2021adaptive" role="doc-biblioref">Chu et al. 2021</a>)</span>. The defined penalties encourage fusion between predictors in simple regression, or outcomes that share similar model coefficients in multitask learning. Fusions can be formulated in a general form assuming no order on the variables as in convex clustering <span class="citation" data-cites="Hoefling2010 petry2011pairwise">(<a href="#ref-Hoefling2010" role="doc-biblioref">Hoefling 2010</a>; <a href="#ref-petry2011pairwise" role="doc-biblioref">Petry, Flexeder, and Tutz 2011</a>)</span> or assuming the availability of prior information about clusters <span class="citation" data-cites="rudin1992nonlinear hallac2015network">(<a href="#ref-rudin1992nonlinear" role="doc-biblioref">Rudin, Osher, and Fatemi 1992</a>; <a href="#ref-hallac2015network" role="doc-biblioref">Hallac, Leskovec, and Boyd 2015</a>)</span>.</p>
<p>The multitask learning framework can be extended to the learning of GGMs. <span class="citation" data-cites="chiquet2011inferring">Chiquet, Grandvalet, and Ambroise (<a href="#ref-chiquet2011inferring" role="doc-biblioref">2011</a>)</span> introduced a multitask inference for multiple graphical models when observations belong to different clusters. In MGLasso, the multitask learning framework is combined with a novel general fusion penalty to uncover clustering over variables. In the defined fusion term, we consider reordering the regression coefficients to match common predictors and symmetric coefficients. That results in enforcing the grouping property by encouraging variables belonging to the same cluster to have the same neighborhood. MGLasso exploits the multitask learning framework for GGMs inference coupled with a convex clustering problem over the nodes to infer multiscale networks and clusters simultaneously. To our knowledge, this is the first attempt in the literature of undirected GGMs. MGLasso can also be seen as an extension of sparse group fused Lasso for graphical models and be straightforwardly extended to probability distributions belonging to the exponential family <span class="citation" data-cites="yang2012graphical">(<a href="#ref-yang2012graphical" role="doc-biblioref">E. Yang et al. 2012</a>)</span>. The MGLasso algorithm is implemented in the R package <em>mglasso</em> available at <a href="https://CRAN.R-project.org/package=mglasso" class="uri">https://CRAN.R-project.org/package=mglasso</a>. The remainder of this paper is organized as follows. In Section <a href="#multiscale-graphical-lasso">2</a> and Section <a href="#numerical-scheme">3</a>, we formally introduce the Multiscale Graphical Lasso and its optimization algorithm. Section <a href="#simulation-experiments">4</a> presents simulated and real data numerical results.</p>
</section>
<section id="multiscale-graphical-lasso" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Multiscale Graphical Lasso</h1>
<p>Let <span class="math inline">\mathbf X = (X^1, \dots, X^p)^T</span> be a <span class="math inline">p</span>-dimensional Gaussian random vector, with mean vector <span class="math inline">\boldsymbol \mu \in \mathbb R^p</span> and positive definite covariance matrix <span class="math inline">\mathbf \Sigma \in \mathbb R^{p \times p}</span>. Let <span class="math inline">G = (V, E)</span> be a graph encoding the conditional independence structure of the normal distribution <span class="math inline">\mathcal N(\boldsymbol \mu, \mathbf \Sigma),</span> where <span class="math inline">V = \{1,\ldots p\}</span> is the set of vertices and <span class="math inline">E</span> the set of edges. The graph <span class="math inline">G</span> is uniquely determined by the support of the precision matrix <span class="math inline">\mathbf{\Omega} = \mathbf{\Sigma}^{-1}</span> <span class="citation" data-cites="Dempster1972">(<a href="#ref-Dempster1972" role="doc-biblioref">Dempster 1972</a>)</span>. Specifically, for any two vertices <span class="math inline">i \neq j\in V</span>, the edge <span class="math inline">(i,j)</span> belongs to the set <span class="math inline">E</span> if and only if <span class="math inline">\Omega_{ij} \neq 0.</span> On the contrary, if <span class="math inline">\Omega_{ij} = 0</span>, the variables <span class="math inline">X^i</span> and <span class="math inline">X^j</span> are said to be independent conditionally to the remaining variables <span class="math inline">X^{\setminus (i, j)}</span>. We note, <span class="math display">
X^i
\perp \!\!\! \perp X^j |X^{\setminus (i, j)} \Leftrightarrow \Omega_{ij} = 0.
</span></p>
<p>Let <span class="math inline">\boldsymbol X = \left( \boldsymbol X_1^T, \dots, \boldsymbol X_n^T \right )^T</span> be the <span class="math inline">n \times p</span>-dimensional data matrix composed of <span class="math inline">n</span> i.i.d samples of the Gaussian random vector <span class="math inline">\mathbf X</span>. To perform graphical model inference, <span class="citation" data-cites="Meinshausen2006">Meinshausen and Bühlmann (<a href="#ref-Meinshausen2006" role="doc-biblioref">2006</a>)</span> consider <span class="math inline">p</span> separate linear regressions of the form: <span id="eq-neighborhood"><span class="math display">
\hat{\boldsymbol{\beta}^i}(\lambda) = \underset{\boldsymbol{\beta}^i
\in \mathbb{R}^{p-1}}{\operatorname{argmin}} \frac{1}{n} \left \lVert
\mathbf{X}^i - \mathbf{X}^{\setminus i} \boldsymbol{\beta}^i \right \rVert_2 ^2
+ \lambda \left \lVert \boldsymbol{\beta}^i \right \rVert_1,
\tag{1}</span></span> where <span class="math inline">\lambda</span> is a non-negative regularization parameter, <span class="math inline">\mathbf{X}^{\setminus i}</span> denotes the matrix <span class="math inline">\mathbf{X}</span> deprived of column <span class="math inline">i</span>, <span class="math inline">\boldsymbol{\beta}^i = (\beta^i_j)_{j \in \{1,\dots,p\} \backslash i}</span> is a vector of <span class="math inline">p-1</span> regression coefficients and <span class="math inline">\left \lVert . \right \rVert_1</span> is the <span class="math inline">\ell_1-</span>norm. These Lasso regularized problems estimate the neighborhoods, one variable at a time. The final edge set estimates <span class="math inline">\hat E</span> can be deduced from the union of the estimated neighborhoods using an AND or OR rule (<span class="citation" data-cites="Meinshausen2006">Meinshausen and Bühlmann (<a href="#ref-Meinshausen2006" role="doc-biblioref">2006</a>)</span>). The MB approach is based on the central relationship between simple linear regression and precision matrix coefficients. It can be shown that <span class="math inline">\beta^i_j = -\frac{\Omega_{ij}}{\Omega_{ii}}</span> <span class="citation" data-cites="Lauritzen1996">(<a href="#ref-Lauritzen1996" role="doc-biblioref">Lauritzen 1996</a>)</span>.</p>
<p>On the other hand, let us now consider the clustering analysis of the <span class="math inline">p</span> variables in <span class="math inline">\mathbb R^n.</span> The convex clustering problem <span class="citation" data-cites="Hocking2011 Lindsten2011 pelckmans2005convex">(<a href="#ref-Hocking2011" role="doc-biblioref">Hocking et al. 2011</a>; <a href="#ref-Lindsten2011" role="doc-biblioref">Lindsten, Ohlsson, and Ljung 2011</a>; <a href="#ref-pelckmans2005convex" role="doc-biblioref">Pelckmans et al. 2005</a>)</span> is the minimization of the quantity <span id="eq-clusterpath"><span class="math display">
\frac{1}{2}
\sum_{i=1}^p \left \lVert \boldsymbol X^i - \boldsymbol  \alpha^i \right
\rVert_2^2 + \lambda \sum_{i &lt; j} w_{ij} \left \lVert \boldsymbol  \alpha^i -
\boldsymbol  \alpha^j \right \rVert_q
\tag{2}</span></span> with respect to the matrix <span class="math inline">\boldsymbol \alpha \in \mathbb R^{p \times n}</span>, where <span class="math inline">\lambda</span> is a sparsity penalization parameter, <span class="math inline">\{ w_{ij} \}</span> are symmetric positive weights, <span class="math inline">\boldsymbol \alpha^i \in \mathbb R^n</span> is the centroid to which <span class="math inline">\boldsymbol X^i</span> is assigned to, and <span class="math inline">\left \lVert . \right \rVert_q</span> is the <span class="math inline">\ell_q</span>-norm on <span class="math inline">\mathbb R^p</span> with <span class="math inline">q \ge 1.</span> Points <span class="math inline">\boldsymbol X^i</span> and <span class="math inline">\boldsymbol X^j</span> are assigned to the same cluster if <span class="math inline">\hat{\boldsymbol \alpha^i} \approx \hat{\boldsymbol \alpha^j}.</span> The regularization path of solutions to problem in <a href="#eq-clusterpath">Equation&nbsp;2</a> can be represented as a dendrogram. The path properties have been studied in <span class="citation" data-cites="chi2015splitting">Chi and Lange (<a href="#ref-chi2015splitting" role="doc-biblioref">2015</a>)</span> and <span class="citation" data-cites="chiquet2017fast">Chiquet, Gutierrez, and Rigaill (<a href="#ref-chiquet2017fast" role="doc-biblioref">2017</a>)</span>, among others. Note that these approaches rely on geometric properties of matrix <span class="math inline">\boldsymbol X,</span> and do not require any assumption on the distribution of the covariates.</p>
<p>We propose to combine the MB and convex clustering approaches. Specifically, the <span class="math inline">p</span> independent Lasso regressions of the MB approach are merged into a single optimization criterion where a convex clustering fusion penalty in <span class="math inline">\ell_2</span> is applied on the regression vectors considered as cluster centers. Namely, the <em>Multiscale Graphical Lasso</em> (MGLasso) pseudo-likelihood problem minimizes in a Gaussian framework the following quantity: <span id="eq-cost-fct"><span class="math display">
J_{\lambda_1,
\lambda_2}(\boldsymbol{\beta}; \mathbf{X} ) = \frac{1}{2} \sum_{i=1}^p \left
\lVert \mathbf{X}^i - \mathbf{X}^{\setminus i} \boldsymbol{\beta}^i \right
\rVert_2 ^2  + \lambda_1 \sum_{i = 1}^p  \left \lVert \boldsymbol{\beta}^i
\right \rVert_1 + \lambda_2 \sum_{i &lt; j} \left \lVert \boldsymbol{\beta}^i -
\boldsymbol \tau_{ij}\boldsymbol{\beta}^j \right \rVert_2,
\tag{3}</span></span> with respect to <span class="math inline">\boldsymbol{\beta} := [{\boldsymbol{\beta}^1}, \ldots, {\boldsymbol{\beta}^p}] \in \mathbb{R}^{(p-1) \times p},</span> where <span class="math inline">\mathbf{X}^{i}\in \mathbb{R}^n</span> denotes the <span class="math inline">i</span>-th column of <span class="math inline">\mathbf{X}</span>, <span class="math inline">\lambda_1</span> and <span class="math inline">\lambda_2</span> are penalization parameters, <span class="math inline">\boldsymbol \tau_{ij} \in \mathbb R^{(p-1)\times(p-1)}</span> is a permutation matrix, which permutes the coefficients in the regression vector <span class="math inline">\boldsymbol \beta^j</span> such as <span class="math display">
\left
\lVert \boldsymbol{\beta}^i - \boldsymbol \tau_{ij}\boldsymbol{\beta}^j \right
\rVert_2 = \sqrt{\sum_{k \in \{1, \dots,p \} \backslash \{i,j\}} (\beta^i_k -
\beta^j_k)^2 + (\beta^i_j - \beta^j_i)^2 },
</span> as illustrated in <a href="#fig-permute-beta">Figure&nbsp;1</a>. The coefficient <span class="math inline">\beta^i_k</span> is to be read as the multiple regression coefficients of <span class="math inline">\boldsymbol X^i</span> on <span class="math inline">\boldsymbol X^k.</span></p>
<p>The MGLasso criterion can be seen as a multitask regression problem where the set of responses is identical to the set of predictors. The Lasso penalty term encourages sparsity in the estimated coefficients while the group-fused term encourages fusion in the regression vectors <span class="math inline">\boldsymbol{\beta}^i</span> and <span class="math inline">\boldsymbol{\beta}^j</span>.</p>
<p>Let us illustrate by an example the effect of the fusion term in the proposed approach. Two variables <span class="math inline">i</span> and <span class="math inline">j</span> are in the same group when <span class="math inline">\|\boldsymbol{\beta}^i - \boldsymbol \tau_{ij} \boldsymbol{\beta}^j\|_2 \approx 0</span>. Considering a cluster <span class="math inline">\mathcal C</span> of <span class="math inline">q</span> variables, it is straightforward to show that <span class="math inline">\forall (i,j) \in \mathcal C^2</span>, we have <span class="math inline">\hat {\beta^i_j}=\beta_{\mathcal C}</span>, where <span class="math inline">\beta_{\mathcal C}</span> is a scalar. Thus the algorithm is likely to produce precision matrices with blocks of constant entries for a given value of <span class="math inline">\lambda_2,</span> each block corresponding to a cluster. In the same vein as <span class="citation" data-cites="Park2007">Park, Hastie, and Tibshirani (<a href="#ref-Park2007" role="doc-biblioref">2006</a>)</span>, a cluster composed of variables that share the same coefficients can be summarized by a representative variable.</p>
<p>A component-wise difference between two regression vectors without reordering the coefficients would not necesarily cluster variables which share the same neighborhood. The permutation <span class="math inline">\boldsymbol \tau_{ij}</span> reoders coefficients in such a way that differences are taken between symmetric coeffecients and those corresponding to the same set of predictors. The model is thus likely to cluster together variables that share the same neighboring structure and encourages symmetric graph structures.</p>
<div id="fig-permute-beta" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figures/permute-beta.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Illustration of the permutation between regression coefficients in the MGLasso model.</figcaption>
</figure>
</div>
<p>In practice, when external information about the clustering structure is available, the problem can be generalized into: <span id="eq-cost-fct-general"><span class="math display">
\min_{\boldsymbol{\beta}}
\sum_{i=1}^p\frac{1}{2} \left \lVert \mathbf{X}^i - \mathbf{X}^{\setminus i}
\boldsymbol{\beta}^i \right \rVert_2 ^2  + \lambda_1 \sum_{i = 1}^p \left \lVert
\boldsymbol{\beta}^i \right \rVert_1 + \lambda_2 \sum_{i &lt; j}  w_{ij} \left
\lVert \boldsymbol{\beta}^i - \boldsymbol \tau_{ij}\boldsymbol{\beta}^j \right
\rVert_2,
\tag{4}</span></span> where <span class="math inline">w_{ij}</span> is a positive weight. In the remainder of the paper, we will assume that <span class="math inline">w_{ij} = 1</span> for simplicity.</p>
</section>
<section id="numerical-scheme" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Numerical scheme</h1>
<p>This Section introduces a complete numerical scheme of the Multiscale Graphical Lasso via convex optimization and a model selection procedure. Section <a href="#optimization-via-conesta-algorithm">3.1</a> reviews the principles of the Continuation with Nesterov smoothing in a shrinkage-thresholding algorithm <span class="citation" data-cites="hadjselem2018">(CONESTA, <a href="#ref-hadjselem2018" role="doc-biblioref">Hadj-Selem et al. 2018</a>)</span>. Section <a href="#reformulation-of-mglasso-for-conesta-algorithm">3.2</a> details a reformulation of the MGLasso criterion, which eases the use of CONESTA as a solver. Finally, Section <a href="#model-selection">3.3</a> presents the procedure for selecting the regularization parameters.</p>
<section id="optimization-via-conesta-algorithm" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="optimization-via-conesta-algorithm"><span class="header-section-number">3.1</span> Optimization via CONESTA algorithm</h2>
<p>The optimization problem for Multiscale Graphical Lasso is convex but not straightforward to solve using classical algorithms because of the fused-lasso type penalty, which is non-separable and admits no closed-form solution for the proximal gradient. We rely on the Continuation with Nesterov smoothing in a shrinkage-thresholding algorithm <span class="citation" data-cites="hadjselem2018">(<a href="#ref-hadjselem2018" role="doc-biblioref">Hadj-Selem et al. 2018</a>)</span> dedicated to high-dimensional regression problems with structured sparsity, such as group structures.</p>
<p>The CONESTA solver, initially introduced for neuro-imaging problems, addresses a general class of convex optimization problems that include group-wise penalties. The algorithm solves problems in the form<br>
<span id="eq-conesta-criterion"><span class="math display">
\operatorname{minimize \ w.r.t. }
\boldsymbol{\theta} \quad f(\boldsymbol{\theta}) = g(\boldsymbol{\theta}) +
\lambda_1 h(\boldsymbol{\theta}) + \lambda_2 s(\boldsymbol{\theta}),
\tag{5}</span></span><br>
where <span class="math inline">\boldsymbol{\theta}\in \mathbb{R}^d</span> and <span class="math inline">\lambda_1</span> and <span class="math inline">\lambda_2</span> are penalty parameters.</p>
<p>In the original paper <span class="citation" data-cites="hadjselem2018">(<a href="#ref-hadjselem2018" role="doc-biblioref">Hadj-Selem et al. 2018</a>)</span>, <span class="math inline">g(\boldsymbol{\theta})</span> is a differentiable function, <span class="math inline">h(\boldsymbol{\theta})</span> is a penalty function whose proximal operator <span class="math inline">\operatorname{prox}_{\lambda_1 h}</span> is known in closed-form.</p>
<p>Given <span class="math inline">\phi \subseteq \{1,\ldots, d\},</span> let <span class="math inline">\boldsymbol{\theta}_\phi = (\theta_i)_{i \in \phi}</span> denote the subvector of <span class="math inline">\boldsymbol{\theta}</span> referenced by the indices in <span class="math inline">\phi.</span> Denote <span class="math inline">\Phi = \{ \phi_1, \dots, \phi_{\operatorname{Card}(\Phi)}\}</span> a collection with <span class="math inline">\phi_i \subseteq \{1,\ldots, d\}.</span> Let the matrix <span class="math inline">\mathbf{A}_\phi \in \mathbb{R}^{m \times \operatorname{Card}(\Phi) }</span> define a linear map from <span class="math inline">\mathbb{R}^{\operatorname{Card}(\phi)}</span> to <span class="math inline">\mathbb{R}^m</span> by sending the column vector <span class="math inline">\boldsymbol{\theta}_\phi \in \mathbb{R}^{\operatorname{Card}(\phi)}</span> to the column vector <span class="math inline">\mathbf{A}_\phi \boldsymbol{\theta}_\phi \in \mathbb{R}^m.</span> The function <span class="math inline">s(\boldsymbol{\theta})</span> is assumed to be an <span class="math inline">\ell_{1,2}</span>-norm i.e., the sum of the group-wise <span class="math inline">\ell_2</span>-norms of the elements <span class="math inline">\mathbf{A}_\phi \boldsymbol{\theta}_\phi, \phi \in \Phi.</span> Namely, <span class="math display">
s(\boldsymbol{\theta}) =
\sum_{\phi \in \Phi} \|\mathbf{A}_\phi \boldsymbol{\theta}_\phi\|_2.
</span> When <span class="math inline">\mathbf{A}_\phi</span> is the identity operator, the penalty function <span class="math inline">s</span> is the overlapping group-lasso and <span class="math inline">m = \operatorname{Card}(\phi)</span>. When it is a discrete derivative operator, <span class="math inline">s</span> is a total variation penalty, and <span class="math inline">m</span> can be seen as the number of neighborhood relationships.</p>
<p>The non-smooth <span class="math inline">\ell_{1,2}</span>-norm penalty can be approximated by a smooth function with known gradient computed using Nesterov’s smoothing <span class="citation" data-cites="nesterov2005smooth">(<a href="#ref-nesterov2005smooth" role="doc-biblioref">Nesterov 2005b</a>)</span>. Given a smoothness parameter <span class="math inline">\mu&gt;0</span>, let us define the smooth approximation <span class="math display">
s_{\mu}(\boldsymbol{\theta}) = \max_{\boldsymbol{\alpha}
\in \mathcal{K}} \left \{ \boldsymbol{\alpha}^T \mathbf{A} \boldsymbol{\theta} -
\frac{\mu}{2} \| \boldsymbol{\alpha} \|_2^2 \right \},
</span> where <span class="math inline">\mathcal{K}</span> is the cartesian product of <span class="math inline">\ell_2</span>-unit balls, <span class="math inline">\mathbf{A}</span> is the vertical concatenation of the matrices <span class="math inline">\mathbf{A}_\phi</span> and <span class="math inline">\boldsymbol{\alpha}</span> is an auxiliary variable resulting from the dual reformulation of <span class="math inline">s(\boldsymbol{\theta})</span>. Note that <span class="math inline">\lim_{\mu \rightarrow 0} s_{\mu}(\boldsymbol{\theta}) = s(\boldsymbol{\theta}).</span> A Fast Iterative Shrinkage-Thresholding Algorithm <span class="citation" data-cites="Beck2009">(FISTA, <a href="#ref-Beck2009" role="doc-biblioref">Beck and Teboulle 2009</a>)</span> step can then be applied after computing the gradient of the smooth part i.e.&nbsp;<span class="math inline">g(\boldsymbol{\theta}) + \lambda_2 s_{\mu}(\boldsymbol{\theta})</span> of the approximated criterion.</p>
<p>The main ingredient of CONESTA remains in the determination of the optimal smoothness parameter using the duality gap, which minimizes the number of FISTA iterations for a given precision <span class="math inline">\epsilon.</span> The specification of <span class="math inline">\mu</span> is subject to dynamic update. A sequence of decreasing optimal smoothness parameters is generated in order to dynamically adapt the FISTA algorithm stepsize towards <span class="math inline">\epsilon.</span> Namely, <span class="math inline">\mu^k = \mu_{opt}(\epsilon^k).</span> The smoothness parameter decreases as one gets closer to <span class="math inline">\boldsymbol{\theta} ^\star</span>, the solution of the problem defined in <a href="#eq-conesta-criterion">Equation&nbsp;5</a>. Since <span class="math inline">\boldsymbol{\theta} ^\star</span> is unknown; the approximation of the distance to the minimum is achieved via the duality gap. Indeed <span class="math display">
\operatorname{GAP}_{\mu^k}(\boldsymbol{\theta}^k) \ge
f_{\mu^k}(\boldsymbol{\theta}^k) - f(\boldsymbol{\theta}^\star) \ge 0.
</span> We refer the reader to the seminal paper for more details on the formulation of <span class="math inline">\operatorname{GAP}_{\mu^k}(\boldsymbol{\theta}^k).</span> The CONESTA routine is spelled out in the algorithm CONESTA solver where <span class="math inline">L(g + \lambda_2 s_{\mu})</span> is the Lipschitz constant of <span class="math inline">\nabla(g + \lambda_2 s_{\mu}),</span> <span class="math inline">k</span> is the iteration counter for the inner FISTA updates and <span class="math inline">i</span> is the iteration counter for CONESTA updates.</p>
<div id="alg-conesta" class="pseudocode-container" data-comment-delimiter="//" data-pseudocode-index="1" data-no-end="false" data-alg-title="Algorithm" data-indent-size="1.2em" data-line-number="true" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{CONESTA solver} \begin{algorithmic} \State \textbf{Inputs}: \\ $\quad$ functions $g(\boldsymbol{\theta}), h(\boldsymbol{\theta}), s(\boldsymbol{\theta})$ \\ $\quad$ precision $\epsilon$ \\ $\quad$ penalty parameters $\lambda_1, \lambda_2$ \\ $\quad$ decreasing factor $\boldsymbol \tau \in (0,1)$ for sequence of precisions \State \textbf{Output:} \\ $\quad$ $\boldsymbol{\theta}^{i+1} \in \mathbb{R}^d$ \State \textbf{Initializations:} \\ $\quad \boldsymbol{\theta}^0 \in \mathbb{R}^d$ \\ $\quad \epsilon^0 = \boldsymbol \tau \operatorname{GAP}_{\mu = 10^{-8}}(\boldsymbol{\theta}^0)$ \\ $\quad \mu^0 = \mu_{opt}(\epsilon^0)$ \Repeat \State $\epsilon^i_{\mu} = \epsilon^i - \mu^i \lambda_2 \frac{d}{2}$ \\ \Comment{FISTA} \State $k=2$ \Comment{new iterator} \State $\boldsymbol{\theta}_{\operatorname{FISTA}}^1 = \boldsymbol{\theta}_{\operatorname{FISTA}}^0 = \boldsymbol{\theta}^i$ \Comment{Initial parameters value} \State $t_{\mu} = \frac{1}{L(g + \lambda_2 s_{\mu})}$ \Comment{Compute stepsize with $L(g + \lambda_2 s_{\mu})$ the Lipschitz constant of $\nabla(g + \lambda_2 s_{\mu})$} \Repeat \State $\boldsymbol{z} = \boldsymbol{\theta}_{\operatorname{FISTA}}^{k-1} + \frac{k-2}{k+1}(\boldsymbol{\theta}_{\operatorname{FISTA}}^{k-1} - \boldsymbol{\theta}_{\operatorname{FISTA}}^{k-2})$ \State $\boldsymbol{\theta}_{\operatorname{FISTA}}^k = \operatorname{prox}_{\lambda_1 h}(\boldsymbol{z} - t_{\mu} \nabla(g + \lambda_2 s_{\mu})(\boldsymbol{z}))$ \Until{$\operatorname{GAP}_{\mu}(\boldsymbol{\theta}_{\operatorname{FISTA}}^k) \le \epsilon_{\mu}^i$} \State $\boldsymbol{\theta}^{i+1} = \boldsymbol{\theta}_{\operatorname{FISTA}}^k$ \\ \State $\epsilon^i = \operatorname{GAP}_{\mu = \mu_i} \boldsymbol{\theta}^{i+1} + \mu^i \lambda_2 \frac{d}{2}$ \\ \State $\epsilon^{i+1} = \boldsymbol \tau \epsilon^{i}$ \\ \State $\mu^{i+1} = \mu_{opt}(\epsilon^{i+1})$ \Until{$\epsilon^i \le \epsilon$} \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
<section id="reformulation-of-mglasso-for-conesta-algorithm" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="reformulation-of-mglasso-for-conesta-algorithm"><span class="header-section-number">3.2</span> Reformulation of MGLasso for CONESTA algorithm</h2>
<p>Using CONESTA for solving the MGLasso problem requires a reformulation in order to comply with the form of loss function required by CONESTA. The objective of MGLasso can be written as <span id="eq-refpbm"><span class="math display">
\operatorname{argmin} \frac{1}{2} ||\mathbf{Y} - \tilde{\mathbf{X}}
\tilde{\boldsymbol{\beta}}||_2^2 + \lambda_1 ||\tilde{\boldsymbol{\beta}}||_1 +
\lambda_2 \sum_{i&lt;j} ||\boldsymbol D_{ij} \tilde{\boldsymbol{\beta}}||_2,
\tag{6}</span></span></p>
<p>where <span class="math inline">\mathbf{Y} = \operatorname{Vec}(\mathbf{X}) \in \mathbb{R}^{np}, \tilde{\boldsymbol{\beta}} = \operatorname{Vec(\boldsymbol{\beta})} \in \mathbb{R}^{p (p-1)}, \tilde{\mathbf{X}}</span> is a <span class="math inline">\mathbb{R}^{[np]\times [p \times (p-1)]}</span> block-diagonal matrix with <span class="math inline">\mathbf{X}^{\setminus i}</span> on the <span class="math inline">i</span>-th block. The matrix <span class="math inline">\boldsymbol D_{ij}</span> is a <span class="math inline">(p-1)\times p(p-1)</span> matrix chosen so that <span class="math inline">\boldsymbol D_{ij} \tilde{\boldsymbol{\beta}} = \boldsymbol{\beta}^i - \boldsymbol \tau_{ij} \boldsymbol{\beta}^j.</span></p>
<p>Note that we introduce this notation for simplicity of exposition, but, in practice, the sparsity of the matrices <span class="math inline">\boldsymbol D_{ij}</span> allows a more efficient implementation. Based on reformulation <a href="#eq-refpbm">Equation&nbsp;6</a>, we may apply CONESTA to solve the objective of MGLasso for fixed <span class="math inline">\lambda_1</span> and <span class="math inline">\lambda_2</span>. The procedure is applied, for fixed <span class="math inline">\lambda_1</span>, to a range of decreasing values of <span class="math inline">\lambda_2</span> to obtain a hierarchical clustering. The corresponding pseudo-code is given in the following algorithm where <span class="math inline">(\mathbf{X}^i)^{\dagger}</span> denotes the pseudo-inverse of <span class="math inline">\mathbf{X}^i</span> and <span class="math inline">\epsilon_{fuse}</span> the threshold for merging clusters. <revision> We note here that problem in <a href="#eq-refpbm">Equation&nbsp;6</a> is of the same form as the optimization problem solved in the paper by <span class="citation" data-cites="hadjselem2018">Hadj-Selem et al. (<a href="#ref-hadjselem2018" role="doc-biblioref">2018</a>)</span>: as they showed, CONESTA outperforms other optimization approaches such as the alternating direction method of multipliers <span class="citation" data-cites="Boyd2011">(ADMM, <a href="#ref-Boyd2011" role="doc-biblioref">Boyd et al. 2011</a>)</span>, the excessive gap method <span class="citation" data-cites="nesterov2005excessive">(EGM, <a href="#ref-nesterov2005excessive" role="doc-biblioref">Nesterov 2005a</a>)</span>, the classical FISTA with fixed smoothing and the inexact FISTA <span class="citation" data-cites="schmidt2011convergence">(<a href="#ref-schmidt2011convergence" role="doc-biblioref">Schmidt, Roux, and Bach 2011</a>)</span>. Rather than repeating their experiments, we refer the reader to Section IV of their paper. </revision></p>
<div id="alg-mglasso" class="pseudocode-container" data-comment-delimiter="//" data-pseudocode-index="2" data-no-end="false" data-alg-title="Algorithm" data-indent-size="1.2em" data-line-number="true" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{MGLasso algorithm} \begin{algorithmic} \State \textbf{Inputs}: \\ $\quad$ Set of variables $\mathbf{X} = \{\mathbf{X}^1, \dots, \mathbf{X}^p \} \in \mathbb R^{n\times p}$ \\ $\quad$ Penalty parameters $\lambda_1 \ge 0, {\lambda_2}_{\operatorname{initial}} &gt; 0$ \\ $\quad$ Increasing factor $\eta &gt; 1$ for fusion penalties $\lambda_2$\\ $\quad$ Fusion threshold $\epsilon_{fuse} \ge 0$ \State \textbf{Outputs:} For $\lambda_1$ fixed and $\lambda_2$ from $0$ to ${\lambda_2}_{\operatorname{initial}} \times \eta^{(I)}$ with $I$ the number of iterations: \\ $\quad$ Regression vectors $\boldsymbol{\beta}(\lambda_1, \lambda_2) \in \mathbb R^{p \times (p-1)}$, \\ $\quad$ Clusters partition of variables indices in $K$ clusters: $C(\lambda_1, \lambda_2)$ \State \textbf{Initializations:} \\ $\quad$ $\boldsymbol{\beta}^i = (\mathbf{X}^i)^{\dagger}\mathbf{X}^i$, $\forall i = 1, \dots, p$ for warm start in CONESTA solver \\ $\quad$ $C = \left \{\{1\}, \dots, \{p\}\right \}$ Initial clusters with one element per cluster. \\ $\quad$ Set $\lambda_2 = 0$ \\ $\quad$ Compute $\boldsymbol{\beta}$ using CONESTA solver \\ $\quad$ Update clusters $C$ with rule described in \textbf{while} loop. \State \textbf{Set:} $\lambda_2 = {\lambda_2}_{\operatorname{initial}}$ \\ \Comment{Clustering path} \While{$\operatorname{Card}(C) &gt; 1$} \State Compute $\boldsymbol{\beta}$ using CONESTA solver with warm start from previous iteration \\ \Comment{Clusters update} \State Compute pairwises distances $d(i,j)=\left \lVert \boldsymbol{\beta}^i - \boldsymbol \tau_{ij} \boldsymbol{\beta}^j \right \rVert_2$, $\forall i,j \in \{1, \dots, p\}$ \\ \State Determine clusters $C_k (k=1, \dots, K)$ with the rule $(i,j) \in C_k$ iff. $d(i,j) \le \epsilon_{fuse}$ \State $\lambda_2 = \lambda_2 \times \nu$ \EndWhile \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
<section id="model-selection" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">3.3</span> Model selection</h2>
<p>A crucial question for practical applications is the definition of a rule to select the penalty parameters (<span class="math inline">\lambda_1, \lambda_2</span>). This selection problem operates at two levels: <span class="math inline">\lambda_1</span> controls the sparsity of the graphical model, and <span class="math inline">\lambda_2</span> controls the number of clusters in the optimal clustering partition. These two parameters are dealt with separately: the sparsity parameter <span class="math inline">\lambda_1</span> is chosen via model selection, while the clustering parameter <span class="math inline">\lambda_2</span> varies across a grid of values in order to obtain graphs with different levels of granularity. The problem of model selection in graphical models is difficult in the high dimensional case where the number of samples is small compared to the number of variables, as classical Akaike information criterion <span class="citation" data-cites="akaike1998information">(AIC, <a href="#ref-akaike1998information" role="doc-biblioref">Akaike 1998</a>)</span> and Bayesian information criterion <span class="citation" data-cites="schwarz1978estimating">(BIC, <a href="#ref-schwarz1978estimating" role="doc-biblioref">Schwarz 1978</a>)</span> tend to perform poorly <span class="citation" data-cites="Liu2010">(<a href="#ref-Liu2010" role="doc-biblioref">Liu, Roeder, and Wasserman 2010</a>)</span>.</p>
<p>In this paper, we focus on the StARS stability selection approach proposed by <span class="citation" data-cites="Liu2010">Liu, Roeder, and Wasserman (<a href="#ref-Liu2010" role="doc-biblioref">2010</a>)</span> as suggested by some preliminary tests where we compared the Extended BIC <span class="citation" data-cites="foygel2010extended">(EBIC, <a href="#ref-foygel2010extended" role="doc-biblioref">Foygel and Drton 2010</a>)</span>, a model selection criterion calibrated with slope heuristics <span class="citation" data-cites="baudry2012slope">(<a href="#ref-baudry2012slope" role="doc-biblioref">Baudry, Maugis, and Michel 2012</a>)</span>, the Rotation invariant criterion implemented in the Huge package <span class="citation" data-cites="zhao2012huge">(<a href="#ref-zhao2012huge" role="doc-biblioref">Zhao et al. 2012</a>)</span>, the GGMSelect procedure <span class="citation" data-cites="giraud2012graph">(<a href="#ref-giraud2012graph" role="doc-biblioref">Giraud, Huet, and Verzelen 2012</a>)</span>, cross-validation <span class="citation" data-cites="bien2011sparse">(<a href="#ref-bien2011sparse" role="doc-biblioref">Bien and Tibshirani 2011</a>)</span> and StARS. The method uses <span class="math inline">k</span> subsamples of data to estimate the associated graphs for a given range of <span class="math inline">\lambda_1</span> values. For each value, a global instability of the graph edges is computed. The optimal value of <span class="math inline">\lambda_1</span> is chosen so as to minimize the instability, as follows. Let <span class="math inline">\lambda^{(1)}_1, \dots, \lambda_1^{(K)}</span> be a grid of sparsity regularization parameters, and <span class="math inline">S_1, \dots, S_N</span> be the <span class="math inline">N</span> bootstrap samples obtained by sampling the rows of the data set <span class="math inline">\mathbf{X}</span>. For each <span class="math inline">k\in\{1,\ldots,K\}</span> and for each <span class="math inline">j\in\{1,\ldots, N\}</span>, we denote by <span class="math inline">\mathcal{A}^{k,j}(\mathbf{X})</span> the adjacency matrix of the estimated graph obtained by applying the inference algorithm to <span class="math inline">S_n</span> with regularization parameter <span class="math inline">\lambda_1^{(k)}</span>. For each possible edge <span class="math inline">(s,t)\in\{1,\ldots,p\}^2</span>, the probability of edge appearance is estimated empirically by <span class="math display">
\hat
\theta_{st}^{(k)} = \frac{1}{N} \sum_{j=1}^N \mathcal{A}^{k,j}_{st}.
</span> Define <old> <span class="math display">\hat \xi_{st}(\Lambda) = 2 \hat \theta_{st} (\Lambda) \left ( 1 - \hat
\theta_{st} (\Lambda) \right )</span></old></p>
<p><span class="math display">
\hat \xi_{st}(\lambda_1^{(k)}) = 2 \hat \theta_{st}^{(k)}  \left ( 1 - \hat
\theta_{st}^{(k)} \right )
</span></p>
<p>the empirical instability of edge <span class="math inline">(s,t)</span> (that is, twice the variance of the Bernoulli indicator of edge <span class="math inline">(s,t)</span>). The instability level associated with <span class="math inline">\lambda_1^{(k)}</span> is given by <span class="math display">
\hat D(\lambda_1^{(k)}) = \frac{\sum_{s&lt;t} \hat \xi_{st}(\lambda_1^{(k)})}{
\binom{p}{2}}.
</span> StARS selects the optimal penalty parameter as follows <span class="math display">
\hat \lambda = \max_k\left\{ \lambda_1^{(k)}: \hat D(\lambda_1^{(k)}) \le
\upsilon, k\in\{1,\ldots,K\} \right \},
</span> where <span class="math inline">\upsilon</span> is the threshold chosen for the instability level.</p>
</section>
</section>
<section id="simulation-experiments" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Simulation experiments</h1>
<p>In this Section, we conduct a simulation study to evaluate the performance of the MGLasso method, both in terms of clustering and support recovery. Receiver Operating Characteristic (ROC) curves are used to evaluate the adequacy of the inferred graphs with the ground truth for the MGLasso and GLasso in its neighborhood selection version in the Erdös-Rényi <span class="citation" data-cites="erdHos1960evolution">(<a href="#ref-erdHos1960evolution" role="doc-biblioref">Erdős, Rényi, et al. 1960</a>)</span>, Scale-free <span class="citation" data-cites="newman2001random">(<a href="#ref-newman2001random" role="doc-biblioref">Newman, Strogatz, and Watts 2001</a>)</span>, and Stochastic Block Models <span class="citation" data-cites="fienberg1981categorical">(SBM, <a href="#ref-fienberg1981categorical" role="doc-biblioref">Fienberg and Wasserman 1981</a>)</span> frameworks. The Adjusted Rand indices are used to compare the partitions obtained with MGLasso, hierarchical agglomerative clustering, and K-means clustering in a stochastic block model framework.</p>
<section id="synthetic-data-models" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="synthetic-data-models"><span class="header-section-number">4.1</span> Synthetic data models</h2>
<p>We consider three different synthetic network models: the Stochastic Block Model <span class="citation" data-cites="fienberg1981categorical">(<a href="#ref-fienberg1981categorical" role="doc-biblioref">Fienberg and Wasserman 1981</a>)</span>, the Erdös-Renyi model <span class="citation" data-cites="erdHos1960evolution">(<a href="#ref-erdHos1960evolution" role="doc-biblioref">Erdős, Rényi, et al. 1960</a>)</span> and the Scale-Free model <span class="citation" data-cites="newman2001random">(<a href="#ref-newman2001random" role="doc-biblioref">Newman, Strogatz, and Watts 2001</a>)</span>. In each case, Gaussian data is generated by drawing <span class="math inline">n</span> independent realizations of a multivariate Gaussian distribution <span class="math inline">\mathcal N(0, \mathbf{\Sigma})</span> where <span class="math inline">\mathbf{\Sigma} \in \mathbb{R}^{p \times p}</span> and <span class="math inline">\mathbf{\Omega} = \mathbf{\Sigma} ^{-1}</span>. The support of <span class="math inline">\mathbf{\Omega}</span>, equivalent to the network adjacency matrix, is generated from the three different models. The difficulty level of the problem is controlled by varying the ratio <span class="math inline">\frac{n}{p}</span> with <span class="math inline">p</span> fixed at <span class="math inline">40</span>: <span class="math inline">\frac{n}{p}\in \{0.5,1,2\}</span>.</p>
<section id="stochastic-block-model" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="stochastic-block-model"><span class="header-section-number">4.1.1</span> Stochastic Block Model</h3>
<p>We construct a block-diagonal precision matrix <span class="math inline">\mathbf{\Omega}</span> as follows. First, we generate the support of <span class="math inline">\mathbf{\Omega}</span> as shown in <a href="#fig-model-sbm">Figure&nbsp;2</a>, denoted by <span class="math inline">\boldsymbol A\in\{0,1\}^{p\times p}</span>. To do this, the variables are first partitioned into <span class="math inline">K = 5</span> hidden groups, noted <span class="math inline">C_1, \dots, C_K</span> described by a latent random variable <span class="math inline">Z_i</span>, such that <span class="math inline">Z_i = k</span> if <span class="math inline">i = C_k</span>. <span class="math inline">Z_i</span> follows a multinomial distribution <span class="math display">
P(Z_i = k) = \pi_k, \quad
\forall k \in \{1, \dots, K\},
</span></p>
<p>where <span class="math inline">\pi = (\pi_1, \dots, \pi_k)</span> is the vector of proportions of clusters whose sum is equal to one. The set of latent variables is noted <span class="math inline">\mathbf{Z} = \{ Z_1, \dots, Z_K\}</span>. Conditionally to <span class="math inline">\mathbf{Z}</span>, <span class="math inline">A_{ij}</span> follows a Bernoulli distribution such that <span class="math display">
A_{ij}|Z_i =
k, Z_j = l \sim \mathcal{B}(\alpha_{kl}), \quad \forall k,l \in \{1, \dots,
K\},
</span></p>
<p>where <span class="math inline">\alpha_{kl}</span> is the probability of inter-cluster connectivity, with <span class="math inline">\alpha_{kl} = 0.01</span> if <span class="math inline">k\neq l</span> and <span class="math inline">\alpha_{ll} = 0,75</span>. For <span class="math inline">k\in\{1,\ldots, K\}</span>, we define <span class="math inline">p_k = \sum_{i=1}^p \boldsymbol{1}_{\{Z_i = k\}}</span>. The precision matrix <span class="math inline">\mathbf{\Omega}</span> of the graph is then calculated as follows. We define <span class="math inline">\Omega_{ij} = 0</span> if <span class="math inline">Z_i\neq Z_j</span> ; otherwise, we define <span class="math inline">\Omega_{ij} = A_{ij}\omega_{ij}</span> where, for all <span class="math inline">i\in\{1,\ldots,p\}</span> and for all <span class="math inline">j\in\{1,\ldots,p| Z_j = Z_i\}</span>, <span class="math inline">\omega_{ij}</span> is given by : <span class="math display">
\begin{aligned}
&amp;\omega_{ii} := \frac{1+\rho(p_{Z_i}-2)}{1+\rho(p_{Z_i}-2)-\rho^2(p_{Z_i}-1)};\\
&amp;\omega_{ij} := \frac{-\rho}{1+\rho(p_{Z_i}-2)-\rho^2(p_{Z_i}-1)}.
\end{aligned}
</span> If <span class="math inline">\alpha_{ll}</span> were to be equal to one, this construction of <span class="math inline">\mathbf{\Omega}</span> would make it possible to control the level of correlation between the variables in each block to <span class="math inline">\rho</span>. Introducing a more realistic scheme with <span class="math inline">\alpha_{ll}=0.75</span> allows only to have an approximate control.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mglasso)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2020</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>sim_sbm <span class="ot">&lt;-</span> <span class="fu">sim_data</span>(</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> <span class="dv">40</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">structure =</span> <span class="st">"block_diagonal"</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="fu">rep</span>(<span class="dv">1</span> <span class="sc">/</span> <span class="dv">5</span>, <span class="dv">5</span>),</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">prob_mat =</span> <span class="fu">diag</span>(<span class="fl">0.75</span>, <span class="dv">5</span>),</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">rho =</span> <span class="fl">0.2</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">inter_cluster_edge_prob =</span> <span class="fl">0.01</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>gsbm <span class="ot">&lt;-</span> <span class="fu">adj_mat</span>(sim_sbm<span class="sc">$</span>graph)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as</span>(gsbm, <span class="st">"sparseMatrix"</span>),</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">""</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>)   </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-model-sbm" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-model-sbm-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Adjacency matrix of a stochastic block model defined by <span class="math inline">K=5</span> classes with identical prior probabilities set to <span class="math inline">\pi = 1/K</span>, inter-classes connection probability <span class="math inline">\alpha_{kl}=0.01, k \neq l</span>, intra-classes connection probability <span class="math inline">\alpha_{ll}=0.75</span> and <span class="math inline">p=40</span> vertices.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="erdös-renyi-model" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="erdös-renyi-model"><span class="header-section-number">4.1.2</span> Erdös-Renyi Model</h3>
<p>The Erdös-Renyi model is a special case of the stochastic block model where <span class="math inline">\alpha_{kl} = \alpha_{ll} = \alpha</span> is constant. We set the density <span class="math inline">\alpha</span> of the graph to <span class="math inline">0.1</span>; see <a href="#fig-model-erdos">Figure&nbsp;3</a> for an example of the graph resulting from this model.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2022</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>sim_erdos <span class="ot">&lt;-</span> <span class="fu">sim_data</span>(<span class="at">p =</span> <span class="dv">40</span>, <span class="at">structure =</span> <span class="st">"erdos"</span>, <span class="at">p_erdos =</span> <span class="fl">0.1</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>gerdos <span class="ot">&lt;-</span> <span class="fu">adj_mat</span>(sim_erdos<span class="sc">$</span>graph)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(<span class="fu">as</span>(gerdos, <span class="st">"sparseMatrix"</span>), <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-model-erdos" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-model-erdos-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Adjacency matrix of an Erdös-Renyi model with probability of connection <span class="math inline">\alpha = 0.1</span> and <span class="math inline">p=40</span> vertices.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="scale-free-model" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="scale-free-model"><span class="header-section-number">4.1.3</span> Scale-free Model</h3>
<p>The Scale-free Model generates networks whose degree distributions follow a power law. The graph starts with an initial chain graph of <span class="math inline">2</span> nodes. Then, new nodes are added to the graph one by one. Each new node is connected to an existing node with a probability proportional to the degree of the existing node. We set the number of edges in the graph to <span class="math inline">40</span>. An example of scale-free graph is shown in <a href="#fig-model-sfree">Figure&nbsp;4</a>.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2022</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>sim_sfree <span class="ot">&lt;-</span> <span class="fu">sim_data</span>(<span class="at">p =</span> <span class="dv">40</span>, <span class="at">structure =</span> <span class="st">"scale_free"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>gsfree <span class="ot">&lt;-</span> <span class="fu">adj_mat</span>(sim_sfree<span class="sc">$</span>graph)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(<span class="fu">as</span>(gsfree, <span class="st">"sparseMatrix"</span>), <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-model-sfree" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-model-sfree-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Adjacency matrix of a Scale-free model with <span class="math inline">40</span> edges and <span class="math inline">p=40</span> nodes.</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="support-recovery" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="support-recovery"><span class="header-section-number">4.2</span> Support recovery</h2>
<p>We compare the network structure learning performance of our approach to that of GLasso in its neighborhood selection version using ROC curves. In both GLasso and MGLasso, the sparsity is controlled by a regularization parameter <span class="math inline">\lambda_1</span>; however, MGLasso admits an additional regularization parameter, <span class="math inline">\lambda_2</span>, which controls the strength of convex clustering. To compare the two methods, in each ROC curve, we vary the parameter <span class="math inline">\lambda_1</span> while the parameter <span class="math inline">\lambda_2</span> (for MGLasso) is kept constant. We computed ROC curves for <span class="math inline">4</span> different penalty levels for the <span class="math inline">\lambda_2</span> parameter; since GLasso does not depend on <span class="math inline">\lambda_2</span>, the GLasso ROC curves are replicated.</p>
<p>In a decision rule associated with a sparsity penalty level <span class="math inline">\lambda_1</span>, we recall the definition of the two following functions. The true positive rate is given by <span class="math inline">\frac{TP(\lambda_1)}{TP(\lambda_1) + FN(\lambda_1)}.</span> The false positive rate is defined as follows <span class="math inline">1 - \frac{TN(\lambda_1)}{TN(\lambda_1) + FP(\lambda_1)}</span>, where <span class="math inline">TP</span> is the number of true positives, <span class="math inline">TN</span> the number of true negatives, <span class="math inline">FN</span> the number of false negatives and <span class="math inline">FP</span> the number of false positives. The ROC curve represents the true positive rate as a function of the false positive rate. For a given level of true positive rate, the best method minimizes the false positive rate.</p>
<p>For each configuration (<span class="math inline">n, p</span> fixed), we generate <span class="math inline">50</span> replications and their associated ROC curves, which are then averaged. The average ROC curves for the three models are given in <a href="#fig-roc-erdos">Figure&nbsp;5</a>, <a href="#fig-roc-sfree">Figure&nbsp;6</a> and <a href="#fig-roc-sbm">Figure&nbsp;7</a> by varying <span class="math inline">\frac{n}{p}\in \{0.5,1,2\}</span>.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ghibli)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"./data/roc_dtf_erdos.RData"</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>np.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"frac(n, p) == 0.5"</span>, <span class="st">"frac(n, p) == 1"</span>, <span class="st">"frac(n, p) == 2"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(np.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0.5"</span>, <span class="st">"1"</span>, <span class="st">"2"</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>tv.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"lambda[2] == 0"</span>, <span class="st">"lambda[2] == 3.33"</span>, <span class="st">"lambda[2] == 10"</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(tv.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0"</span>, <span class="st">"3.33"</span>, <span class="st">"10"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>roc_dtf_erdos <span class="ot">&lt;-</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(roc_dtf_erdos, tv <span class="sc">!=</span> <span class="fl">6.67</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(roc_dtf_erdos, <span class="fu">aes</span>(</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">x     =</span> <span class="dv">100</span> <span class="sc">*</span> fpr,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">y     =</span> <span class="dv">100</span> <span class="sc">*</span> tpr,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> method</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>)) <span class="sc">+</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(np <span class="sc">~</span> tv, <span class="at">labeller =</span> <span class="fu">labeller</span>(</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">np =</span> <span class="fu">as_labeller</span>(np.labs, label_parsed),</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">tv =</span> <span class="fu">as_labeller</span>(tv.labs, label_parsed)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> <span class="dv">0</span>,</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">slope =</span> <span class="dv">1</span>,</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">"dashed"</span>,</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"grey"</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"False Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"True Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Method"</span>,</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"GLasso"</span>, <span class="st">"MGLasso"</span>),</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> ghibli<span class="sc">::</span><span class="fu">ghibli_palette</span>(<span class="st">"MarnieMedium1"</span>)[<span class="dv">5</span><span class="sc">:</span><span class="dv">6</span>]</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>  ) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-roc-erdos" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-roc-erdos-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Mean ROC curves for MGLasso and GLasso graph inference in the Erdös-Renyi model. We varied the fusion penalty parameter of MGLasso <span class="math inline">\lambda_2 \in \{0, 3.33, 10\}</span> alongside the ratio <span class="math inline">\frac{n}{p}\in \{0.5,1,2\}</span>. Within each panel, the ROC curve shows the True positive rate (y-axis) vs.&nbsp;the False positive rate (x-axis) for both MGLasso (blue) and GLasso (brown). Since GLasso does not have a fusion penalty, its ROC curves were replicated for panels belonging to the same row. We also plot the random classifier (dotted grey line). The results have been averaged over <span class="math inline">50</span> simulated datasets and suggest that MGLasso performs no worse than GLasso. For <span class="math inline">\lambda_2 = 0</span>, the MGLasso approach is equivalent to GLasso in its neighborhood selection version.</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"./data/roc_dtf_sfree.RData"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>np.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"frac(n, p) == 0.5"</span>, <span class="st">"frac(n, p) == 1"</span>, <span class="st">"frac(n, p) == 2"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(np.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0.5"</span>, <span class="st">"1"</span>, <span class="st">"2"</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>tv.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"lambda[2] == 0"</span>, <span class="st">"lambda[2] == 3.33"</span>, <span class="st">"lambda[2] == 10"</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(tv.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0"</span>, <span class="st">"3.33"</span>, <span class="st">"10"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>roc_dtf_sfree <span class="ot">&lt;-</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(roc_dtf_sfree, tv <span class="sc">!=</span> <span class="fl">6.67</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(roc_dtf_sfree, <span class="fu">aes</span>(</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">x     =</span> <span class="dv">100</span> <span class="sc">*</span> fpr,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">y     =</span> <span class="dv">100</span> <span class="sc">*</span> tpr,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> method</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>)) <span class="sc">+</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(np <span class="sc">~</span> tv, <span class="at">labeller =</span> <span class="fu">labeller</span>(</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">np =</span> <span class="fu">as_labeller</span>(np.labs, label_parsed),</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">tv =</span> <span class="fu">as_labeller</span>(tv.labs, label_parsed)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> <span class="dv">0</span>,</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">slope =</span> <span class="dv">1</span>,</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">"dashed"</span>,</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"grey"</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"False Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"True Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Method"</span>,</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"GLasso"</span>, <span class="st">"MGLasso"</span>),</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">ghibli_palette</span>(<span class="st">"MarnieMedium1"</span>)[<span class="dv">5</span><span class="sc">:</span><span class="dv">6</span>]</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>  ) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-roc-sfree" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-roc-sfree-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Mean ROC curves for MGLasso and GLasso graph inference in the Scale-free model. We varied the fusion penalty parameter of MGLasso <span class="math inline">\lambda_2 \in \{0, 3.33, 10\}</span> alongside the ratio <span class="math inline">\frac{n}{p}\in \{0.5,1,2\}</span>. Within each panel, the ROC curve shows the True positive rate (y-axis) vs.&nbsp;the False positive rate (x-axis) for both MGLasso (blue) and GLasso (brown). Since GLasso does not have a fusion penalty, its ROC curves were replicated for panels belonging to the same row. We also plot the random classifier (dotted grey line). The results have been averaged over <span class="math inline">50</span> simulated datasets and suggest that MGLasso performs no worse than GLasso. For <span class="math inline">\lambda_2 = 0</span>, the MGLasso approach is equivalent to Glasso in its neighborhood selection version.</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"./data/roc_dtf_sbm.RData"</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>np.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"frac(n, p) == 0.5"</span>, <span class="st">"frac(n, p) == 1"</span>, <span class="st">"frac(n, p) == 2"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(np.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0.5"</span>, <span class="st">"1"</span>, <span class="st">"2"</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>tv.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"lambda[2] == 0"</span>, <span class="st">"lambda[2] == 3.33"</span>, <span class="st">"lambda[2] == 10"</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(tv.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0"</span>, <span class="st">"3.33"</span>, <span class="st">"10"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>roc_dtf_sbm <span class="ot">&lt;-</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(roc_dtf_sbm, tv <span class="sc">!=</span> <span class="fl">6.67</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(roc_dtf_sbm, <span class="fu">aes</span>(</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">x     =</span> <span class="dv">100</span> <span class="sc">*</span> fpr,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">y     =</span> <span class="dv">100</span> <span class="sc">*</span> tpr,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> method</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>)) <span class="sc">+</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(np <span class="sc">~</span> tv, <span class="at">labeller =</span> <span class="fu">labeller</span>(</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">np =</span> <span class="fu">as_labeller</span>(np.labs, label_parsed),</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">tv =</span> <span class="fu">as_labeller</span>(tv.labs, label_parsed)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> <span class="dv">0</span>,</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">slope =</span> <span class="dv">1</span>,</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">"dashed"</span>,</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"grey"</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"False Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"True Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Method"</span>,</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"GLasso"</span>, <span class="st">"MGLasso"</span>),</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">ghibli_palette</span>(<span class="st">"MarnieMedium1"</span>)[<span class="dv">5</span><span class="sc">:</span><span class="dv">6</span>]</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>  ) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-roc-sbm" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-roc-sbm-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Mean ROC curves for MGLasso and GLasso graph inference in the stochastic block model. We varied the fusion penalty parameter of MGLasso <span class="math inline">\lambda_2 \in \{0, 3.33, 10\}</span> alongside the ratio <span class="math inline">\frac{n}{p}\in \{0.5,1,2\}</span>. Within each panel, the ROC curve shows the True positive rate (y-axis) vs.&nbsp;the False positive rate (x-axis) for both MGLasso (blue) and GLasso (brown). Since GLasso does not have a fusion penalty, its ROC curves were replicated for panels belonging to the same row. We also plot the random classifier (dotted grey line). The results have been averaged over <span class="math inline">50</span> simulated datasets and suggest that MGLasso performs no worse than GLasso. For <span class="math inline">\lambda_2 = 0</span>, the MGLasso approach is equivalent to Glasso in its neighborhood selection version.</figcaption>
</figure>
</div>
</div>
</div>
<p>Based on these empirical results, we first observe that, in all the considered simulation models, MGLasso improves over GLasso in terms of support recovery in the high-dimensional setting where <span class="math inline">p&lt;n</span>. In addition, in the absence of a fusion penalty, i.e., <span class="math inline">\lambda_2 = 0</span>, MGLasso performs no worse than GLasso in each of the <span class="math inline">3</span> models. However, for <span class="math inline">\lambda_2&gt;0</span>, increasing penalty value does not seem to significantly improve the support recovery performances for the MGLasso, as we observe similar results for <span class="math inline">\lambda_2=3.3,10</span>. Preliminary analyses show that, as <span class="math inline">\lambda_2</span> increases, the estimates of the regression vectors are shrunk towards <span class="math inline">0</span>. This shrinkage effect of group-fused penalty terms was also observed in <span class="citation" data-cites="chu2021adaptive">(<a href="#ref-chu2021adaptive" role="doc-biblioref">Chu et al. 2021</a>)</span>. Note that the performance of the MGLasso deteriorates comparatively to GLasso when the inter-clusters edge connection probability of the stochastic block model is high.</p>
</section>
<section id="clustering" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="clustering"><span class="header-section-number">4.3</span> Clustering</h2>
<p>In order to study clustering performance, we compared the partitions estimated by MGLasso, Hierarchical Agglomerative Clustering (HAC) with Ward’s distance and K-means to the true partition in a stochastic block model framework. Euclidean distances between variables are used for HAC and K-means. The criterion used for the comparison is the adjusted Rand index (ARI). We studied the influence of the correlation level inside clusters on the clustering performances through two different parameters: <span class="math inline">\rho \in \{ 0.1, 0.3 \}</span>; the vector of cluster proportions is fixed at <span class="math inline">\mathbf \pi = (1/5, \dots, 1/5)</span>. Hundred Gaussian data sets were then simulated for each configuration (<span class="math inline">\rho</span>, <span class="math inline">n/p</span> fixed).The optimal sparsity penalty for MGLasso was chosen by the Stability Approach to Regularization Selection (StARS) method <span class="citation" data-cites="Liu2010">(<a href="#ref-Liu2010" role="doc-biblioref">Liu, Roeder, and Wasserman 2010</a>)</span>. In practice, we estimated a stability-like parameter in a sample of graphs simulated via the stochastic block model. This estimation of edge variability was then used as the threshold for the StARS method. The parameter <span class="math inline">\lambda_2</span> has been varied.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"./data/rand_dt_lower_cor_sbm.RData"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_res</span>(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  dt_rand,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">crit_ =</span> <span class="st">"rand"</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">ncluster_ =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>),</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">cor_ =</span> <span class="fl">0.25</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">np_ =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">main =</span> <span class="st">""</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-ari-low-cor" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-ari-low-cor-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;8: Boxplots of Adjusted Rand Indices for the stochastic block model with <span class="math inline">5</span> classes and <span class="math inline">p=40</span> variables for a correlation level <span class="math inline">\rho=0.1</span>. The number of estimated clusters <span class="math inline">\{5,10,15,20\}</span> vary alongside the ratio <span class="math inline">\frac{n}{p}\in \{0.5,1,2\}</span>. Within each panel, the boxplots of ARI between true partition (with <span class="math inline">5</span> classes) and estimated clustering partitions on <span class="math inline">100</span> simulated datasets for <span class="math inline">k</span>-means (blue), hierarchical agglomerative clustering (yellow), and MGLasso (brown) methods are plotted against the ratio <span class="math inline">\frac{n}{p}.</span> The cluster assignments of MGLasso are computed from a distance between estimated regression vectors for a given value of <span class="math inline">\lambda_2.</span> Missing boxplots for MGLasso thus mean computed partitions in the grid of values of <span class="math inline">\lambda_2</span> do not yield the fixed number of clusters. The higher the ARI values, the better the estimated clustering partition is.</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"./data/rand_dt_higher_cor_sbm.RData"</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_res</span>(</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  dt_rand,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">crit_ =</span> <span class="st">"rand"</span>,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">ncluster_ =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>),</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">cor_ =</span> <span class="fl">0.95</span>,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">np_ =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">main =</span> <span class="st">""</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-ari-high-cor" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-ari-high-cor-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;9: Boxplots of Adjusted Rand Indices for the stochastic block model with <span class="math inline">5</span> classes and <span class="math inline">p=40</span> variables for a correlation level <span class="math inline">\rho=0.3</span>. The number of estimated clusters <span class="math inline">\{5,10,15,20\}</span> vary alongside the ratio <span class="math inline">\frac{n}{p}\in \{0.5,1,2\}</span>. Within each panel, the boxplots of ARI between true partition (with <span class="math inline">5</span> classes) and estimated clustering partitions on <span class="math inline">100</span> simulated datasets for <span class="math inline">k</span>-means (blue), hierarchical agglomerative clustering (yellow), and MGLasso (brown) methods are plotted against the ratio <span class="math inline">\frac{n}{p}.</span> The cluster assignments of MGLasso are computed from a distance between estimated regression vectors for a given value of <span class="math inline">\lambda_2.</span> The higher the ARI values, the better the estimated clustering partition is.</figcaption>
</figure>
</div>
</div>
</div>
<p>The expected empirical evidence that MGLasso would work reasonably well for strongly correlated variables is somehow highlighted in <a href="#fig-ari-low-cor">Figure&nbsp;8</a> and <a href="#fig-ari-high-cor">Figure&nbsp;9</a>. The performances of MGLasso slightly improve when going from <a href="#fig-ari-low-cor">Figure&nbsp;8</a> to <a href="#fig-ari-high-cor">Figure&nbsp;9</a>, which corresponds to correlation levels of 0.1 and 0.3 between variables belonging to the same block, respectively. We observe the same trend for the HAC and the k-means. Compared to these two approaches, the MGLasso presents the lowest values of adjusted Rand indices, thus suggesting a lower quality of clustering. It should be noted that the performance of MGLasso can be sensitive to the selection of the Lasso penalty parameter and the threshold fixed to determine clusters’ fusion. In practice, this fusion threshold is varied in a grid of values close to zero and lower than <span class="math inline">10^{-3}</span>. The value leading to the maximum number of intermediate clusters in the clustering path is chosen. Using non-trivial weights could also improve the overall performance of MGLasso.</p>
<p><revision> During the revision of this paper, an interesting question was raised regarding the behavior of the algorithm in a phylogenetic-based model. To investigate this, extensive numerical experiments were conducted on a phylogenetic-based model that evaluates only clustering performances. The results showed that the MGLASSO algorithm’s performance improves, and the method performs as well as some state-of-the-art clustering approaches, including vanilla convex clustering and spectral clustering. In phylogenetic-based models, adjusted Rand indices can be computed between the estimated partition with <span class="math inline">k</span> clusters and the true partition in <span class="math inline">k</span> clusters computed from the tree used for the simulation procedure. This differs from the clustering performance evaluation scheme applied in the stochastic block model, where the true partition is considered fixed. </revision></p>
</section>
</section>
<section id="applications" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Applications</h1>
<p>To illustrate the proposed simultaneous graphs and clusters inference approach, we present analyses where the MGLasso model is applied to microbial association data for the study of multiscale networks between operational taxonomic units and to transcriptomic and methylation genotypes for multi-omics data integration.</p>
<section id="application-to-microbial-associations-in-gut-data" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="application-to-microbial-associations-in-gut-data"><span class="header-section-number">5.1</span> Application to microbial associations in gut data</h2>
<p>We analyze microbial associations in human gut microbiome data acquired from the round <span class="math inline">1</span> of the American Gut Project (AGP, <span class="citation" data-cites="mcdonald2018american">McDonald et al. (<a href="#ref-mcdonald2018american" role="doc-biblioref">2018</a>)</span>) for <span class="math inline">p = 127</span> operational taxonomic units (OTUs) and <span class="math inline">n = 289</span> individuals samples. The count of microbial OTUs is an indicator of the abundance of underlying microbial populations. Here, we investigate the network and clustering structures of the OTUs for different levels of granularity on the processed data included in the SpiecEasi R package (see <span class="citation" data-cites="Kurtz2015">Kurtz (<a href="#ref-Kurtz2015" role="doc-biblioref">2015</a>)</span> for details). The data is first normalized to have a unit-sum per sample and to remove biases. Then, a centered log-ratio <span class="citation" data-cites="aitchison1982statistical">(clr, <a href="#ref-aitchison1982statistical" role="doc-biblioref">Aitchison 1982</a>)</span> transformation with an added unit pseudo-count is applied to come back to an unconstrained Euclidean space. For fitting the MGLasso model, we select the Lasso penalty parameter <span class="math inline">\lambda_1</span> via the StARS approach with threshold <span class="math inline">\upsilon = 0.05</span> and vary the fusion penalty <span class="math inline">\lambda_2</span> in the interval <span class="math inline">[0, 20]</span> with irregular steps. The CPU time taken for <span class="math inline">20</span> values of <span class="math inline">\lambda_2</span> is about <span class="math inline">8</span> hours with parallel evaluations on a computation cluster with as many cores as <span class="math inline">\lambda_2</span> values. The maximal number of iterations is set to <span class="math inline">10000</span> and the solver precision to <span class="math inline">0.01</span>.</p>
<p><old>We finally illustrate our new method of inferring the multiscale Gaussian graphical model, with an application to the analysis of microbial associations in the American Gut Project. The data used are count data that have been previously normalized by applying the log-centered ratio technique as used in <span class="citation" data-cites="Kurtz2015">(<a href="#ref-Kurtz2015" role="doc-biblioref">Kurtz 2015</a>)</span>. After some filtering steps <span class="citation" data-cites="Kurtz2015">(<a href="#ref-Kurtz2015" role="doc-biblioref">Kurtz 2015</a>)</span> on the operational taxonomic units (OTUs) counts (removed if present in less than <span class="math inline">37\%</span> of the samples) and the samples (removed if sequencing depth below 2700), the top OTUs are grouped in a dataset composed of <span class="math inline">n = 289</span> for <span class="math inline">127</span> OTUs. <old> As a preliminary analysis, we perform a hierarchical agglomerative clustering (HAC) on the OTUs, which allows us to identify four significant groups. The correlation matrix of the dataset is given in fig-emp-cor; variables have been rearranged according to the HAC partition.</old></old></p>
<p>Using these settings, we compute a clustering path of the solutions and estimated graphs for <span class="math inline">5</span> values of <span class="math inline">\lambda_2</span> corresponding to <span class="math inline">5</span> different clusters partitions. The <a href="#fig-clusterpath">Figure&nbsp;10</a> shows how the predicted <span class="math inline">\hat{\boldsymbol X}</span> evolves through <span class="math inline">\lambda_2.</span> The <span class="math inline">\hat{\boldsymbol X}</span> are computed from estimated centroids <span class="math inline">\hat{\boldsymbol \beta}</span> and projected onto two principal components of the original data. The path is not always agglomerative, but the clusters’ splits observed ensure optimal solutions.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(SpiecEasi)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(colorspace)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggrepel)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>path_data <span class="ot">&lt;-</span> <span class="st">"./data/"</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">paste0</span>(path_data, <span class="st">"mgl_amgut_rev_l2_seq0to1_20val.RData"</span>))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">paste0</span>(path_data, <span class="st">"mgl_amgut_rev_l2_seq1to20_20val.RData"</span>))</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">paste0</span>(path_data, <span class="st">"mgl_amgut_rev_l2_seq0to4_20val.RData"</span>))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">paste0</span>(path_data, <span class="st">"amgut1.filt.phy.rda"</span>)) <span class="co"># Data for the phylum taxonomic classifier loaded from supplementary files of the SpiecEasi package. See https://github.com/zdk123/SpiecEasi/blob/master/data/amgut2.filt.phy.rda</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">paste0</span>(path_data, <span class="st">"amgut1.filt.rda"</span>))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>amgut1.filt <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">clr</span>(amgut1.filt <span class="sc">+</span> <span class="dv">1</span> , <span class="dv">1</span>))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>rank2_table <span class="ot">&lt;-</span> <span class="fu">table</span>(taxas[,<span class="st">"Rank2"</span>])</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>col_leaves <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">rep</span>(<span class="fu">rainbow_hcl</span>(<span class="dv">6</span>, <span class="at">c=</span><span class="dv">90</span>, <span class="at">l=</span><span class="dv">50</span>), <span class="at">times =</span> rank2_table))</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>plot_clusterpath <span class="ot">&lt;-</span> <span class="cf">function</span>(X, mglasso_res, <span class="at">colnames_ =</span> <span class="cn">NULL</span>, max.overlaps, <span class="at">cut_k_vars =</span> <span class="dv">5</span>, colors_) {</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Initialisations</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>  df.paths <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">c</span>(),<span class="at">y=</span><span class="fu">c</span>(), <span class="at">group=</span><span class="fu">c</span>())</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>  nlevel <span class="ot">&lt;-</span> <span class="fu">length</span>(mglasso_res)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Principal component analysis</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>  svdX <span class="ot">&lt;-</span> <span class="fu">svd</span>(X)                <span class="do">## singular value decomposition</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>  pc <span class="ot">&lt;-</span> svdX<span class="sc">$</span>u[,<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>,drop<span class="ot">=</span><span class="cn">FALSE</span>] <span class="do">## singular vectors</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> cut_k_vars<span class="sc">:</span>nlevel) {</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    Beta <span class="ot">&lt;-</span> mglasso_res[[j]]<span class="sc">$</span>selected_Theta</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    Xpred <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>p, <span class="cf">function</span>(i){X <span class="sc">%*%</span> Beta[i,]})</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    pcs <span class="ot">&lt;-</span> <span class="fu">t</span>(pc)<span class="sc">%*%</span>Xpred</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> pcs[<span class="dv">1</span>,]</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> pcs[<span class="dv">2</span>,]</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>pcs[<span class="dv">1</span>,], <span class="at">y=</span>pcs[<span class="dv">2</span>,], <span class="at">group=</span><span class="dv">1</span><span class="sc">:</span>p, <span class="at">Rank2 =</span> colors_)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    df.paths <span class="ot">&lt;-</span> <span class="fu">rbind</span>(df.paths,df)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>  <span class="co"># X_data &lt;- as.data.frame(t(X) %*% pc) ## PCA projections (scores)</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>  X_data <span class="ot">&lt;-</span> df.paths[<span class="dv">1</span><span class="sc">:</span>p,]</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>  <span class="co">#colnames(X_data) &lt;- c("x", "y")</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ifelse</span>(<span class="fu">is.null</span>(colnames_),</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>         X_data<span class="sc">$</span>Name <span class="ot">&lt;-</span> <span class="fu">colnames</span>(X),</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>         X_data<span class="sc">$</span>Name <span class="ot">&lt;-</span> colnames_)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> df.paths, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_path</span>(<span class="fu">aes</span>(<span class="at">group =</span> group,  <span class="at">colour =</span> Rank2), <span class="at">alpha =</span> <span class="fl">0.5</span>)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_text_repel</span>(<span class="at">data =</span> X_data,</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>                                <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">label =</span> Name),</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>                                <span class="at">max.overlaps =</span> max.overlaps)</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">data =</span> X_data, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">colour =</span> Rank2), <span class="at">size =</span> <span class="fl">1.5</span>)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">'Principal Component 3'</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">'Principal Component 4'</span>)</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="sc">+</span> <span class="fu">theme_bw</span>()</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_clusterpath</span>(amgut1.filt, <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3), <span class="at">max.overlaps =</span> <span class="dv">10</span>, <span class="at">cut_k_vars =</span> <span class="dv">1</span>, <span class="at">colors_ =</span> taxas[,<span class="st">"Rank2"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-clusterpath" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-clusterpath-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;10: Clustering path of the MGLasso convex clustering solutions on microbiome data with <span class="math inline">127</span> OTUs. The predicted data are projected onto the two principal components of the original data, while the fusion penalty varies. As <span class="math inline">\lambda_2</span> increases, it reaches a value for which all the estimated centroids are equal; thus, the branches of the path converge to a unique point in the center of the graph. OTUs are colored according to their phylum classification. The path displays abrupt merges. The pure cluster on the graph’s left side (down) corresponds to the phylum Bacteroidetes.</figcaption>
</figure>
</div>
</div>
</div>
<p>The <a href="#fig-meta-graphs">Figure&nbsp;11</a> displays graphs and clusters for different levels of granularity: <span class="math inline">127</span>, <span class="math inline">63</span>, <span class="math inline">31</span>, <span class="math inline">15</span> and <span class="math inline">2</span> clusters. For computing the clusters’ assignment of nodes, the fusion threshold has been set to <span class="math inline">\epsilon_{fuse} = 0.001</span>. Variables that belong to the same cluster share the same neighborhood; thus, the neighboring information is summarized into a single variable representative of the group. The subfigures show graphs at multiple levels of granularity which are built on the meta-variables or representative variables.</p>
<div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(igraph)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(phyloseq)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>all_clusters_partition <span class="ot">&lt;-</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(<span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3), <span class="cf">function</span>(x)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">get_clusters_mgl</span>(x<span class="sc">$</span>selected_Theta))</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>all_num_clusters <span class="ot">&lt;-</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unlist</span>(<span class="fu">lapply</span>(all_clusters_partition, <span class="cf">function</span>(x)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">length</span>(<span class="fu">unique</span>(x))))</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">127</span>)[<span class="dv">1</span>]</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">extract_meta</span>(<span class="at">clusters =</span> all_clusters_partition[[ind]])</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]<span class="sc">$</span>selected_Theta[vec, vec]</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span> <span class="fu">symmetrize</span>(metaG)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj2igraph</span>(metaG, <span class="at">vertex.attr =</span> <span class="fu">list</span>(<span class="at">name =</span> <span class="fu">taxa_names</span>(amgut1.filt.phy)[vec]))</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(metaG)<span class="sc">$</span>weight <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">E</span>(metaG)<span class="sc">$</span>weight)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> <span class="fu">cbind</span>(clusters, taxas)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> taxas[vec, ]</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_network</span>(</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>  metaG,</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>  taxas,</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"taxa"</span>,</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">layout.method =</span> layout_with_fr,</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="st">"Rank2"</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">63</span>)[<span class="dv">1</span>]</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">extract_meta</span>(<span class="at">clusters =</span> all_clusters_partition[[ind]])</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]<span class="sc">$</span>selected_Theta[vec, vec]</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span> <span class="fu">symmetrize</span>(metaG)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj2igraph</span>(metaG, <span class="at">vertex.attr =</span> <span class="fu">list</span>(<span class="at">name =</span> <span class="fu">taxa_names</span>(amgut1.filt.phy)[vec]))</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(metaG)<span class="sc">$</span>weight <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">E</span>(metaG)<span class="sc">$</span>weight)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> <span class="fu">cbind</span>(clusters, taxas)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> taxas[vec, ]</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_network</span>(</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>  metaG,</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>  taxas,</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"taxa"</span>,</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>  <span class="at">layout.method =</span> layout_with_fr,</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="st">"Rank2"</span></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">31</span>)[<span class="dv">1</span>]</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">extract_meta</span>(<span class="at">clusters =</span> all_clusters_partition[[ind]])</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]<span class="sc">$</span>selected_Theta[vec, vec]</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span> <span class="fu">symmetrize</span>(metaG)</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj2igraph</span>(metaG, <span class="at">vertex.attr =</span> <span class="fu">list</span>(<span class="at">name =</span> <span class="fu">taxa_names</span>(amgut1.filt.phy)[vec]))</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(metaG)<span class="sc">$</span>weight <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">E</span>(metaG)<span class="sc">$</span>weight)</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> <span class="fu">cbind</span>(clusters, taxas)</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> taxas[vec, ]</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_network</span>(</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>  metaG,</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>  taxas,</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"taxa"</span>,</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>  <span class="at">layout.method =</span> layout_with_dh,</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="st">"Rank2"</span></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">15</span>)[<span class="dv">1</span>]</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">extract_meta</span>(<span class="at">clusters =</span> all_clusters_partition[[ind]])</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]<span class="sc">$</span>selected_Theta[vec, vec]</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span> <span class="fu">symmetrize</span>(metaG)</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj2igraph</span>(metaG, <span class="at">vertex.attr =</span> <span class="fu">list</span>(<span class="at">name =</span> <span class="fu">taxa_names</span>(amgut1.filt.phy)[vec]))</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(metaG)<span class="sc">$</span>weight <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">E</span>(metaG)<span class="sc">$</span>weight)</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> <span class="fu">cbind</span>(clusters, taxas)</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> taxas[vec, ]</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_network</span>(</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>  metaG,</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>  taxas,</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"taxa"</span>,</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>  <span class="at">layout.method =</span> layout_with_dh,</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="st">"Rank2"</span></span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">2</span>)[<span class="dv">1</span>]</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">extract_meta</span>(<span class="at">clusters =</span> all_clusters_partition[[ind]])</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]<span class="sc">$</span>selected_Theta[vec, vec]</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span> <span class="fu">symmetrize</span>(metaG)</span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj2igraph</span>(metaG, <span class="at">vertex.attr =</span> <span class="fu">list</span>(<span class="at">name =</span> <span class="fu">taxa_names</span>(amgut1.filt.phy)[vec]))</span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(metaG)<span class="sc">$</span>weight <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">E</span>(metaG)<span class="sc">$</span>weight)</span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> <span class="fu">cbind</span>(clusters, taxas)</span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> taxas[vec, ]</span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_network</span>(</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>  metaG,</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>  taxas,</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"taxa"</span>,</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a>  <span class="at">layout.method =</span> layout_with_dh,</span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="st">"Rank2"</span></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-meta-graphs" class="cell quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-meta-graphs-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-meta-graphs-1.svg" class="img-fluid figure-img" data-ref-parent="fig-meta-graphs"></p>
<figcaption class="figure-caption">(a) 127 clusters graph</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-meta-graphs-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-meta-graphs-2.svg" class="img-fluid figure-img" data-ref-parent="fig-meta-graphs"></p>
<figcaption class="figure-caption">(b) Meta-variables graph with 63 clusters</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-meta-graphs-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-meta-graphs-3.svg" class="img-fluid figure-img" data-ref-parent="fig-meta-graphs"></p>
<figcaption class="figure-caption">(c) Meta-variables graph with 31 clusters</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-meta-graphs-4" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-meta-graphs-4.svg" class="img-fluid figure-img" data-ref-parent="fig-meta-graphs"></p>
<figcaption class="figure-caption">(d) Meta-variables graph with 15 clusters</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-meta-graphs-5" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-meta-graphs-5.svg" class="img-fluid figure-img" data-ref-parent="fig-meta-graphs"></p>
<figcaption class="figure-caption">(e) Meta-variables graph with 2 clusters</figcaption>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;11: Estimated graphs at multiple levels of granularity. The first graph shows a network inferred when <span class="math inline">\lambda_2=0</span>.The number of clusters is equal to the number of OTUs. Increasing the fusion penalty makes it possible to uncover graphs built on the representative variable of each cluster. OTUs are colored according to their phylum taxonomic classifier. The number of clusters is computed from the regression vectors with a fixed fusion threshold.</figcaption><p></p>
</figure>
</div>
</div>
<p>To assess the relevance of the inferred clusters, they are compared to known taxonomic ranks (phylum, class, order, family, genera, or species). The phylum classification is used. For example, for a clustering partition in <span class="math inline">2</span> groups, the MGLasso clustering partition is composed of <span class="math inline">120</span> variables versus <span class="math inline">7</span> variables. The cluster <span class="math inline">2</span> is exclusively composed of OTUs belonging to the Proteobacteria phylum. The cluster <span class="math inline">1</span> also contains Proteobacteria OTUs, so those identified in cluster <span class="math inline">2</span> might share more intimate characteristics.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">2</span>)[<span class="dv">1</span>]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>taxonomic.classification <span class="ot">&lt;-</span> taxas[,<span class="st">"Rank2"</span>]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="do">## remove "p__" characters in species names</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>taxonomic.classification <span class="ot">&lt;-</span> <span class="fu">sub</span>(<span class="st">"p__"</span>, <span class="st">""</span>, taxonomic.classification)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>tables<span class="sc">::</span><span class="fu">as.tabular</span>(<span class="fu">table</span>(clusters, taxonomic.classification))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<table class="Rtable table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header center">
<th data-quarto-table-cell-role="th">&nbsp;</th>
<th colspan="6" data-quarto-table-cell-role="th">taxonomic.classification</th>
</tr>
<tr class="odd center">
<th data-quarto-table-cell-role="th">clusters</th>
<th data-quarto-table-cell-role="th">Actinobacteria</th>
<th data-quarto-table-cell-role="th">Bacteroidetes</th>
<th data-quarto-table-cell-role="th">Firmicutes</th>
<th data-quarto-table-cell-role="th">Proteobacteria</th>
<th data-quarto-table-cell-role="th">Tenericutes</th>
<th data-quarto-table-cell-role="th">Verrucomicrobia</th>
</tr>
</thead>
<tbody>
<tr class="odd center">
<td class="left" data-quarto-table-cell-role="th">1</td>
<td>2</td>
<td>27</td>
<td>76</td>
<td>13</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even center">
<td class="left" data-quarto-table-cell-role="th">2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>7</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>


</div>
</div>
<p>Adjusted Rand indices are not calculated for comparisons as the unitary weights in the convex clustering problem can be suboptimal. The abundance of OTUs belonging to cluster <span class="math inline">1</span>, mainly composed of Bacteroidetes and Firmicutes phyla, is seemingly dependent on the abundance of OTUS in cluster <span class="math inline">2</span>, i.e., Proteobacteria phylum.</p>
</section>
<section id="application-to-methylation-and-transcriptomic-genotypes-in-poplar" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="application-to-methylation-and-transcriptomic-genotypes-in-poplar"><span class="header-section-number">5.2</span> Application to methylation and transcriptomic genotypes in poplar</h2>
<p>Next, we investigate interactions between European poplar genotypes for transcriptomic and DNA methylation data extracted from the Evolutionary and functional impact of EPIgenetic variation in forest TREEs project <span class="citation" data-cites="maury2019epigenetics">(EPITREE, <a href="#ref-maury2019epigenetics" role="doc-biblioref">Maury et al. 2019</a>)</span>. The analysis was purposefully applied to the samples and not the genes in order to highlight the MGLasso clustering performance and show some potential relationships between DNA methylation and gene expression levels for some genotypes.</p>
<!-- Classic correlation approaches can lead to spurious relationships between
variables. Through the gaussian graphical framework of MGLasso, one can focus on
the conditional dependency structure which gets rid of confusion effects. We
refer to @akalin2020computational for a broader definition of the central dogma
of molecular biology (DNA-RNA-proteins). -->
<p>Poplar (<em>Populus</em>) is often used as a model tree for the study of drought response. Natural populations of black poplars (<em>Populus nigra</em>) have been planted in common gardens in France, Italy, and Germany (see <a href="#fig-context-epitree">Figure&nbsp;12</a>) with control on some environmental variables such as water availability <span class="citation" data-cites="sow2018narrow">(<a href="#ref-sow2018narrow" role="doc-biblioref">Sow et al. 2018</a>)</span>. The poplar has economic importance and is one of the most endangered species as a result of global climate change. The drought response can be studied via DNA methylation, which is a necessary process in plant development and response to environmental variations <span class="citation" data-cites="amaral2020advances">(<a href="#ref-amaral2020advances" role="doc-biblioref">Amaral et al. 2020</a>)</span>. It consists of the addition of a Methyl group to a cytosine (C) in the genome and occurs in three contexts (CG, CHG, and CHH, where H <span class="math inline">\in \{ A, C, T\}</span>). Methylation can be measured on two regions of the gene. Methylation in promoters is linked to gene silencing, and methylation in the body of the gene can be related to tissue-specific expression or alternative splicing <span class="citation" data-cites="sow2019role">(<a href="#ref-sow2019role" role="doc-biblioref">Sow 2019</a>)</span>.</p>

<!-- Epigenetic is the study of heritable changes which are not the result of a
modification in the DNA sequence [@plomion2016forest]. Epigenetic marks in
forest trees can be studied via  -->
<div id="fig-context-epitree" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./figures/peuplier-noir-Christian-Fischer.jpeg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Black poplar (C. Fischer Wikimedia)</figcaption>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;12: <img src="./figures/carte-genotypes.png" class="img-fluid figure-img" alt="Map of genotypes"></figcaption><p></p>
</figure>
</div>
<p>The collected DNA methylation and expression data are counts data. Details on the plant material and experimental design can be found in <span class="citation" data-cites="sow2019role">Sow (<a href="#ref-sow2019role" role="doc-biblioref">2019</a>)</span> and <span class="citation" data-cites="chateigner2020gene">Chateigner et al. (<a href="#ref-chateigner2020gene" role="doc-biblioref">2020</a>)</span>. The transcriptomic data were measured via RNA-Seq and normalized using Trimmed Mean of M-Values combined with a Best linear unbiased predictor (BLUP) correction as described in <span class="citation" data-cites="chateigner2020gene">Chateigner et al. (<a href="#ref-chateigner2020gene" role="doc-biblioref">2020</a>)</span>. The methylation data were measured through whole-genome bisulfite sequencing (WGBS) and are normalized via the read per density approach then passed to a logarithm function <span class="math inline">log_2(x+1)</span> with <span class="math inline">x \in \mathbb R</span>. For each one of the <span class="math inline">10</span> populations (see <a href="#fig-context-epitree">Figure&nbsp;12</a>), DNA methylation in CG, CHG, and CHH contexts for promoters and gene-body and RNA sequencing data are observed on genotypes. A mean measure is computed from two replicates per population. The analysis has been restricted to a set of <span class="math inline">151</span> target genes which explains the most variability in the omics data and the subsequent number of samples from different omic variables, which is <span class="math inline">70.</span></p>
<p>The MGLasso model is fitted with fusion penalty values chosen in <span class="math inline">[0, 30.94]</span> and a Lasso penalty <span class="math inline">\lambda_1</span> parameter chosen via the StARS approach with threshold <span class="math inline">0.05</span>. In the resulting clustering path (see <a href="#fig-clusterpath-poplar">Figure&nbsp;13</a>), we can identify three distinct and coherent clusters, which are samples corresponding to gene expression genotypes, gene-body methylation samples, and gene promoter samples.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>mglasso_genot <span class="ot">&lt;-</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">readRDS</span>(<span class="fu">paste0</span>(path_data, <span class="st">"mgl_epit_sparse_geno.rds"</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>epit_sparse <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="fu">paste0</span>(path_data, <span class="st">"epit-spca-select.rds"</span>))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Shorten columns' names</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># To do: add colors to cluster path for known groups</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>names_epit <span class="ot">&lt;-</span> epit_sparse <span class="sc">%&gt;%</span> <span class="fu">colnames</span>()</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>cut_names <span class="ot">&lt;-</span> names_epit <span class="sc">%&gt;%</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sapply</span>(<span class="cf">function</span>(x)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">gsub</span>(<span class="st">"log2_rpd."</span>, <span class="st">""</span>, x)) <span class="sc">%&gt;%</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sapply</span>(<span class="cf">function</span>(x)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">gsub</span>(<span class="st">"new_"</span>, <span class="st">""</span>, x)) <span class="sc">%&gt;%</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.character</span>()</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="do">####</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>order_omics <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"exp"</span>, cut_names),</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"gbM.CG"</span>, cut_names),</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"gbM.CHG"</span>, cut_names),</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"gbM.CHH"</span>, cut_names),</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"prom.CG"</span>, cut_names),</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"prom.CHG"</span>, cut_names),</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"prom.CHH"</span>, cut_names))</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>col_leaves <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">rep</span>(<span class="fu">rainbow_hcl</span>(<span class="dv">7</span>, <span class="at">c=</span><span class="dv">90</span>, <span class="at">l=</span><span class="dv">50</span>), <span class="at">each =</span> <span class="dv">10</span>))</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>col_leaves <span class="ot">&lt;-</span> col_leaves[<span class="fu">order</span>(order_omics)]</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(col_leaves) <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="st">"RNA-Seq"</span> <span class="ot">=</span> <span class="st">"#0093A9"</span>,</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CpG-Body"</span> <span class="ot">=</span> <span class="st">"#00944F"</span>,</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CHG-Body"</span> <span class="ot">=</span> <span class="st">"#4473D7"</span>,</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CHH-Body"</span> <span class="ot">=</span> <span class="st">"#5D8400"</span>,</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CpG-Promoter"</span> <span class="ot">=</span> <span class="st">"#A86B00"</span>,</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CHG-Promoter"</span> <span class="ot">=</span> <span class="st">"#C03FBE"</span>,</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CHH-Promoter"</span> <span class="ot">=</span> <span class="st">"#CC476B"</span>)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="do">####</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>plot_clusterpath <span class="ot">&lt;-</span> <span class="cf">function</span>(X, mglasso_res, <span class="at">colnames_ =</span> <span class="cn">NULL</span>, max.overlaps, <span class="at">cut_k_vars =</span> <span class="dv">5</span>, colors_) {</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Initialisations</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>  df.paths <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">c</span>(),<span class="at">y=</span><span class="fu">c</span>(), <span class="at">group=</span><span class="fu">c</span>())</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>  nlevel <span class="ot">&lt;-</span> <span class="fu">length</span>(mglasso_res)</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Principal component analysis</span></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>  svdX <span class="ot">&lt;-</span> <span class="fu">svd</span>(X)                <span class="do">## singular value decomposition</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>  pc <span class="ot">&lt;-</span> svdX<span class="sc">$</span>u[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,drop<span class="ot">=</span><span class="cn">FALSE</span>] <span class="do">## singular vectors</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> cut_k_vars<span class="sc">:</span>nlevel) {</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>    Beta <span class="ot">&lt;-</span> mglasso_res[[j]]<span class="sc">$</span>selected_Theta</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>    Xpred <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>p, <span class="cf">function</span>(i){X <span class="sc">%*%</span> Beta[i,]})</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>    pcs <span class="ot">&lt;-</span> <span class="fu">t</span>(pc)<span class="sc">%*%</span>Xpred</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> pcs[<span class="dv">1</span>,]</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> pcs[<span class="dv">2</span>,]</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>    df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>pcs[<span class="dv">1</span>,], <span class="at">y=</span>pcs[<span class="dv">2</span>,], <span class="at">group=</span><span class="dv">1</span><span class="sc">:</span>p, <span class="at">Data =</span> colors_)</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>    df.paths <span class="ot">&lt;-</span> <span class="fu">rbind</span>(df.paths,df)</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>  <span class="co"># X_data &lt;- as.data.frame(t(X) %*% pc) ## PCA projections (scores)</span></span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>  X_data <span class="ot">&lt;-</span> df.paths[<span class="dv">1</span><span class="sc">:</span>p,]</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>  <span class="co">#colnames(X_data) &lt;- c("x", "y")</span></span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ifelse</span>(<span class="fu">is.null</span>(colnames_),</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>         X_data<span class="sc">$</span>Name <span class="ot">&lt;-</span> <span class="fu">colnames</span>(X),</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>         X_data<span class="sc">$</span>Name <span class="ot">&lt;-</span> colnames_)</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> df.paths, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_path</span>(<span class="fu">aes</span>(<span class="at">group =</span> group,  <span class="at">colour =</span> Data), <span class="at">alpha =</span> <span class="fl">0.5</span>)</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_text_repel</span>(<span class="at">data =</span> X_data,</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>                                <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">label =</span> Name),</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>                                <span class="at">max.overlaps =</span> max.overlaps)</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">data =</span> X_data, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">colour =</span> Data), <span class="at">size =</span> <span class="fl">1.5</span>)</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">'Principal Component 1'</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">'Principal Component 2'</span>)</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="sc">+</span> <span class="fu">theme_bw</span>()</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_clusterpath</span>(<span class="fu">as.matrix</span>(epit_sparse), mglasso_genot, cut_names, <span class="at">max.overlaps =</span> <span class="dv">20</span>, <span class="at">cut_k_vars =</span> <span class="dv">1</span>, <span class="at">colors_ =</span> col_leaves)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-clusterpath-poplar" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-clusterpath-poplar-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;13: Clustering path of solutions on DNA methylation and transcriptomic samples. The figure shows <span class="math inline">3</span> distinct clusters which correspond to omics data of different natures: transcriptomic (right), methylation on the promoter (bottom), and methylation on gene-body (top left).</figcaption>
</figure>
</div>
</div>
</div>
<p>The results of the MGLasso can also be represented in the expanded way where meta-variables are not computed from clusters. In <a href="#fig-graphpath-poplar">Figure&nbsp;14</a>, a focus is put on the effect of the fusion penalty. Clusters partitions are not presented. The higher the fusion penalty, variables are encouraged to share the same neighborhood structure. Note that an equivalent graph over meta-variables can be computed after choosing a fusion threshold as in <a href="#fig-meta-graphs">Figure&nbsp;11</a>.</p>
<div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot adjacency matrices for some levels </span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Selection based on network interpretability  </span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">#' symmetrize matrix of regression vectors pxp</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj_mat</span>(mglasso_genot<span class="sc">$</span><span class="st">`</span><span class="at">1</span><span class="st">`</span><span class="sc">$</span>selected_Theta[order_omics, order_omics]),</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj_mat</span>(mglasso_genot<span class="sc">$</span><span class="st">`</span><span class="at">2</span><span class="st">`</span><span class="sc">$</span>selected_Theta[order_omics, order_omics]),</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj_mat</span>(mglasso_genot<span class="sc">$</span><span class="st">`</span><span class="at">3</span><span class="st">`</span><span class="sc">$</span>selected_Theta[order_omics, order_omics]),</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj_mat</span>(mglasso_genot<span class="sc">$</span><span class="st">`</span><span class="at">4</span><span class="st">`</span><span class="sc">$</span>selected_Theta[order_omics, order_omics]),</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj_mat</span>(mglasso_genot<span class="sc">$</span><span class="st">`</span><span class="at">20</span><span class="st">`</span><span class="sc">$</span>selected_Theta[order_omics, order_omics]),</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-graphpath-poplar" class="cell quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-graphpath-poplar-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-graphpath-poplar-1.svg" class="img-fluid figure-img" data-ref-parent="fig-graphpath-poplar"></p>
<figcaption class="figure-caption">(a) Full graph with <span class="math inline">\lambda_2</span> = 0</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-graphpath-poplar-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-graphpath-poplar-2.svg" class="img-fluid figure-img" data-ref-parent="fig-graphpath-poplar"></p>
<figcaption class="figure-caption">(b) Full graph with <span class="math inline">\lambda_2</span> = 1.63</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-graphpath-poplar-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-graphpath-poplar-3.svg" class="img-fluid figure-img" data-ref-parent="fig-graphpath-poplar"></p>
<figcaption class="figure-caption">(c) Full graph with <span class="math inline">\lambda_2</span> = 3.26</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-graphpath-poplar-4" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-graphpath-poplar-4.svg" class="img-fluid figure-img" data-ref-parent="fig-graphpath-poplar"></p>
<figcaption class="figure-caption">(d) Full graph with <span class="math inline">\lambda_2</span> = 4.89</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-graphpath-poplar-5" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-202306-sanou-multiscale_glasso_files/figure-html/fig-graphpath-poplar-5.svg" class="img-fluid figure-img" data-ref-parent="fig-graphpath-poplar"></p>
<figcaption class="figure-caption">(e) Full graph with <span class="math inline">\lambda_2</span> = 30.94</figcaption>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;14: Adjacency matrices for different fusion penalty parameters. The first graph shows the inferred network when no fusion penalty is added to the model. In that graph, the first block of size <span class="math inline">10 \times 10</span> variables corresponds to RNA-Seq samples. The second sparser block of size <span class="math inline">30 \times 30</span> corresponds to gene-body DNA methylation data in the three methylation contexts. The last sparse block of the same size corresponds to promoter methylation. The edge bands suggest a relationship between DNA methylation measurements that belong to the same context. For example, the Loire methylation sample in the CpG context is likely related to the Loire samples in the CHG and CHH contexts. The graphs also suggest some relationships between expression and methylation for some natural populations. As the merging penalty increases, the blocks corresponding to the three methylation contexts merge first, then follow the upper left block corresponding to the expression data. For <span class="math inline">\lambda_2 = 30.94,</span> all natural populations merge into a single cluster and complete graph.</figcaption><p></p>
</figure>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion</h1>
<p>We proposed a new technique that combines Gaussian Graphical Model inference and hierarchical clustering called MGLasso. The method proceeds via convex optimization and minimizes the neighborhood selection objective penalized by a hybrid regularization combining a sparsity-inducing norm and a convex clustering penalty. We developed a complete numerical scheme to apply MGLasso in practice, with an optimization algorithm based on CONESTA and a model selection procedure. Our simulations results over synthetic and real datasets showed that MGLasso can perform better than GLasso in network support recovery in the presence of groups of correlated variables, and we illustrated the method with the analysis of microbial associations data and methylation mixed with transcriptomic data. The present work paves the way for future improvements: first, by incorporating prior knowledge through more flexible weighted regularization; second, by studying the theoretical properties of the method in terms of statistical guarantees for the MGLasso estimator. Moreover, the node-wise regression approach on which our method is based can be extended to a broader family of non-Gaussian distributions belonging to the exponential family as outlined by <span class="citation" data-cites="yang2012graphical">E. Yang et al. (<a href="#ref-yang2012graphical" role="doc-biblioref">2012</a>)</span>. Our MGLasso approach can be easily extended to non-Gaussian distributions belonging to the exponential family and mixed graphical models.</p>
</section>



<section id="bibliography" class="level1 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section id="appendix" class="level1 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Appendix</h2><div class="quarto-appendix-contents">

<p>The scripts to reproduce the simulations are available at <a href="https://github.com/computorg/published-202306-sanou-multiscale_glasso/tree/main/scripts/simulation-experiments" class="uri">https://github.com/computorg/published-202306-sanou-multiscale_glasso/tree/main/scripts/simulation-experiments</a>.</p>
</div></section><section id="acknowledgments" class="level1 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Acknowledgments</h2><div class="quarto-appendix-contents">

<p>The authors would like to thank the Editors and referees for comments that led to substantial improvements in the manuscript.</p>
</div></section><section id="session-information" class="level1 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Session information</h2><div class="quarto-appendix-contents">

<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>R version 4.2.2 (2022-10-31)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 22.04.2 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so

locale:
 [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       
 [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   
 [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          
[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base     

other attached packages:
[1] igraph_1.4.2     phyloseq_1.42.0  ggrepel_0.9.3    colorspace_2.1-0
[5] SpiecEasi_1.1.2  ghibli_0.3.3     ggplot2_3.4.2    mglasso_0.1.3   

loaded via a namespace (and not attached):
  [1] simone_1.0-4           nlme_3.1-162           bitops_1.0-7          
  [4] blockmodels_1.1.5      httr_1.4.6             GenomeInfoDb_1.34.9   
  [7] tools_4.2.2            vegan_2.6-4            utf8_1.2.3            
 [10] R6_2.5.1               mgcv_1.8-42            BiocGenerics_0.44.0   
 [13] permute_0.9-7          rhdf5filters_1.10.1    ade4_1.7-22           
 [16] withr_2.5.0            tidyselect_1.2.0       Exact_3.2             
 [19] compiler_4.2.2         glmnet_4.1-7           cli_3.6.1             
 [22] Biobase_2.58.0         expm_0.999-7           prismatic_1.1.1       
 [25] labeling_0.4.2         scales_1.2.1           mvtnorm_1.1-3         
 [28] tables_0.9.10          proxy_0.4-27           stringr_1.5.0         
 [31] digest_0.6.31          rmarkdown_2.21         XVector_0.38.0        
 [34] pkgconfig_2.0.3        htmltools_0.5.5        fastmap_1.1.1         
 [37] rlang_1.1.1            readxl_1.4.2           rstudioapi_0.14       
 [40] huge_1.3.5             VGAM_1.1-8             shape_1.4.6           
 [43] farver_2.1.1           generics_0.1.3         jsonlite_1.8.4        
 [46] dplyr_1.1.2            RCurl_1.98-1.12        magrittr_2.0.3        
 [49] GenomeInfoDbData_1.2.9 biomformat_1.26.0      Matrix_1.5-4          
 [52] Rhdf5lib_1.20.0        Rcpp_1.0.10            DescTools_0.99.49     
 [55] munsell_0.5.0          S4Vectors_0.36.2       fansi_1.0.4           
 [58] ape_5.7-1              reticulate_1.28        lifecycle_1.0.3       
 [61] stringi_1.7.12         yaml_2.3.7             MASS_7.3-59           
 [64] rootSolve_1.8.2.3      zlibbioc_1.44.0        rhdf5_2.42.1          
 [67] plyr_1.8.8             grid_4.2.2             parallel_4.2.2        
 [70] crayon_1.5.2           lmom_2.9               lattice_0.21-8        
 [73] Biostrings_2.66.0      splines_4.2.2          multtest_2.54.0       
 [76] capushe_1.1.1          knitr_1.42             pillar_1.9.0          
 [79] boot_1.3-28.1          pulsar_0.3.10          gld_2.6.6             
 [82] reshape2_1.4.4         codetools_0.2-18       stats4_4.2.2          
 [85] glue_1.6.2             evaluate_0.20          data.table_1.14.8     
 [88] renv_0.17.3            BiocManager_1.30.20    png_0.1-8             
 [91] vctrs_0.6.2            foreach_1.5.2          cellranger_1.1.0      
 [94] gtable_0.3.3           xfun_0.39              e1071_1.7-13          
 [97] class_7.3-20           survival_3.4-0         tibble_3.2.1          
[100] iterators_1.0.14       IRanges_2.32.0         cluster_2.1.4         </code></pre>
</div>
</div>
<!-- -->

</div></section><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">Bibliography</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-aitchison1982statistical" class="csl-entry" role="listitem">
Aitchison, John. 1982. <span>“The Statistical Analysis of Compositional Data.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 44 (2): 139–60.
</div>
<div id="ref-akaike1998information" class="csl-entry" role="listitem">
Akaike, Hirotogu. 1998. <span>“Information Theory and an Extension of the Maximum Likelihood Principle.”</span> In <em>Selected Papers of Hirotugu Akaike</em>, 199–213. Springer.
</div>
<div id="ref-amaral2020advances" class="csl-entry" role="listitem">
Amaral, Joana, Zoé Ribeyre, Julien Vigneaud, Mamadou Dia Sow, Régis Fichot, Christian Messier, Gloria Pinto, Philippe Nolet, and Stéphane Maury. 2020. <span>“Advances and Promises of Epigenetics for Forest Trees.”</span> <em>Forests</em> 11 (9): 976.
</div>
<div id="ref-Ambroise2009" class="csl-entry" role="listitem">
Ambroise, Christophe, Julien Chiquet, and Catherine Matias. 2009. <span>“<span class="nocase">Inferring sparse gaussian graphical models with latent structure</span>.”</span> <em>Electronic Journal of Statistics</em> 3 (0): 205–38. <a href="https://doi.org/10.1214/08-EJS314">https://doi.org/10.1214/08-EJS314</a>.
</div>
<div id="ref-Banerjee2008" class="csl-entry" role="listitem">
Banerjee, Onureena, Laurent El Ghaoui, and Alexandre d’Aspremont. 2008. <span>“Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data”</span> 9 (June): 485–516.
</div>
<div id="ref-baudry2012slope" class="csl-entry" role="listitem">
Baudry, Jean-Patrick, Cathy Maugis, and Bertrand Michel. 2012. <span>“Slope Heuristics: Overview and Implementation.”</span> <em>Statistics and Computing</em> 22 (2): 455–70.
</div>
<div id="ref-Beck2009" class="csl-entry" role="listitem">
Beck, Amir, and Marc Teboulle. 2009. <span>“A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems.”</span> <em>SIAM J. Imaging Sciences</em> 2 (January): 183–202. <a href="https://doi.org/10.1137/080716542">https://doi.org/10.1137/080716542</a>.
</div>
<div id="ref-bien2011sparse" class="csl-entry" role="listitem">
Bien, Jacob, and Robert J Tibshirani. 2011. <span>“Sparse Estimation of a Covariance Matrix.”</span> <em>Biometrika</em> 98 (4): 807–20.
</div>
<div id="ref-Boyd2011" class="csl-entry" role="listitem">
Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. 2011. <span>“Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.”</span> <em>Found. Trends Mach. Learn.</em> 3 (1): 1–122. <a href="https://doi.org/10.1561/2200000016">https://doi.org/10.1561/2200000016</a>.
</div>
<div id="ref-Buhlmann2012" class="csl-entry" role="listitem">
Bühlmann, Peter, Philipp Rütimann, Sara Van De Geer, and Cun-Hui Zhang. 2012. <span>“<span class="nocase">Correlated variables in regression: clustering and sparse estimation</span>.”</span>
</div>
<div id="ref-Cai2011" class="csl-entry" role="listitem">
Cai, Tony, Weidong Liu, and Xi Luo. 2011. <span>“A Constrained L1 Minimization Approach to Sparse Precision Matrix Estimation.”</span> <em>Journal of the American Statistical Association</em> 106 (494): 594–607. <a href="https://doi.org/10.1198/jasa.2011.tm10155">https://doi.org/10.1198/jasa.2011.tm10155</a>.
</div>
<div id="ref-chateigner2020gene" class="csl-entry" role="listitem">
Chateigner, Aurélien, Marie-Claude Lesage-Descauses, Odile Rogier, Véronique Jorge, Jean-Charles Leplé, Véronique Brunaud, Christine Paysant-Le Roux, et al. 2020. <span>“Gene Expression Predictions and Networks in Natural Populations Supports the Omnigenic Theory.”</span> <em>BMC Genomics</em> 21 (1): 1–16.
</div>
<div id="ref-chen2010graph" class="csl-entry" role="listitem">
Chen, Xi, Seyoung Kim, Qihang Lin, Jaime G Carbonell, and Eric P Xing. 2010. <span>“Graph-Structured Multi-Task Regression and an Efficient Optimization Method for General Fused Lasso.”</span> <em>arXiv Preprint arXiv:1005.3579</em>.
</div>
<div id="ref-Cheng2017" class="csl-entry" role="listitem">
Cheng, Lulu, Liang Shan, and Inyoung Kim. 2017. <span>“<span class="nocase">Multilevel Gaussian graphical model for multilevel networks</span>.”</span> <em>Journal of Statistical Planning and Inference</em> 190 (November): 1–14. <a href="https://doi.org/10.1016/j.jspi.2017.05.003">https://doi.org/10.1016/j.jspi.2017.05.003</a>.
</div>
<div id="ref-chi2015splitting" class="csl-entry" role="listitem">
Chi, Eric C, and Kenneth Lange. 2015. <span>“Splitting Methods for Convex Clustering.”</span> <em>Journal of Computational and Graphical Statistics</em> 24 (4): 994–1013.
</div>
<div id="ref-chiquet2011inferring" class="csl-entry" role="listitem">
Chiquet, Julien, Yves Grandvalet, and Christophe Ambroise. 2011. <span>“Inferring Multiple Graphical Structures.”</span> <em>Statistics and Computing</em> 21 (4): 537–53.
</div>
<div id="ref-chiquet2017fast" class="csl-entry" role="listitem">
Chiquet, Julien, Pierre Gutierrez, and Guillem Rigaill. 2017. <span>“Fast Tree Inference with Weighted Fusion Penalties.”</span> <em>Journal of Computational and Graphical Statistics</em> 26 (1): 205–16.
</div>
<div id="ref-chu2021adaptive" class="csl-entry" role="listitem">
Chu, Shuyu, Huijing Jiang, Zhengliang Xue, and Xinwei Deng. 2021. <span>“Adaptive Convex Clustering of Generalized Linear Models with Application in Purchase Likelihood Prediction.”</span> <em>Technometrics</em> 63 (2): 171–83.
</div>
<div id="ref-danaher2014joint" class="csl-entry" role="listitem">
Danaher, Patrick, Pei Wang, and Daniela M Witten. 2014. <span>“The Joint Graphical Lasso for Inverse Covariance Estimation Across Multiple Classes.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 76 (2): 373–97.
</div>
<div id="ref-degras2021sparse" class="csl-entry" role="listitem">
Degras, David. 2021. <span>“Sparse Group Fused Lasso for Model Segmentation: A Hybrid Approach.”</span> <em>Advances in Data Analysis and Classification</em> 15 (3): 625–71.
</div>
<div id="ref-Dempster1972" class="csl-entry" role="listitem">
Dempster, A. P. 1972. <span>“<span>Covariance Selection</span>.”</span> <em>Biometrics</em> 28 (1): 157. <a href="https://doi.org/10.2307/2528966">https://doi.org/10.2307/2528966</a>.
</div>
<div id="ref-Devijver2018" class="csl-entry" role="listitem">
Devijver, Emilie, and Mélina Gallopin. 2018. <span>“<span class="nocase">Block-Diagonal Covariance Selection for High-Dimensional Gaussian Graphical Models</span>.”</span> <em>Journal of the American Statistical Association</em> 113 (521): 306–14. <a href="https://doi.org/10.1080/01621459.2016.1247002">https://doi.org/10.1080/01621459.2016.1247002</a>.
</div>
<div id="ref-dondelinger2020joint" class="csl-entry" role="listitem">
Dondelinger, Frank, Sach Mukherjee, and Alzheimer’s Disease Neuroimaging Initiative. 2020. <span>“The Joint Lasso: High-Dimensional Regression for Group Structured Data.”</span> <em>Biostatistics</em> 21 (2): 219–35.
</div>
<div id="ref-erdHos1960evolution" class="csl-entry" role="listitem">
Erdős, Paul, Alfréd Rényi, et al. 1960. <span>“On the Evolution of Random Graphs.”</span> <em>Publ. Math. Inst. Hung. Acad. Sci</em> 5 (1): 17–60.
</div>
<div id="ref-Fan2016" class="csl-entry" role="listitem">
Fan, Jianqing, Yuan Liao, and Han Liu. 2016. <span>“An Overview of the Estimation of Large Covariance and Precision Matrices.”</span> <em>The Econometrics Journal</em> 19 (1): C1–32. https://doi.org/<a href="https://doi.org/10.1111/ectj.12061">https://doi.org/10.1111/ectj.12061</a>.
</div>
<div id="ref-fienberg1981categorical" class="csl-entry" role="listitem">
Fienberg, Stephen E, and Stanley S Wasserman. 1981. <span>“Categorical Data Analysis of Single Sociometric Relations.”</span> <em>Sociological Methodology</em> 12: 156–92.
</div>
<div id="ref-foygel2010extended" class="csl-entry" role="listitem">
Foygel, Rina, and Mathias Drton. 2010. <span>“Extended Bayesian Information Criteria for Gaussian Graphical Models.”</span> <em>arXiv Preprint arXiv:1011.6640</em>.
</div>
<div id="ref-Friedman2007" class="csl-entry" role="listitem">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2007. <span>“<span class="nocase">Sparse inverse covariance estimation with the graphical lasso</span>.”</span>
</div>
<div id="ref-ganguly2014" class="csl-entry" role="listitem">
Ganguly, Apratim, and Wolfgang Polonik. 2014. <span>“Local Neighborhood Fusion in Locally Constant Gaussian Graphical Models.”</span> <a href="https://arxiv.org/abs/1410.8766">https://arxiv.org/abs/1410.8766</a>.
</div>
<div id="ref-giraud2012graph" class="csl-entry" role="listitem">
Giraud, Christophe, Sylvie Huet, and Nicolas Verzelen. 2012. <span>“Graph Selection with GGMselect.”</span> <em>Statistical Applications in Genetics and Molecular Biology</em> 11 (3).
</div>
<div id="ref-hadjselem2018" class="csl-entry" role="listitem">
Hadj-Selem, Fouad, Tommy Lofstedt, Elvis Dohmatob, Vincent Frouin, Mathieu Dubois, Vincent Guillemot, and Edouard Duchesnay. 2018. <span>“<span class="nocase">Continuation of Nesterov’s Smoothing for Regression with Structured Sparsity in High-Dimensional Neuroimaging</span>.”</span> <em><span>IEEE Transactions on Medical Imaging</span></em> 2018. <a href="https://doi.org/10.1109/TMI.2018.2829802">https://doi.org/10.1109/TMI.2018.2829802</a>.
</div>
<div id="ref-hallac2015network" class="csl-entry" role="listitem">
Hallac, David, Jure Leskovec, and Stephen Boyd. 2015. <span>“Network Lasso: Clustering and Optimization in Large Graphs.”</span> In <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 387–96.
</div>
<div id="ref-Hocking2011" class="csl-entry" role="listitem">
Hocking, T., Jean-Philippe Vert, F. Bach, and Armand Joulin. 2011. <span>“Clusterpath: An Algorithm for Clustering Using Convex Fusion Penalties.”</span> In <em>ICML</em>.
</div>
<div id="ref-Hoefling2010" class="csl-entry" role="listitem">
Hoefling, Holger. 2010. <span>“A Path Algorithm for the Fused Lasso Signal Approximator.”</span> <em>Journal of Computational and Graphical Statistics</em> 19 (4): 984–1006. <a href="https://doi.org/10.1198/jcgs.2010.09208">https://doi.org/10.1198/jcgs.2010.09208</a>.
</div>
<div id="ref-Honorio2009" class="csl-entry" role="listitem">
Honorio, Jean, Dimitris Samaras, Nikos Paragios, Rita Goldstein, and Luis E Ortiz. 2009. <span>“Sparse and Locally Constant Gaussian Graphical Models.”</span> <em>Advances in Neural Information Processing Systems</em> 22: 745–53.
</div>
<div id="ref-Hsieh2014" class="csl-entry" role="listitem">
Hsieh, Cho-Jui, Mátyás A. Sustik, Inderjit S. Dhillon, and Pradeep Ravikumar. 2014. <span>“QUIC: Quadratic Approximation for Sparse Inverse Covariance Estimation.”</span> <em>Journal of Machine Learning Research</em> 15 (83): 2911–47. <a href="http://jmlr.org/papers/v15/hsieh14a.html">http://jmlr.org/papers/v15/hsieh14a.html</a>.
</div>
<div id="ref-Koller2009" class="csl-entry" role="listitem">
Koller, Daphne, and Nir Friedman. 2009. <span>“<span>Probabilistic Graphical Models: Principles</span>.”</span> <em>Italica</em> 51 (3): 327. <a href="https://doi.org/10.2307/478142">https://doi.org/10.2307/478142</a>.
</div>
<div id="ref-Kurtz2015" class="csl-entry" role="listitem">
Kurtz, Christian L. AND Miraldi, Zachary D. AND Müller. 2015. <span>“Sparse and Compositionally Robust Inference of Microbial Ecological Networks.”</span> <em>PLOS Computational Biology</em> 11 (May): 1–25. <a href="https://doi.org/10.1371/journal.pcbi.1004226">https://doi.org/10.1371/journal.pcbi.1004226</a>.
</div>
<div id="ref-Lauritzen1996" class="csl-entry" role="listitem">
Lauritzen, Steffen L. 1996. <em><span class="nocase">Graphical models</span></em>. Clarendon Press. <a href="https://global.oup.com/academic/product/graphical-models-9780198522195?cc=fr&amp;lang=en&amp;">https://global.oup.com/academic/product/graphical-models-9780198522195?cc=fr&amp;lang=en&amp;</a>.
</div>
<div id="ref-lin2020estimation" class="csl-entry" role="listitem">
Lin, Meixia, Defeng Sun, Kim-Chuan Toh, and Chengjing Wang. 2020. <span>“Estimation of Sparse Gaussian Graphical Models with Hidden Clustering Structure.”</span> <em>arXiv Preprint arXiv:2004.08115</em>.
</div>
<div id="ref-Lindsten2011" class="csl-entry" role="listitem">
Lindsten, F., H. Ohlsson, and L. Ljung. 2011. <span>“Clustering Using Sum-of-Norms Regularization: With Application to Particle Filter Output Computation.”</span> In <em>2011 IEEE Statistical Signal Processing Workshop (SSP)</em>, 201–4. <a href="https://doi.org/10.1109/SSP.2011.5967659">https://doi.org/10.1109/SSP.2011.5967659</a>.
</div>
<div id="ref-Liu2010" class="csl-entry" role="listitem">
Liu, Han, Kathryn Roeder, and Larry Wasserman. 2010. <span>“<a href="https://www.ncbi.nlm.nih.gov/pubmed/25152607"><span class="nocase">Stability approach to regularization selection (StARS) for high dimensional graphical models</span></a>.”</span> <em>Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010, NIPS 2010</em>, 1–14.
</div>
<div id="ref-maury2019epigenetics" class="csl-entry" role="listitem">
Maury, Stéphane, Régis Fichot, MD Sow, Alain Delaunay, I Le Jan, G Laskar, Marie-Claude Lesage Descauses, et al. 2019. <span>“Epigenetics in Forest Trees: Role in Plasticity, Adaptation and Potential Implications for Breeding in a Context of Climate Change (EPITREE).”</span>
</div>
<div id="ref-Mazumder2012" class="csl-entry" role="listitem">
Mazumder, Rahul, and Trevor Hastie. 2012. <span>“<span class="nocase">The graphical lasso: New insights and alternatives</span>.”</span> <em>Electronic Journal of Statistics</em> 6 (none): 2125–49. <a href="https://doi.org/10.1214/12-EJS740">https://doi.org/10.1214/12-EJS740</a>.
</div>
<div id="ref-mcdonald2018american" class="csl-entry" role="listitem">
McDonald, Daniel, Embriette Hyde, Justine W Debelius, James T Morton, Antonio Gonzalez, Gail Ackermann, Alexander A Aksenov, et al. 2018. <span>“American Gut: An Open Platform for Citizen Science Microbiome Research.”</span> <em>Msystems</em> 3 (3): e00031–18.
</div>
<div id="ref-Meinshausen2006" class="csl-entry" role="listitem">
Meinshausen, Nicolai, and Peter Bühlmann. 2006. <span>“<span class="nocase">High-dimensional graphs and variable selection with the Lasso</span>.”</span> <em>Annals of Statistics</em> 34 (3): 1436–62. <a href="https://doi.org/10.1214/009053606000000281">https://doi.org/10.1214/009053606000000281</a>.
</div>
<div id="ref-nesterov2005excessive" class="csl-entry" role="listitem">
Nesterov, Yu. 2005a. <span>“Excessive Gap Technique in Nonsmooth Convex Minimization.”</span> <em>SIAM Journal on Optimization</em> 16 (1): 235–49.
</div>
<div id="ref-nesterov2005smooth" class="csl-entry" role="listitem">
———. 2005b. <span>“Smooth Minimization of Non-Smooth Functions.”</span> <em>Mathematical Programming</em> 103 (1): 127–52.
</div>
<div id="ref-newman2001random" class="csl-entry" role="listitem">
Newman, Mark EJ, Steven H Strogatz, and Duncan J Watts. 2001. <span>“Random Graphs with Arbitrary Degree Distributions and Their Applications.”</span> <em>Physical Review E</em> 64 (2): 026118.
</div>
<div id="ref-Park2007" class="csl-entry" role="listitem">
Park, Mee Young, Trevor Hastie, and Robert Tibshirani. 2006. <span>“<span class="nocase">Averaged gene expressions for regression</span>.”</span> <em>Biostatistics</em> 8 (2): 212–27. <a href="https://doi.org/10.1093/biostatistics/kxl002">https://doi.org/10.1093/biostatistics/kxl002</a>.
</div>
<div id="ref-pelckmans2005convex" class="csl-entry" role="listitem">
Pelckmans, Kristiaan, Joseph De Brabanter, Johan AK Suykens, and Bart De Moor. 2005. <span>“Convex Clustering Shrinkage.”</span> In <em>PASCAL Workshop on Statistics and Optimization of Clustering Workshop</em>.
</div>
<div id="ref-Peng2009" class="csl-entry" role="listitem">
Peng, Jie, Pei Wang, Nengfeng Zhou, and Ji Zhu. 2009. <span>“Partial Correlation Estimation by Joint Sparse Regression Models.”</span> <em>Journal of the American Statistical Association</em> 104 (486): 735–46. <a href="https://doi.org/10.1198/jasa.2009.0126">https://doi.org/10.1198/jasa.2009.0126</a>.
</div>
<div id="ref-petry2011pairwise" class="csl-entry" role="listitem">
Petry, Sebastian, Claudia Flexeder, and Gerhard Tutz. 2011. <span>“Pairwise Fused Lasso.”</span>
</div>
<div id="ref-Rocha2008" class="csl-entry" role="listitem">
Rocha, Guilherme V., Peng Zhao, and Bin Yu. 2008. <span>“A Path Following Algorithm for Sparse Pseudo-Likelihood Inverse Covariance Estimation (SPLICE).”</span>
</div>
<div id="ref-Rothman2008" class="csl-entry" role="listitem">
Rothman, Adam J., Peter J. Bickel, Elizaveta Levina, and Ji Zhu. 2008. <span>“<span class="nocase">Sparse permutation invariant covariance estimation</span>.”</span> <em>Electronic Journal of Statistics</em> 2 (none): 494–515. <a href="https://doi.org/10.1214/08-EJS176">https://doi.org/10.1214/08-EJS176</a>.
</div>
<div id="ref-rudin1992nonlinear" class="csl-entry" role="listitem">
Rudin, Leonid I, Stanley Osher, and Emad Fatemi. 1992. <span>“Nonlinear Total Variation Based Noise Removal Algorithms.”</span> <em>Physica D: Nonlinear Phenomena</em> 60 (1-4): 259–68.
</div>
<div id="ref-schmidt2011convergence" class="csl-entry" role="listitem">
Schmidt, Mark, Nicolas Roux, and Francis Bach. 2011. <span>“Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization.”</span> <em>Advances in Neural Information Processing Systems</em> 24.
</div>
<div id="ref-schwarz1978estimating" class="csl-entry" role="listitem">
Schwarz, Gideon. 1978. <span>“Estimating the Dimension of a Model.”</span> <em>The Annals of Statistics</em>, 461–64.
</div>
<div id="ref-sow2019role" class="csl-entry" role="listitem">
Sow, Mamadou Dia. 2019. <span>“R<span>ô</span>le Fonctionnel de l’<span>é</span>pig<span>é</span>n<span>é</span>tique (m<span>é</span>thylation de l’ADN) Dans La r<span>é</span>ponse Du Peuplier <span class="nocase">à</span> Des Variations de Disponibilit<span>é</span> En Eau Du Sol.”</span> PhD thesis, Universit<span>é</span> d’Orl<span>é</span>ans.
</div>
<div id="ref-sow2018narrow" class="csl-entry" role="listitem">
Sow, Mamadou Dia, Vincent Segura, Sylvain Chamaillard, Véronique Jorge, Alain Delaunay, Clément Lafon-Placette, Régis Fichot, et al. 2018. <span>“Narrow-Sense Heritability and PST Estimates of DNA Methylation in Three Populus Nigra l. Populations Under Contrasting Water Availability.”</span> <em>Tree Genetics &amp; Genomes</em> 14 (5): 1–12.
</div>
<div id="ref-Tan2013" class="csl-entry" role="listitem">
Tan, Kean Ming, Daniela Witten, and Ali Shojaie. 2013. <span>“<span class="nocase">The Cluster Graphical Lasso for improved estimation of Gaussian graphical models</span>,”</span> July. <a href="http://arxiv.org/abs/1307.5339">http://arxiv.org/abs/1307.5339</a>.
</div>
<div id="ref-tibshirani1996" class="csl-entry" role="listitem">
Tibshirani, R. 1996. <span>“Regression Shrinkage and Selection via the Lasso.”</span> <em>Journal of the Royal Statistical Society (Series B)</em> 58: 267–88.
</div>
<div id="ref-tibshirani2005sparsity" class="csl-entry" role="listitem">
Tibshirani, Robert, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. 2005. <span>“Sparsity and Smoothness via the Fused Lasso.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (1): 91–108.
</div>
<div id="ref-yang2012graphical" class="csl-entry" role="listitem">
Yang, Eunho, Genevera Allen, Zhandong Liu, and Pradeep Ravikumar. 2012. <span>“Graphical Models via Generalized Linear Models.”</span> <em>Advances in Neural Information Processing Systems</em> 25.
</div>
<div id="ref-yang2015fused" class="csl-entry" role="listitem">
Yang, Sen, Zhaosong Lu, Xiaotong Shen, Peter Wonka, and Jieping Ye. 2015. <span>“Fused Multiple Graphical Lasso.”</span> <em>SIAM Journal on Optimization</em> 25 (2): 916–43.
</div>
<div id="ref-Yao2019" class="csl-entry" role="listitem">
Yao, Tianyi, and Genevera I. Allen. 2019. <span>“Clustered Gaussian Graphical Model via Symmetric Convex Clustering.”</span> In <em>2019 IEEE Data Science Workshop (DSW)</em>, 76–82. <a href="https://doi.org/10.1109/DSW.2019.8755774">https://doi.org/10.1109/DSW.2019.8755774</a>.
</div>
<div id="ref-Yuan2010" class="csl-entry" role="listitem">
Yuan, Ming. 2010. <span>“High Dimensional Inverse Covariance Matrix Estimation via Linear Programming.”</span> <em>Journal of Machine Learning Research</em> 11 (79): 2261–86. <a href="http://jmlr.org/papers/v11/yuan10b.html">http://jmlr.org/papers/v11/yuan10b.html</a>.
</div>
<div id="ref-Yuan2007" class="csl-entry" role="listitem">
Yuan, Ming, and Yi Lin. 2007. <span>“<span class="nocase">Model selection and estimation in the Gaussian graphical model</span>.”</span> <em>Biometrika</em> 94 (1): 19–35. <a href="https://doi.org/10.1093/biomet/asm018">https://doi.org/10.1093/biomet/asm018</a>.
</div>
<div id="ref-zhao2012huge" class="csl-entry" role="listitem">
Zhao, Tuo, Han Liu, Kathryn Roeder, John Lafferty, and Larry Wasserman. 2012. <span>“The Huge Package for High-Dimensional Undirected Graph Estimation in r.”</span> <em>The Journal of Machine Learning Research</em> 13 (1): 1059–62.
</div>
</div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{sanou2023,
  author = {Sanou, Edmond and Ambroise, Christophe and Robin, Geneviève},
  publisher = {Société Française de Statistique},
  title = {Inference of {Multiscale} {Gaussian} {Graphical} {Models}},
  journal = {Computo},
  date = {2023-06-28},
  url = {https://computo.sfds.asso.fr/published-202306-sanou-multiscale_glasso},
  doi = {10.57750/1f4p-7955},
  issn = {2824-7795},
  langid = {en},
  abstract = {Gaussian Graphical Models (GGMs) are widely used in
    high-dimensional data analysis to synthesize the interaction between
    variables. In many applications, such as genomics or image analysis,
    graphical models rely on sparsity and clustering to reduce
    dimensionality and improve performances. This paper explores a
    slightly different paradigm where clustering is not knowledge-driven
    but performed simultaneously with the graph inference task. We
    introduce a novel Multiscale Graphical Lasso (MGLasso) to improve
    networks interpretability by proposing graphs at different
    granularity levels. The method estimates clusters through a convex
    clustering approach -\/-\/- a relaxation of \$k\$-means, and
    hierarchical clustering. The conditional independence graph is
    simultaneously inferred through a neighborhood selection scheme for
    undirected graphical models. MGLasso extends and generalizes the
    sparse group fused lasso problem to undirected graphical models. We
    use continuation with Nesterov smoothing in a shrinkage-thresholding
    algorithm (CONESTA) to propose a regularization path of solutions
    along the group fused Lasso penalty, while the Lasso penalty is kept
    constant. Extensive experiments on synthetic data compare the
    performances of our model to state-of-the-art clustering methods and
    network inference models. Applications to gut microbiome data and
    poplar’s methylation mixed with transcriptomic data are presented.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-sanou2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Sanou, Edmond, Christophe Ambroise, and Geneviève Robin. 2023.
<span>“Inference of Multiscale Gaussian Graphical Models.”</span>
<em>Computo</em>, June. <a href="https://doi.org/10.57750/1f4p-7955">https://doi.org/10.57750/1f4p-7955</a>.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb16" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Inference of Multiscale Gaussian Graphical Models"</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> ""</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Edmond Sanou</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">    corresponding: true</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">    email: doedmond.sanou@univ-evry.fr </span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://desanou.github.io/</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université Paris-Saclay, CNRS, Univ Evry, Laboratoire de Mathématiques et Modélisation d'Evry</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">        url: http://www.math-evry.cnrs.fr/</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Christophe Ambroise</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">    email: christophe.ambroise@univ-evry.fr</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://cambroise.github.io/</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université Paris-Saclay, CNRS, Univ Evry, Laboratoire de Mathématiques et Modélisation d'Evry</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co">        url: http://www.math-evry.cnrs.fr/</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Geneviève Robin</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co">    email: genevievelrobin@gmail.com</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://genevieverobin.wordpress.com/</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université Paris-Saclay, CNRS, Univ Evry, Laboratoire de Mathématiques et Modélisation d'Evry</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="co">        url: http://www.math-evry.cnrs.fr/</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> June 28, 2023</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> &gt;+</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="co">  Gaussian Graphical Models (GGMs) are widely used in high-dimensional data </span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co">  analysis to synthesize the interaction between variables. In many </span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co">  applications, such as genomics or image analysis, graphical models </span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="co">  rely on sparsity and clustering to reduce dimensionality and improve </span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="co">  performances. This paper explores a slightly different paradigm where </span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="co">  clustering is not knowledge-driven but performed simultaneously with </span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a><span class="co">  the graph inference task. We introduce a novel Multiscale Graphical </span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="co">  Lasso (MGLasso) to improve networks interpretability by proposing graphs </span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a><span class="co">  at different granularity levels. The method estimates clusters through a </span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a><span class="co">  convex clustering approach --- a relaxation of $k$-means, and hierarchical </span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a><span class="co">  clustering. The conditional independence graph is simultaneously inferred </span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="co">  through a neighborhood selection scheme for undirected graphical models. </span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="co">  MGLasso extends and generalizes the sparse group fused lasso problem to </span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a><span class="co">  undirected graphical models. We use continuation with Nesterov smoothing </span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="co">  in a shrinkage-thresholding algorithm (CONESTA) to propose a regularization </span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a><span class="co">  path of solutions along the group fused Lasso penalty, </span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a><span class="co">  while the Lasso penalty is kept constant. Extensive </span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a><span class="co">  experiments on synthetic data compare the performances of our model </span></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a><span class="co">  to state-of-the-art clustering methods and network inference models. </span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a><span class="co">  Applications to gut microbiome data and poplar's methylation mixed with </span></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a><span class="co">  transcriptomic data are presented.</span></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span><span class="co"> [Neighborhood selection, Convex hierarchical clustering, Gaussian graphical models]</span></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a><span class="co">  type: article-journal</span></span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a><span class="co">  container-title: "Computo"</span></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a><span class="co">  doi: "10.57750/1f4p-7955"</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a><span class="co">  url: https://computo.sfds.asso.fr/published-202306-sanou-multiscale_glasso</span></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a><span class="co">  publisher: "Société Française de Statistique"</span></span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a><span class="co">  issn: "2824-7795"</span></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a><span class="an">github-user:</span><span class="co"> computorg</span></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a><span class="an">repo:</span><span class="co"> "published-202306-sanou-multiscale_glasso"</span></span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a><span class="an">published:</span><span class="co"> true</span></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-html:</span></span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-width: 8</span></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-height: 6</span></span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-pdf: default</span></span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a><span class="co">  freeze: auto  # re-render only when source changes</span></span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction </span></span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>Probabilistic graphical models <span class="co">[</span><span class="ot">@Lauritzen1996; @Koller2009</span><span class="co">]</span> are widely used in</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>high-dimensional data analysis to synthesize the interaction between variables.</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>In many applications, such as genomics or image analysis, graphical models</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>reduce the number of parameters by selecting the most relevant interactions</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>between variables. Undirected _Gaussian Graphical Models_ (GGMs) are a class of</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>graphical models used in Gaussian settings. In the context of high-dimensional</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>statistics, graphical models are generally assumed sparse, meaning that a small</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>number of variables interact compared to the total number of possible</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>interactions. This assumption has been shown to provide both statistical and</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>computational advantages by simplifying the structure of dependence between</span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>variables <span class="co">[</span><span class="ot">@Dempster1972</span><span class="co">]</span> and allowing efficient algorithms <span class="co">[</span><span class="ot">@Meinshausen2006</span><span class="co">]</span>.</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>See, for instance, @Fan2016 for a review of sparse graphical models inference.</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>In GGMs, it is well known <span class="co">[</span><span class="ot">@Lauritzen1996</span><span class="co">]</span> that inferring the graphical model</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>or, equivalently, the _conditional independence graph_ (CIG) boils down to</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>inferring the support of the precision matrix $\mathbf{\Omega}$ (the inverse of</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>the variance-covariance matrix). Several $\ell_1$ penalized methods have been</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>proposed in the literature to learn the CIG of GGMs. For instance, _the</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>neighborhood selection_ <span class="co">[</span><span class="ot">MB,@Meinshausen2006</span><span class="co">]</span> based on a nodewise regression</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>approach via the _least absolute shrinkage and selection operator_ [Lasso,</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>@tibshirani1996] is a popular method. Each variable is regressed on the others,</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>taking advantage of the link between the so-obtained regression coefficients and</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>partial correlations. The MB method has generated a long line of work in</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>nodewise regression methods. For instance, @Rocha2008 and @Ambroise2009 showed</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>that nodewise regression could be seen as a pseudo-likelihood approximation and</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>@Peng2009 extended the MB method to estimate sparse partial correlations using a</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>single regression problem. Other inference methods similar to nodewise</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>regression include a method based on the Dantzig selector <span class="co">[</span><span class="ot">@Yuan2010</span><span class="co">]</span> and the</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>introduction of the Clime estimator <span class="co">[</span><span class="ot">@Cai2011</span><span class="co">]</span>. Another family of sparse CIG</span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a>inference methods directly estimates $\mathbf{\Omega}$ via direct minimization</span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a>of the $\ell_1$-penalized negative log-likelihood <span class="co">[</span><span class="ot">@Banerjee2008</span><span class="co">]</span>, without</span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a>resorting to the auxiliary regression problem. This method called the _graphical</span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a>Lasso_ <span class="co">[</span><span class="ot">GLasso, @Friedman2007</span><span class="co">]</span>, benefits from many optimization algorithms</span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@Yuan2007; @Rothman2008; @Banerjee2008; @Hsieh2014</span><span class="co">]</span>.</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a>Such inference methods are widely used and enjoy many favorable theoretical and</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a>empirical properties, including robustness to high-dimensional problems.</span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a>However, some limitations have been observed, particularly in the presence of</span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a>strongly correlated variables. Known impairments of Lasso-type regularization</span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a>cause these limitations in this context <span class="co">[</span><span class="ot">@Buhlmann2012; @Park2007</span><span class="co">]</span>. To overcome</span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a>this, in addition to sparsity, several previous works attempt to estimate CIG by</span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a>integrating clustering structures among variables for statistical sanity and</span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a>interpretability. A non-exhaustive list of works that integrate a clustering</span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a>structure to speed up or improve the estimation procedure includes @Honorio2009,</span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a>@Ambroise2009, @Mazumder2012, @Tan2013, @Devijver2018, @Yao2019.</span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a>The above methods exploit the group structure to simplify the graph inference</span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a>problem and infer the CIG between single variables. Another question that has</span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a>received less attention is the inference of the CIG between the groups of</span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a>variables, i.e., between the meta-variables representative of the group</span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a>structure. A recent work introducing inference of graphical models on multiple</span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a>grouping levels is @Cheng2017. They proposed inferring the CIG of gene data on</span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a>two levels corresponding to genes and pathways, respectively. Note that pathways</span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a>are considered as groups of functionally related genes known in advance. The</span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a>inference is achieved by optimizing a penalized maximum likelihood that</span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a>estimates a sparse network at both gene and group levels. Our work is also part</span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a>of this dynamic. We introduce a penalty term allowing parsimonious networks to</span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a>be built at different clustering levels. The main difference with the procedure</span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a>of @Cheng2017 is that we do not require prior knowledge of the group structure,</span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a>which makes the problem significantly more complex. In addition, our method has</span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a>the advantage of proposing CIGs at more than two levels of granularity.</span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a>We introduce the Multiscale Graphical Lasso (MGLasso), a novel method to</span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a>estimate simultaneously a hierarchical clustering structure and graphical models</span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a>depicting the conditional independence structure between clusters of variables</span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a>at each level of the hierarchy. Our approach is based on neighborhood selection</span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@Meinshausen2006</span><span class="co">]</span> and considers an additional fused-Lasso type penalty for</span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a>clustering <span class="co">[</span><span class="ot">@pelckmans2005convex; @Hocking2011; @Lindsten2011</span><span class="co">]</span>.</span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a>The use of fusion penalties in Gaussian graphical model inference is a</span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a>well-studied area. Some prior works on learning sparse GGMs with a fusion</span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a>penalty term have focused on penalized likelihood. Among those, a line of works</span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@danaher2014joint; @yang2015fused</span><span class="co">]</span> infers multiple graphs across several</span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a>classes while assuming the observations belong to different known clusters.</span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a>Another line of research <span class="co">[</span><span class="ot">@Honorio2009; @Yao2019; @lin2020estimation</span><span class="co">]</span></span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a>investigates fusion penalties for enforcing local constancy in the nodes of the</span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a>inferred network. Variables belonging to the same clusters are thus more likely</span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a>to share the same neighborhood. These ordinary likelihood-based models are</span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a>computationally challenging compared to pseudo-likelihood approximations. The</span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a>unpublished manuscript of @ganguly2014 introduces a fusion-like penalty in the</span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a>neighborhood selection framework. However, the problem is solved in a node-wise</span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a>regression fashion where the $p$ regressions problems are not combined.</span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a>Fusion penalties have also been used in simple regression problems</span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@tibshirani2005sparsity</span><span class="co">]</span> and multivariate regression analysis (multitask</span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a>learning) with multiple outcomes [see, e.g., @chen2010graph; @degras2021sparse;</span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a>@dondelinger2020joint; @hallac2015network; @chu2021adaptive]. The defined</span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a>penalties encourage fusion between predictors in simple regression, or outcomes</span>
<span id="cb16-160"><a href="#cb16-160" aria-hidden="true" tabindex="-1"></a>that share similar model coefficients in multitask learning. Fusions can be</span>
<span id="cb16-161"><a href="#cb16-161" aria-hidden="true" tabindex="-1"></a>formulated in a general form assuming no order on the variables as in convex</span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a>clustering <span class="co">[</span><span class="ot">@Hoefling2010; @petry2011pairwise</span><span class="co">]</span> or assuming the availability of</span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a>prior information about clusters <span class="co">[</span><span class="ot">@rudin1992nonlinear; @hallac2015network</span><span class="co">]</span>.</span>
<span id="cb16-164"><a href="#cb16-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-165"><a href="#cb16-165" aria-hidden="true" tabindex="-1"></a>The multitask learning framework can be extended to the learning of GGMs.</span>
<span id="cb16-166"><a href="#cb16-166" aria-hidden="true" tabindex="-1"></a>@chiquet2011inferring introduced a multitask inference for multiple graphical</span>
<span id="cb16-167"><a href="#cb16-167" aria-hidden="true" tabindex="-1"></a>models when observations belong to different clusters. In MGLasso, the multitask</span>
<span id="cb16-168"><a href="#cb16-168" aria-hidden="true" tabindex="-1"></a>learning framework is combined with a novel general fusion penalty to uncover</span>
<span id="cb16-169"><a href="#cb16-169" aria-hidden="true" tabindex="-1"></a>clustering over variables. In the defined fusion term, we consider reordering</span>
<span id="cb16-170"><a href="#cb16-170" aria-hidden="true" tabindex="-1"></a>the regression coefficients to match common predictors and symmetric</span>
<span id="cb16-171"><a href="#cb16-171" aria-hidden="true" tabindex="-1"></a>coefficients. That results in enforcing the grouping property by encouraging</span>
<span id="cb16-172"><a href="#cb16-172" aria-hidden="true" tabindex="-1"></a>variables belonging to the same cluster to have the same neighborhood. MGLasso</span>
<span id="cb16-173"><a href="#cb16-173" aria-hidden="true" tabindex="-1"></a>exploits the multitask learning framework for GGMs inference coupled with a</span>
<span id="cb16-174"><a href="#cb16-174" aria-hidden="true" tabindex="-1"></a>convex clustering problem over the nodes to infer multiscale networks and</span>
<span id="cb16-175"><a href="#cb16-175" aria-hidden="true" tabindex="-1"></a>clusters simultaneously. To our knowledge, this is the first attempt in the</span>
<span id="cb16-176"><a href="#cb16-176" aria-hidden="true" tabindex="-1"></a>literature of undirected GGMs. MGLasso can also be seen as an extension of sparse</span>
<span id="cb16-177"><a href="#cb16-177" aria-hidden="true" tabindex="-1"></a>group fused Lasso for graphical models and be straightforwardly extended to</span>
<span id="cb16-178"><a href="#cb16-178" aria-hidden="true" tabindex="-1"></a>probability distributions belonging to the exponential family</span>
<span id="cb16-179"><a href="#cb16-179" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@yang2012graphical</span><span class="co">]</span>. The MGLasso algorithm is implemented in the R package _mglasso_ available at</span>
<span id="cb16-180"><a href="#cb16-180" aria-hidden="true" tabindex="-1"></a><span class="ot">&lt;https://CRAN.R-project.org/package=mglasso&gt;</span>. The remainder of this paper is</span>
<span id="cb16-181"><a href="#cb16-181" aria-hidden="true" tabindex="-1"></a>organized as follows. In Section <span class="co">[</span><span class="ot">2</span><span class="co">](#multiscale-graphical-lasso)</span> and Section</span>
<span id="cb16-182"><a href="#cb16-182" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">3</span><span class="co">](#numerical-scheme)</span>, we formally introduce the Multiscale Graphical Lasso and</span>
<span id="cb16-183"><a href="#cb16-183" aria-hidden="true" tabindex="-1"></a>its optimization algorithm. Section <span class="co">[</span><span class="ot">4</span><span class="co">](#simulation-experiments)</span> presents</span>
<span id="cb16-184"><a href="#cb16-184" aria-hidden="true" tabindex="-1"></a>simulated and real data numerical results.</span>
<span id="cb16-185"><a href="#cb16-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-186"><a href="#cb16-186" aria-hidden="true" tabindex="-1"></a><span class="fu"># Multiscale Graphical Lasso {#multiscale-graphical-lasso}</span></span>
<span id="cb16-187"><a href="#cb16-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-188"><a href="#cb16-188" aria-hidden="true" tabindex="-1"></a>Let $\mathbf X = (X^1, \dots, X^p)^T$ be a $p$-dimensional Gaussian random</span>
<span id="cb16-189"><a href="#cb16-189" aria-hidden="true" tabindex="-1"></a>vector, with mean vector $\boldsymbol \mu \in \mathbb R^p$ and positive definite</span>
<span id="cb16-190"><a href="#cb16-190" aria-hidden="true" tabindex="-1"></a>covariance matrix $\mathbf \Sigma \in \mathbb R^{p \times p}$. Let $G = (V, E)$</span>
<span id="cb16-191"><a href="#cb16-191" aria-hidden="true" tabindex="-1"></a>be a graph encoding the conditional independence structure of the normal</span>
<span id="cb16-192"><a href="#cb16-192" aria-hidden="true" tabindex="-1"></a>distribution $\mathcal N(\boldsymbol \mu, \mathbf \Sigma),$ where </span>
<span id="cb16-193"><a href="#cb16-193" aria-hidden="true" tabindex="-1"></a>$V = <span class="sc">\{</span>1,\ldots p<span class="sc">\}</span>$ is the set of vertices and $E$ the set of edges. The graph $G$</span>
<span id="cb16-194"><a href="#cb16-194" aria-hidden="true" tabindex="-1"></a>is uniquely determined by the support of the precision matrix </span>
<span id="cb16-195"><a href="#cb16-195" aria-hidden="true" tabindex="-1"></a>$\mathbf{\Omega} = \mathbf{\Sigma}^{-1}$ <span class="co">[</span><span class="ot">@Dempster1972</span><span class="co">]</span>. </span>
<span id="cb16-196"><a href="#cb16-196" aria-hidden="true" tabindex="-1"></a>Specifically, for any two vertices $i \neq j\in V$, </span>
<span id="cb16-197"><a href="#cb16-197" aria-hidden="true" tabindex="-1"></a>the edge $(i,j)$ belongs to the set $E$ if and only if</span>
<span id="cb16-198"><a href="#cb16-198" aria-hidden="true" tabindex="-1"></a>$\Omega_{ij} \neq 0.$ On the contrary, if $\Omega_{ij} = 0$,</span>
<span id="cb16-199"><a href="#cb16-199" aria-hidden="true" tabindex="-1"></a>the variables $X^i$ and $X^j$ are said to be independent</span>
<span id="cb16-200"><a href="#cb16-200" aria-hidden="true" tabindex="-1"></a>conditionally to the remaining variables $X^{\setminus (i, j)}$. We note, </span>
<span id="cb16-201"><a href="#cb16-201" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-202"><a href="#cb16-202" aria-hidden="true" tabindex="-1"></a>X^i</span>
<span id="cb16-203"><a href="#cb16-203" aria-hidden="true" tabindex="-1"></a>\perp <span class="sc">\!\!\!</span> \perp X^j |X^{\setminus (i, j)} \Leftrightarrow \Omega_{ij} = 0.</span>
<span id="cb16-204"><a href="#cb16-204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-205"><a href="#cb16-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-206"><a href="#cb16-206" aria-hidden="true" tabindex="-1"></a>Let $\boldsymbol X = \left( \boldsymbol X_1^T, \dots, \boldsymbol X_n^T</span>
<span id="cb16-207"><a href="#cb16-207" aria-hidden="true" tabindex="-1"></a>\right )^T$ be the $n \times p$-dimensional data matrix composed of $n$ i.i.d</span>
<span id="cb16-208"><a href="#cb16-208" aria-hidden="true" tabindex="-1"></a>samples of the Gaussian random vector $\mathbf X$. To perform graphical model</span>
<span id="cb16-209"><a href="#cb16-209" aria-hidden="true" tabindex="-1"></a>inference, @Meinshausen2006 consider $p$ separate linear regressions of the</span>
<span id="cb16-210"><a href="#cb16-210" aria-hidden="true" tabindex="-1"></a>form: </span>
<span id="cb16-211"><a href="#cb16-211" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-212"><a href="#cb16-212" aria-hidden="true" tabindex="-1"></a>\hat{\boldsymbol{\beta}^i}(\lambda) = \underset{\boldsymbol{\beta}^i</span>
<span id="cb16-213"><a href="#cb16-213" aria-hidden="true" tabindex="-1"></a>\in \mathbb{R}^{p-1}}{\operatorname{argmin}} \frac{1}{n} \left \lVert</span>
<span id="cb16-214"><a href="#cb16-214" aria-hidden="true" tabindex="-1"></a>\mathbf{X}^i - \mathbf{X}^{\setminus i} \boldsymbol{\beta}^i \right \rVert_2 ^2</span>
<span id="cb16-215"><a href="#cb16-215" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>\lambda \left \lVert \boldsymbol{\beta}^i \right \rVert_1,</span>
<span id="cb16-216"><a href="#cb16-216" aria-hidden="true" tabindex="-1"></a>$${#eq-neighborhood} </span>
<span id="cb16-217"><a href="#cb16-217" aria-hidden="true" tabindex="-1"></a>where $\lambda$ is a non-negative regularization parameter,</span>
<span id="cb16-218"><a href="#cb16-218" aria-hidden="true" tabindex="-1"></a>$\mathbf{X}^{\setminus i}$ denotes the matrix $\mathbf{X}$ deprived of column</span>
<span id="cb16-219"><a href="#cb16-219" aria-hidden="true" tabindex="-1"></a>$i$, $\boldsymbol{\beta}^i = (\beta^i_j)_{j \in <span class="sc">\{</span>1,\dots,p<span class="sc">\}</span> \backslash i}$ </span>
<span id="cb16-220"><a href="#cb16-220" aria-hidden="true" tabindex="-1"></a>is a vector of $p-1$ regression coefficients and</span>
<span id="cb16-221"><a href="#cb16-221" aria-hidden="true" tabindex="-1"></a>$\left \lVert . \right \rVert_1$ is the $\ell_1-$norm. These Lasso regularized</span>
<span id="cb16-222"><a href="#cb16-222" aria-hidden="true" tabindex="-1"></a>problems estimate the neighborhoods, one variable at a time. The final edge set</span>
<span id="cb16-223"><a href="#cb16-223" aria-hidden="true" tabindex="-1"></a>estimates $\hat E$ can be deduced from the union of the estimated neighborhoods</span>
<span id="cb16-224"><a href="#cb16-224" aria-hidden="true" tabindex="-1"></a>using an AND or OR rule (@Meinshausen2006). The MB approach is based on the</span>
<span id="cb16-225"><a href="#cb16-225" aria-hidden="true" tabindex="-1"></a>central relationship between simple linear regression and precision matrix</span>
<span id="cb16-226"><a href="#cb16-226" aria-hidden="true" tabindex="-1"></a>coefficients. It can be shown that $\beta^i_j =</span>
<span id="cb16-227"><a href="#cb16-227" aria-hidden="true" tabindex="-1"></a>-\frac{\Omega_{ij}}{\Omega_{ii}}$ <span class="co">[</span><span class="ot">@Lauritzen1996</span><span class="co">]</span>.</span>
<span id="cb16-228"><a href="#cb16-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-229"><a href="#cb16-229" aria-hidden="true" tabindex="-1"></a>On the other hand, let us now consider the clustering analysis of the $p$</span>
<span id="cb16-230"><a href="#cb16-230" aria-hidden="true" tabindex="-1"></a>variables in $\mathbb R^n.$ The convex clustering problem [@Hocking2011; @Lindsten2011;</span>
<span id="cb16-231"><a href="#cb16-231" aria-hidden="true" tabindex="-1"></a>@pelckmans2005convex] is the minimization of the quantity </span>
<span id="cb16-232"><a href="#cb16-232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-233"><a href="#cb16-233" aria-hidden="true" tabindex="-1"></a>\frac{1}{2}</span>
<span id="cb16-234"><a href="#cb16-234" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^p \left \lVert \boldsymbol X^i - \boldsymbol  \alpha^i \right</span>
<span id="cb16-235"><a href="#cb16-235" aria-hidden="true" tabindex="-1"></a>\rVert_2^2 + \lambda \sum_{i &lt; j} w_{ij} \left \lVert \boldsymbol  \alpha^i -</span>
<span id="cb16-236"><a href="#cb16-236" aria-hidden="true" tabindex="-1"></a>\boldsymbol  \alpha^j \right \rVert_q </span>
<span id="cb16-237"><a href="#cb16-237" aria-hidden="true" tabindex="-1"></a>$$ {#eq-clusterpath} </span>
<span id="cb16-238"><a href="#cb16-238" aria-hidden="true" tabindex="-1"></a>with respect to the</span>
<span id="cb16-239"><a href="#cb16-239" aria-hidden="true" tabindex="-1"></a>matrix $\boldsymbol \alpha \in \mathbb R^{p \times n}$, where $\lambda$ is a</span>
<span id="cb16-240"><a href="#cb16-240" aria-hidden="true" tabindex="-1"></a>sparsity penalization parameter, $<span class="sc">\{</span> w_{ij} <span class="sc">\}</span>$ are symmetric positive weights,</span>
<span id="cb16-241"><a href="#cb16-241" aria-hidden="true" tabindex="-1"></a>$\boldsymbol \alpha^i \in \mathbb R^n$ is the centroid to which $\boldsymbol</span>
<span id="cb16-242"><a href="#cb16-242" aria-hidden="true" tabindex="-1"></a>X^i$ is assigned to, and $\left \lVert . \right \rVert_q$ is the $\ell_q$-norm</span>
<span id="cb16-243"><a href="#cb16-243" aria-hidden="true" tabindex="-1"></a>on $\mathbb R^p$ with $q \ge 1.$ Points $\boldsymbol X^i$ and $\boldsymbol X^j$</span>
<span id="cb16-244"><a href="#cb16-244" aria-hidden="true" tabindex="-1"></a>are assigned to the same cluster if $\hat{\boldsymbol \alpha^i} \approx</span>
<span id="cb16-245"><a href="#cb16-245" aria-hidden="true" tabindex="-1"></a>\hat{\boldsymbol \alpha^j}.$ The regularization path of solutions to problem in</span>
<span id="cb16-246"><a href="#cb16-246" aria-hidden="true" tabindex="-1"></a>@eq-clusterpath can be represented as a dendrogram. The path properties have</span>
<span id="cb16-247"><a href="#cb16-247" aria-hidden="true" tabindex="-1"></a>been studied in @chi2015splitting and @chiquet2017fast, among others.</span>
<span id="cb16-248"><a href="#cb16-248" aria-hidden="true" tabindex="-1"></a>Note that these approaches rely on geometric properties of matrix $\boldsymbol X,$</span>
<span id="cb16-249"><a href="#cb16-249" aria-hidden="true" tabindex="-1"></a>and do not require any assumption on the distribution of the covariates.</span>
<span id="cb16-250"><a href="#cb16-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-251"><a href="#cb16-251" aria-hidden="true" tabindex="-1"></a>We propose to combine the MB and convex clustering approaches.</span>
<span id="cb16-252"><a href="#cb16-252" aria-hidden="true" tabindex="-1"></a>Specifically, the $p$ independent Lasso regressions of the MB approach are merged</span>
<span id="cb16-253"><a href="#cb16-253" aria-hidden="true" tabindex="-1"></a>into a single optimization criterion where a convex clustering fusion penalty in</span>
<span id="cb16-254"><a href="#cb16-254" aria-hidden="true" tabindex="-1"></a>$\ell_2$ is applied on the regression vectors considered as cluster centers.</span>
<span id="cb16-255"><a href="#cb16-255" aria-hidden="true" tabindex="-1"></a>Namely, the _Multiscale Graphical Lasso_ (MGLasso) pseudo-likelihood problem</span>
<span id="cb16-256"><a href="#cb16-256" aria-hidden="true" tabindex="-1"></a>minimizes in a Gaussian framework the following quantity: </span>
<span id="cb16-257"><a href="#cb16-257" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-258"><a href="#cb16-258" aria-hidden="true" tabindex="-1"></a>J_{\lambda_1,</span>
<span id="cb16-259"><a href="#cb16-259" aria-hidden="true" tabindex="-1"></a>\lambda_2}(\boldsymbol{\beta}; \mathbf{X} ) = \frac{1}{2} \sum_{i=1}^p \left</span>
<span id="cb16-260"><a href="#cb16-260" aria-hidden="true" tabindex="-1"></a>\lVert \mathbf{X}^i - \mathbf{X}^{\setminus i} \boldsymbol{\beta}^i \right</span>
<span id="cb16-261"><a href="#cb16-261" aria-hidden="true" tabindex="-1"></a>\rVert_2 ^2  + \lambda_1 \sum_{i = 1}^p  \left \lVert \boldsymbol{\beta}^i</span>
<span id="cb16-262"><a href="#cb16-262" aria-hidden="true" tabindex="-1"></a>\right \rVert_1 + \lambda_2 \sum_{i &lt; j} \left \lVert \boldsymbol{\beta}^i -</span>
<span id="cb16-263"><a href="#cb16-263" aria-hidden="true" tabindex="-1"></a>\boldsymbol \tau_{ij}\boldsymbol{\beta}^j \right \rVert_2,</span>
<span id="cb16-264"><a href="#cb16-264" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cost-fct}</span>
<span id="cb16-265"><a href="#cb16-265" aria-hidden="true" tabindex="-1"></a>with respect to $\boldsymbol{\beta} := [{\boldsymbol{\beta}^1}, \ldots,</span>
<span id="cb16-266"><a href="#cb16-266" aria-hidden="true" tabindex="-1"></a>{\boldsymbol{\beta}^p}] \in \mathbb{R}^{(p-1) \times p},$ where</span>
<span id="cb16-267"><a href="#cb16-267" aria-hidden="true" tabindex="-1"></a>$\mathbf{X}^{i}\in \mathbb{R}^n$ denotes the $i$-th column of $\mathbf{X}$,</span>
<span id="cb16-268"><a href="#cb16-268" aria-hidden="true" tabindex="-1"></a>$\lambda_1$ and $\lambda_2$ are penalization parameters, $\boldsymbol \tau_{ij}</span>
<span id="cb16-269"><a href="#cb16-269" aria-hidden="true" tabindex="-1"></a>\in \mathbb R^{(p-1)\times(p-1)}$ is a permutation matrix, which permutes the</span>
<span id="cb16-270"><a href="#cb16-270" aria-hidden="true" tabindex="-1"></a>coefficients in the regression vector $\boldsymbol \beta^j$ such as </span>
<span id="cb16-271"><a href="#cb16-271" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-272"><a href="#cb16-272" aria-hidden="true" tabindex="-1"></a>\left</span>
<span id="cb16-273"><a href="#cb16-273" aria-hidden="true" tabindex="-1"></a>\lVert \boldsymbol{\beta}^i - \boldsymbol \tau_{ij}\boldsymbol{\beta}^j \right</span>
<span id="cb16-274"><a href="#cb16-274" aria-hidden="true" tabindex="-1"></a>\rVert_2 = \sqrt{\sum_{k \in <span class="sc">\{</span>1, \dots,p <span class="sc">\}</span> \backslash <span class="sc">\{</span>i,j<span class="sc">\}</span>} (\beta^i_k -</span>
<span id="cb16-275"><a href="#cb16-275" aria-hidden="true" tabindex="-1"></a>\beta^j_k)^2 + (\beta^i_j - \beta^j_i)^2 },</span>
<span id="cb16-276"><a href="#cb16-276" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-277"><a href="#cb16-277" aria-hidden="true" tabindex="-1"></a>as illustrated in</span>
<span id="cb16-278"><a href="#cb16-278" aria-hidden="true" tabindex="-1"></a>@fig-permute-beta. The coefficient $\beta^i_k$ is to be read as the multiple</span>
<span id="cb16-279"><a href="#cb16-279" aria-hidden="true" tabindex="-1"></a>regression coefficients of $\boldsymbol X^i$ on $\boldsymbol X^k.$</span>
<span id="cb16-280"><a href="#cb16-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-281"><a href="#cb16-281" aria-hidden="true" tabindex="-1"></a>The MGLasso criterion can be seen as a multitask regression problem where the</span>
<span id="cb16-282"><a href="#cb16-282" aria-hidden="true" tabindex="-1"></a>set of responses is identical to the set of predictors. The Lasso penalty term</span>
<span id="cb16-283"><a href="#cb16-283" aria-hidden="true" tabindex="-1"></a>encourages sparsity in the estimated coefficients while the group-fused term</span>
<span id="cb16-284"><a href="#cb16-284" aria-hidden="true" tabindex="-1"></a>encourages fusion in the regression vectors $\boldsymbol{\beta}^i$ and</span>
<span id="cb16-285"><a href="#cb16-285" aria-hidden="true" tabindex="-1"></a>$\boldsymbol{\beta}^j$.</span>
<span id="cb16-286"><a href="#cb16-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-287"><a href="#cb16-287" aria-hidden="true" tabindex="-1"></a>Let us illustrate by an example the effect of the fusion term in the proposed</span>
<span id="cb16-288"><a href="#cb16-288" aria-hidden="true" tabindex="-1"></a>approach. Two variables $i$ and $j$ are in the same group when</span>
<span id="cb16-289"><a href="#cb16-289" aria-hidden="true" tabindex="-1"></a>$\|\boldsymbol{\beta}^i - \boldsymbol \tau_{ij} \boldsymbol{\beta}^j\|_2 \approx</span>
<span id="cb16-290"><a href="#cb16-290" aria-hidden="true" tabindex="-1"></a>0$. Considering a cluster $\mathcal C$ of $q$ variables, it is straightforward</span>
<span id="cb16-291"><a href="#cb16-291" aria-hidden="true" tabindex="-1"></a>to show that $\forall (i,j) \in \mathcal C^2$, we have $\hat</span>
<span id="cb16-292"><a href="#cb16-292" aria-hidden="true" tabindex="-1"></a>{\beta^i_j}=\beta_{\mathcal C}$, where $\beta_{\mathcal C}$ is a scalar. Thus</span>
<span id="cb16-293"><a href="#cb16-293" aria-hidden="true" tabindex="-1"></a>the algorithm is likely to produce precision matrices with blocks of constant</span>
<span id="cb16-294"><a href="#cb16-294" aria-hidden="true" tabindex="-1"></a>entries for a given value of $\lambda_2,$ each block corresponding to a cluster.</span>
<span id="cb16-295"><a href="#cb16-295" aria-hidden="true" tabindex="-1"></a>In the same vein as @Park2007, a cluster composed of variables that share the</span>
<span id="cb16-296"><a href="#cb16-296" aria-hidden="true" tabindex="-1"></a>same coefficients can be summarized by a representative variable.</span>
<span id="cb16-297"><a href="#cb16-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-298"><a href="#cb16-298" aria-hidden="true" tabindex="-1"></a>A component-wise difference between two regression vectors without reordering</span>
<span id="cb16-299"><a href="#cb16-299" aria-hidden="true" tabindex="-1"></a>the coefficients would not necesarily cluster variables which share the same</span>
<span id="cb16-300"><a href="#cb16-300" aria-hidden="true" tabindex="-1"></a>neighborhood. The permutation $\boldsymbol \tau_{ij}$ reoders coefficients in</span>
<span id="cb16-301"><a href="#cb16-301" aria-hidden="true" tabindex="-1"></a>such a way that differences are taken between symmetric coeffecients and those</span>
<span id="cb16-302"><a href="#cb16-302" aria-hidden="true" tabindex="-1"></a>corresponding to the same set of predictors. The model is thus likely to cluster</span>
<span id="cb16-303"><a href="#cb16-303" aria-hidden="true" tabindex="-1"></a>together variables that share the same neighboring structure and encourages</span>
<span id="cb16-304"><a href="#cb16-304" aria-hidden="true" tabindex="-1"></a>symmetric graph structures.</span>
<span id="cb16-305"><a href="#cb16-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-306"><a href="#cb16-306" aria-hidden="true" tabindex="-1"></a>:::{#fig-permute-beta} </span>
<span id="cb16-307"><a href="#cb16-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-308"><a href="#cb16-308" aria-hidden="true" tabindex="-1"></a><span class="al">![](./figures/permute-beta.png)</span>  </span>
<span id="cb16-309"><a href="#cb16-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-310"><a href="#cb16-310" aria-hidden="true" tabindex="-1"></a>Illustration of the permutation between regression coefficients in the MGLasso</span>
<span id="cb16-311"><a href="#cb16-311" aria-hidden="true" tabindex="-1"></a>model. </span>
<span id="cb16-312"><a href="#cb16-312" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-313"><a href="#cb16-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-314"><a href="#cb16-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-315"><a href="#cb16-315" aria-hidden="true" tabindex="-1"></a>In practice, when external information about the clustering structure is</span>
<span id="cb16-316"><a href="#cb16-316" aria-hidden="true" tabindex="-1"></a>available, the problem can be generalized into: </span>
<span id="cb16-317"><a href="#cb16-317" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-318"><a href="#cb16-318" aria-hidden="true" tabindex="-1"></a>\min_{\boldsymbol{\beta}}</span>
<span id="cb16-319"><a href="#cb16-319" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^p\frac{1}{2} \left \lVert \mathbf{X}^i - \mathbf{X}^{\setminus i}</span>
<span id="cb16-320"><a href="#cb16-320" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\beta}^i \right \rVert_2 ^2  + \lambda_1 \sum_{i = 1}^p \left \lVert</span>
<span id="cb16-321"><a href="#cb16-321" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\beta}^i \right \rVert_1 + \lambda_2 \sum_{i &lt; j}  w_{ij} \left</span>
<span id="cb16-322"><a href="#cb16-322" aria-hidden="true" tabindex="-1"></a>\lVert \boldsymbol{\beta}^i - \boldsymbol \tau_{ij}\boldsymbol{\beta}^j \right</span>
<span id="cb16-323"><a href="#cb16-323" aria-hidden="true" tabindex="-1"></a>\rVert_2,</span>
<span id="cb16-324"><a href="#cb16-324" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cost-fct-general} </span>
<span id="cb16-325"><a href="#cb16-325" aria-hidden="true" tabindex="-1"></a>where $w_{ij}$ is a positive weight. In the</span>
<span id="cb16-326"><a href="#cb16-326" aria-hidden="true" tabindex="-1"></a>remainder of the paper, we will assume that $w_{ij} = 1$ for simplicity.</span>
<span id="cb16-327"><a href="#cb16-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-328"><a href="#cb16-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-329"><a href="#cb16-329" aria-hidden="true" tabindex="-1"></a><span class="fu"># Numerical scheme</span></span>
<span id="cb16-330"><a href="#cb16-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-331"><a href="#cb16-331" aria-hidden="true" tabindex="-1"></a>This Section introduces a complete numerical scheme of the Multiscale Graphical</span>
<span id="cb16-332"><a href="#cb16-332" aria-hidden="true" tabindex="-1"></a>Lasso via convex optimization and a model selection procedure. Section</span>
<span id="cb16-333"><a href="#cb16-333" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">3.1</span><span class="co">](#optimization-via-conesta-algorithm)</span> reviews the principles of the</span>
<span id="cb16-334"><a href="#cb16-334" aria-hidden="true" tabindex="-1"></a>Continuation with Nesterov smoothing in a shrinkage-thresholding algorithm</span>
<span id="cb16-335"><a href="#cb16-335" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">CONESTA, @hadjselem2018</span><span class="co">]</span>. Section</span>
<span id="cb16-336"><a href="#cb16-336" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">3.2</span><span class="co">](#reformulation-of-mglasso-for-conesta-algorithm)</span> details a reformulation</span>
<span id="cb16-337"><a href="#cb16-337" aria-hidden="true" tabindex="-1"></a>of the MGLasso criterion, which eases the use of CONESTA as a solver. Finally,</span>
<span id="cb16-338"><a href="#cb16-338" aria-hidden="true" tabindex="-1"></a>Section <span class="co">[</span><span class="ot">3.3</span><span class="co">](#model-selection)</span> presents the procedure for selecting the</span>
<span id="cb16-339"><a href="#cb16-339" aria-hidden="true" tabindex="-1"></a>regularization parameters.</span>
<span id="cb16-340"><a href="#cb16-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-341"><a href="#cb16-341" aria-hidden="true" tabindex="-1"></a><span class="fu">## Optimization via CONESTA algorithm {#optimization-via-conesta-algorithm}</span></span>
<span id="cb16-342"><a href="#cb16-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-343"><a href="#cb16-343" aria-hidden="true" tabindex="-1"></a>The optimization problem for Multiscale Graphical Lasso is convex but not</span>
<span id="cb16-344"><a href="#cb16-344" aria-hidden="true" tabindex="-1"></a>straightforward to solve using classical algorithms because of the fused-lasso</span>
<span id="cb16-345"><a href="#cb16-345" aria-hidden="true" tabindex="-1"></a>type penalty, which is non-separable and admits no closed-form solution for the</span>
<span id="cb16-346"><a href="#cb16-346" aria-hidden="true" tabindex="-1"></a>proximal gradient. We rely on the Continuation with Nesterov smoothing in a</span>
<span id="cb16-347"><a href="#cb16-347" aria-hidden="true" tabindex="-1"></a>shrinkage-thresholding algorithm <span class="co">[</span><span class="ot">@hadjselem2018</span><span class="co">]</span> dedicated to high-dimensional</span>
<span id="cb16-348"><a href="#cb16-348" aria-hidden="true" tabindex="-1"></a>regression problems with structured sparsity, such as group structures.</span>
<span id="cb16-349"><a href="#cb16-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-350"><a href="#cb16-350" aria-hidden="true" tabindex="-1"></a>The CONESTA solver, initially introduced for neuro-imaging problems, addresses a</span>
<span id="cb16-351"><a href="#cb16-351" aria-hidden="true" tabindex="-1"></a>general class of convex optimization problems that include group-wise penalties.</span>
<span id="cb16-352"><a href="#cb16-352" aria-hidden="true" tabindex="-1"></a>The algorithm solves problems in the form  </span>
<span id="cb16-353"><a href="#cb16-353" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-354"><a href="#cb16-354" aria-hidden="true" tabindex="-1"></a>\operatorname{minimize \ w.r.t. }</span>
<span id="cb16-355"><a href="#cb16-355" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\theta} \quad f(\boldsymbol{\theta}) = g(\boldsymbol{\theta}) +</span>
<span id="cb16-356"><a href="#cb16-356" aria-hidden="true" tabindex="-1"></a>\lambda_1 h(\boldsymbol{\theta}) + \lambda_2 s(\boldsymbol{\theta}),</span>
<span id="cb16-357"><a href="#cb16-357" aria-hidden="true" tabindex="-1"></a>$$ {#eq-conesta-criterion}  </span>
<span id="cb16-358"><a href="#cb16-358" aria-hidden="true" tabindex="-1"></a>where $\boldsymbol{\theta}\in \mathbb{R}^d$ and</span>
<span id="cb16-359"><a href="#cb16-359" aria-hidden="true" tabindex="-1"></a>$\lambda_1$ and $\lambda_2$ are penalty parameters.</span>
<span id="cb16-360"><a href="#cb16-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-361"><a href="#cb16-361" aria-hidden="true" tabindex="-1"></a>In the original paper <span class="co">[</span><span class="ot">@hadjselem2018</span><span class="co">]</span>, $g(\boldsymbol{\theta})$ is a</span>
<span id="cb16-362"><a href="#cb16-362" aria-hidden="true" tabindex="-1"></a>differentiable function, $h(\boldsymbol{\theta})$ is a penalty function whose</span>
<span id="cb16-363"><a href="#cb16-363" aria-hidden="true" tabindex="-1"></a>proximal operator $\operatorname{prox}_{\lambda_1 h}$ is known in closed-form.</span>
<span id="cb16-364"><a href="#cb16-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-365"><a href="#cb16-365" aria-hidden="true" tabindex="-1"></a>Given $\phi \subseteq <span class="sc">\{</span>1,\ldots, d<span class="sc">\}</span>,$ let $\boldsymbol{\theta}_\phi =</span>
<span id="cb16-366"><a href="#cb16-366" aria-hidden="true" tabindex="-1"></a>(\theta_i)_{i \in \phi}$ denote the subvector of $\boldsymbol{\theta}$</span>
<span id="cb16-367"><a href="#cb16-367" aria-hidden="true" tabindex="-1"></a>referenced by the indices in $\phi.$ Denote $\Phi = <span class="sc">\{</span> \phi_1, \dots,</span>
<span id="cb16-368"><a href="#cb16-368" aria-hidden="true" tabindex="-1"></a>\phi_{\operatorname{Card}(\Phi)}<span class="sc">\}</span>$ a collection with $\phi_i \subseteq</span>
<span id="cb16-369"><a href="#cb16-369" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>1,\ldots, d<span class="sc">\}</span>.$ Let the matrix $\mathbf{A}_\phi \in \mathbb{R}^{m \times</span>
<span id="cb16-370"><a href="#cb16-370" aria-hidden="true" tabindex="-1"></a>\operatorname{Card}(\Phi) }$ define a linear map from</span>
<span id="cb16-371"><a href="#cb16-371" aria-hidden="true" tabindex="-1"></a>$\mathbb{R}^{\operatorname{Card}(\phi)}$ to $\mathbb{R}^m$ by sending the column</span>
<span id="cb16-372"><a href="#cb16-372" aria-hidden="true" tabindex="-1"></a>vector $\boldsymbol{\theta}_\phi \in \mathbb{R}^{\operatorname{Card}(\phi)}$ to</span>
<span id="cb16-373"><a href="#cb16-373" aria-hidden="true" tabindex="-1"></a>the column vector $\mathbf{A}_\phi \boldsymbol{\theta}_\phi \in \mathbb{R}^m.$</span>
<span id="cb16-374"><a href="#cb16-374" aria-hidden="true" tabindex="-1"></a>The function $s(\boldsymbol{\theta})$ is assumed to be an $\ell_{1,2}$-norm</span>
<span id="cb16-375"><a href="#cb16-375" aria-hidden="true" tabindex="-1"></a>i.e., the sum of the group-wise $\ell_2$-norms of the elements $\mathbf{A}_\phi</span>
<span id="cb16-376"><a href="#cb16-376" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\theta}_\phi, \phi \in \Phi.$ Namely, </span>
<span id="cb16-377"><a href="#cb16-377" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-378"><a href="#cb16-378" aria-hidden="true" tabindex="-1"></a>s(\boldsymbol{\theta}) =</span>
<span id="cb16-379"><a href="#cb16-379" aria-hidden="true" tabindex="-1"></a>\sum_{\phi \in \Phi} \|\mathbf{A}_\phi \boldsymbol{\theta}_\phi\|_2.</span>
<span id="cb16-380"><a href="#cb16-380" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-381"><a href="#cb16-381" aria-hidden="true" tabindex="-1"></a>When</span>
<span id="cb16-382"><a href="#cb16-382" aria-hidden="true" tabindex="-1"></a>$\mathbf{A}_\phi$ is the identity operator, the penalty function $s$ is the</span>
<span id="cb16-383"><a href="#cb16-383" aria-hidden="true" tabindex="-1"></a>overlapping group-lasso and $m = \operatorname{Card}(\phi)$. When it is a</span>
<span id="cb16-384"><a href="#cb16-384" aria-hidden="true" tabindex="-1"></a>discrete derivative operator,  $s$ is a total variation penalty, and $m$ can be</span>
<span id="cb16-385"><a href="#cb16-385" aria-hidden="true" tabindex="-1"></a>seen as the number of neighborhood relationships.</span>
<span id="cb16-386"><a href="#cb16-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-387"><a href="#cb16-387" aria-hidden="true" tabindex="-1"></a>The non-smooth $\ell_{1,2}$-norm penalty can be approximated by a smooth</span>
<span id="cb16-388"><a href="#cb16-388" aria-hidden="true" tabindex="-1"></a>function with known gradient computed using Nesterov's smoothing</span>
<span id="cb16-389"><a href="#cb16-389" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@nesterov2005smooth</span><span class="co">]</span>. Given a smoothness parameter $\mu&gt;0$, let us define the</span>
<span id="cb16-390"><a href="#cb16-390" aria-hidden="true" tabindex="-1"></a>smooth approximation </span>
<span id="cb16-391"><a href="#cb16-391" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-392"><a href="#cb16-392" aria-hidden="true" tabindex="-1"></a>s_{\mu}(\boldsymbol{\theta}) = \max_{\boldsymbol{\alpha}</span>
<span id="cb16-393"><a href="#cb16-393" aria-hidden="true" tabindex="-1"></a>\in \mathcal{K}} \left <span class="sc">\{</span> \boldsymbol{\alpha}^T \mathbf{A} \boldsymbol{\theta} -</span>
<span id="cb16-394"><a href="#cb16-394" aria-hidden="true" tabindex="-1"></a>\frac{\mu}{2} \| \boldsymbol{\alpha} \|_2^2 \right <span class="sc">\}</span>,</span>
<span id="cb16-395"><a href="#cb16-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-396"><a href="#cb16-396" aria-hidden="true" tabindex="-1"></a>where $\mathcal{K}$ is</span>
<span id="cb16-397"><a href="#cb16-397" aria-hidden="true" tabindex="-1"></a>the cartesian product of $\ell_2$-unit balls, $\mathbf{A}$ is the vertical</span>
<span id="cb16-398"><a href="#cb16-398" aria-hidden="true" tabindex="-1"></a>concatenation of the matrices $\mathbf{A}_\phi$ and $\boldsymbol{\alpha}$ is an</span>
<span id="cb16-399"><a href="#cb16-399" aria-hidden="true" tabindex="-1"></a>auxiliary variable resulting from the dual reformulation of</span>
<span id="cb16-400"><a href="#cb16-400" aria-hidden="true" tabindex="-1"></a>$s(\boldsymbol{\theta})$. Note that $\lim_{\mu \rightarrow 0}</span>
<span id="cb16-401"><a href="#cb16-401" aria-hidden="true" tabindex="-1"></a>s_{\mu}(\boldsymbol{\theta}) = s(\boldsymbol{\theta}).$ A Fast Iterative</span>
<span id="cb16-402"><a href="#cb16-402" aria-hidden="true" tabindex="-1"></a>Shrinkage-Thresholding Algorithm <span class="co">[</span><span class="ot">FISTA, @Beck2009</span><span class="co">]</span> step can then be applied</span>
<span id="cb16-403"><a href="#cb16-403" aria-hidden="true" tabindex="-1"></a>after computing the gradient of the smooth part i.e. $g(\boldsymbol{\theta}) +</span>
<span id="cb16-404"><a href="#cb16-404" aria-hidden="true" tabindex="-1"></a>\lambda_2 s_{\mu}(\boldsymbol{\theta})$ of the approximated criterion.</span>
<span id="cb16-405"><a href="#cb16-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-406"><a href="#cb16-406" aria-hidden="true" tabindex="-1"></a>The main ingredient of CONESTA remains in the determination of the optimal</span>
<span id="cb16-407"><a href="#cb16-407" aria-hidden="true" tabindex="-1"></a>smoothness parameter using the duality gap, which minimizes the number of FISTA</span>
<span id="cb16-408"><a href="#cb16-408" aria-hidden="true" tabindex="-1"></a>iterations for a given precision $\epsilon.$ The specification of $\mu$ is</span>
<span id="cb16-409"><a href="#cb16-409" aria-hidden="true" tabindex="-1"></a>subject to dynamic update. A sequence of decreasing optimal smoothness</span>
<span id="cb16-410"><a href="#cb16-410" aria-hidden="true" tabindex="-1"></a>parameters is generated in order to dynamically adapt the FISTA algorithm</span>
<span id="cb16-411"><a href="#cb16-411" aria-hidden="true" tabindex="-1"></a>stepsize towards $\epsilon.$ Namely, $\mu^k = \mu_{opt}(\epsilon^k).$ The</span>
<span id="cb16-412"><a href="#cb16-412" aria-hidden="true" tabindex="-1"></a>smoothness parameter decreases as one gets closer to $\boldsymbol{\theta}</span>
<span id="cb16-413"><a href="#cb16-413" aria-hidden="true" tabindex="-1"></a>^\star$, the solution of the problem defined in @eq-conesta-criterion. Since</span>
<span id="cb16-414"><a href="#cb16-414" aria-hidden="true" tabindex="-1"></a>$\boldsymbol{\theta} ^\star$ is unknown; the approximation of the distance to</span>
<span id="cb16-415"><a href="#cb16-415" aria-hidden="true" tabindex="-1"></a>the minimum is achieved via the duality gap. Indeed </span>
<span id="cb16-416"><a href="#cb16-416" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-417"><a href="#cb16-417" aria-hidden="true" tabindex="-1"></a>\operatorname{GAP}_{\mu^k}(\boldsymbol{\theta}^k) \ge</span>
<span id="cb16-418"><a href="#cb16-418" aria-hidden="true" tabindex="-1"></a>f_{\mu^k}(\boldsymbol{\theta}^k) - f(\boldsymbol{\theta}^\star) \ge 0.</span>
<span id="cb16-419"><a href="#cb16-419" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-420"><a href="#cb16-420" aria-hidden="true" tabindex="-1"></a>We</span>
<span id="cb16-421"><a href="#cb16-421" aria-hidden="true" tabindex="-1"></a>refer the reader to the seminal paper for more details on the formulation of</span>
<span id="cb16-422"><a href="#cb16-422" aria-hidden="true" tabindex="-1"></a>$\operatorname{GAP}_{\mu^k}(\boldsymbol{\theta}^k).$ The CONESTA routine is</span>
<span id="cb16-423"><a href="#cb16-423" aria-hidden="true" tabindex="-1"></a>spelled out in the algorithm CONESTA solver where $L(g + \lambda_2 s_{\mu})$ is</span>
<span id="cb16-424"><a href="#cb16-424" aria-hidden="true" tabindex="-1"></a>the Lipschitz constant of $\nabla(g + \lambda_2 s_{\mu}),$ $k$ is the iteration</span>
<span id="cb16-425"><a href="#cb16-425" aria-hidden="true" tabindex="-1"></a>counter for the inner FISTA updates and $i$ is the iteration counter for CONESTA</span>
<span id="cb16-426"><a href="#cb16-426" aria-hidden="true" tabindex="-1"></a>updates.</span>
<span id="cb16-427"><a href="#cb16-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-428"><a href="#cb16-428" aria-hidden="true" tabindex="-1"></a><span class="in">```pseudocode</span></span>
<span id="cb16-429"><a href="#cb16-429" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: alg-conesta</span></span>
<span id="cb16-430"><a href="#cb16-430" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-indent-size: "1.2em"</span></span>
<span id="cb16-431"><a href="#cb16-431" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-comment-delimiter: "//"</span></span>
<span id="cb16-432"><a href="#cb16-432" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-line-number: true</span></span>
<span id="cb16-433"><a href="#cb16-433" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-line-number-punc: ":"</span></span>
<span id="cb16-434"><a href="#cb16-434" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-no-end: false</span></span>
<span id="cb16-435"><a href="#cb16-435" aria-hidden="true" tabindex="-1"></a><span class="in">#| pdf-placement: "htb!"</span></span>
<span id="cb16-436"><a href="#cb16-436" aria-hidden="true" tabindex="-1"></a><span class="in">#| pdf-line-number: true</span></span>
<span id="cb16-437"><a href="#cb16-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-438"><a href="#cb16-438" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb16-439"><a href="#cb16-439" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{CONESTA solver}</span></span>
<span id="cb16-440"><a href="#cb16-440" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb16-441"><a href="#cb16-441" aria-hidden="true" tabindex="-1"></a><span class="in">  \State \textbf{Inputs}: \\</span></span>
<span id="cb16-442"><a href="#cb16-442" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ functions $g(\boldsymbol{\theta}), h(\boldsymbol{\theta}), s(\boldsymbol{\theta})$ \\</span></span>
<span id="cb16-443"><a href="#cb16-443" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ precision $\epsilon$ \\</span></span>
<span id="cb16-444"><a href="#cb16-444" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ penalty parameters $\lambda_1, \lambda_2$ \\</span></span>
<span id="cb16-445"><a href="#cb16-445" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ decreasing factor $\boldsymbol \tau \in (0,1)$ for sequence of precisions</span></span>
<span id="cb16-446"><a href="#cb16-446" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb16-447"><a href="#cb16-447" aria-hidden="true" tabindex="-1"></a><span class="in">  \State \textbf{Output:} \\</span></span>
<span id="cb16-448"><a href="#cb16-448" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ $\boldsymbol{\theta}^{i+1} \in \mathbb{R}^d$</span></span>
<span id="cb16-449"><a href="#cb16-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-450"><a href="#cb16-450" aria-hidden="true" tabindex="-1"></a><span class="in">  \State \textbf{Initializations:} \\</span></span>
<span id="cb16-451"><a href="#cb16-451" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad \boldsymbol{\theta}^0 \in \mathbb{R}^d$ \\</span></span>
<span id="cb16-452"><a href="#cb16-452" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad \epsilon^0 = \boldsymbol \tau \operatorname{GAP}_{\mu = 10^{-8}}(\boldsymbol{\theta}^0)$ \\</span></span>
<span id="cb16-453"><a href="#cb16-453" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad \mu^0 = \mu_{opt}(\epsilon^0)$</span></span>
<span id="cb16-454"><a href="#cb16-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-455"><a href="#cb16-455" aria-hidden="true" tabindex="-1"></a><span class="in">  \Repeat</span></span>
<span id="cb16-456"><a href="#cb16-456" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $\epsilon^i_{\mu} = \epsilon^i - \mu^i \lambda_2 \frac{d}{2}$ \\</span></span>
<span id="cb16-457"><a href="#cb16-457" aria-hidden="true" tabindex="-1"></a><span class="in">    \Comment{FISTA}</span></span>
<span id="cb16-458"><a href="#cb16-458" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $k=2$ \Comment{new iterator}</span></span>
<span id="cb16-459"><a href="#cb16-459" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $\boldsymbol{\theta}_{\operatorname{FISTA}}^1 = \boldsymbol{\theta}_{\operatorname{FISTA}}^0 = \boldsymbol{\theta}^i$ \Comment{Initial parameters value}</span></span>
<span id="cb16-460"><a href="#cb16-460" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $t_{\mu} = \frac{1}{L(g + \lambda_2 s_{\mu})}$ \Comment{Compute stepsize with $L(g + \lambda_2 s_{\mu})$ the Lipschitz constant of $\nabla(g + \lambda_2 s_{\mu})$}</span></span>
<span id="cb16-461"><a href="#cb16-461" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb16-462"><a href="#cb16-462" aria-hidden="true" tabindex="-1"></a><span class="in">    \Repeat</span></span>
<span id="cb16-463"><a href="#cb16-463" aria-hidden="true" tabindex="-1"></a><span class="in">      \State $\boldsymbol{z} = \boldsymbol{\theta}_{\operatorname{FISTA}}^{k-1} + \frac{k-2}{k+1}(\boldsymbol{\theta}_{\operatorname{FISTA}}^{k-1} - \boldsymbol{\theta}_{\operatorname{FISTA}}^{k-2})$</span></span>
<span id="cb16-464"><a href="#cb16-464" aria-hidden="true" tabindex="-1"></a><span class="in">      \State $\boldsymbol{\theta}_{\operatorname{FISTA}}^k = \operatorname{prox}_{\lambda_1 h}(\boldsymbol{z} - t_{\mu} \nabla(g + \lambda_2 s_{\mu})(\boldsymbol{z}))$</span></span>
<span id="cb16-465"><a href="#cb16-465" aria-hidden="true" tabindex="-1"></a><span class="in">    \Until{$\operatorname{GAP}_{\mu}(\boldsymbol{\theta}_{\operatorname{FISTA}}^k) \le \epsilon_{\mu}^i$} </span></span>
<span id="cb16-466"><a href="#cb16-466" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb16-467"><a href="#cb16-467" aria-hidden="true" tabindex="-1"></a><span class="in">  \State $\boldsymbol{\theta}^{i+1} = \boldsymbol{\theta}_{\operatorname{FISTA}}^k$ \\</span></span>
<span id="cb16-468"><a href="#cb16-468" aria-hidden="true" tabindex="-1"></a><span class="in">  \State $\epsilon^i = \operatorname{GAP}_{\mu = \mu_i} \boldsymbol{\theta}^{i+1} + \mu^i \lambda_2 \frac{d}{2}$ \\</span></span>
<span id="cb16-469"><a href="#cb16-469" aria-hidden="true" tabindex="-1"></a><span class="in">  \State $\epsilon^{i+1} = \boldsymbol \tau \epsilon^{i}$ \\</span></span>
<span id="cb16-470"><a href="#cb16-470" aria-hidden="true" tabindex="-1"></a><span class="in">  \State $\mu^{i+1} = \mu_{opt}(\epsilon^{i+1})$</span></span>
<span id="cb16-471"><a href="#cb16-471" aria-hidden="true" tabindex="-1"></a><span class="in">  \Until{$\epsilon^i \le \epsilon$}</span></span>
<span id="cb16-472"><a href="#cb16-472" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb16-473"><a href="#cb16-473" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb16-474"><a href="#cb16-474" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb16-475"><a href="#cb16-475" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-476"><a href="#cb16-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-477"><a href="#cb16-477" aria-hidden="true" tabindex="-1"></a><span class="fu">## Reformulation of MGLasso for CONESTA algorithm {#reformulation-of-mglasso-for-conesta-algorithm}</span></span>
<span id="cb16-478"><a href="#cb16-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-479"><a href="#cb16-479" aria-hidden="true" tabindex="-1"></a>Using CONESTA for solving the MGLasso problem requires a reformulation in order </span>
<span id="cb16-480"><a href="#cb16-480" aria-hidden="true" tabindex="-1"></a>to comply with the form of loss function required by CONESTA. </span>
<span id="cb16-481"><a href="#cb16-481" aria-hidden="true" tabindex="-1"></a>The objective of MGLasso can be written as</span>
<span id="cb16-482"><a href="#cb16-482" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-483"><a href="#cb16-483" aria-hidden="true" tabindex="-1"></a>\operatorname{argmin} \frac{1}{2} ||\mathbf{Y} - \tilde{\mathbf{X}}</span>
<span id="cb16-484"><a href="#cb16-484" aria-hidden="true" tabindex="-1"></a>\tilde{\boldsymbol{\beta}}||_2^2 + \lambda_1 ||\tilde{\boldsymbol{\beta}}||_1 +</span>
<span id="cb16-485"><a href="#cb16-485" aria-hidden="true" tabindex="-1"></a>\lambda_2 \sum_{i&lt;j} ||\boldsymbol D_{ij} \tilde{\boldsymbol{\beta}}||_2, </span>
<span id="cb16-486"><a href="#cb16-486" aria-hidden="true" tabindex="-1"></a>$$ {#eq-refpbm}</span>
<span id="cb16-487"><a href="#cb16-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-488"><a href="#cb16-488" aria-hidden="true" tabindex="-1"></a>where $\mathbf{Y} = \operatorname{Vec}(\mathbf{X}) \in \mathbb{R}^{np},</span>
<span id="cb16-489"><a href="#cb16-489" aria-hidden="true" tabindex="-1"></a>\tilde{\boldsymbol{\beta}} = \operatorname{Vec(\boldsymbol{\beta})} \in</span>
<span id="cb16-490"><a href="#cb16-490" aria-hidden="true" tabindex="-1"></a>\mathbb{R}^{p (p-1)}, \tilde{\mathbf{X}}$ is a $\mathbb{R}^{<span class="co">[</span><span class="ot">np</span><span class="co">]</span>\times [p \times</span>
<span id="cb16-491"><a href="#cb16-491" aria-hidden="true" tabindex="-1"></a>(p-1)]}$ block-diagonal matrix with $\mathbf{X}^{\setminus i}$ on the $i$-th</span>
<span id="cb16-492"><a href="#cb16-492" aria-hidden="true" tabindex="-1"></a>block. The matrix $\boldsymbol D_{ij}$ is a $(p-1)\times p(p-1)$ matrix chosen</span>
<span id="cb16-493"><a href="#cb16-493" aria-hidden="true" tabindex="-1"></a>so that $\boldsymbol D_{ij} \tilde{\boldsymbol{\beta}} = \boldsymbol{\beta}^i -</span>
<span id="cb16-494"><a href="#cb16-494" aria-hidden="true" tabindex="-1"></a>\boldsymbol \tau_{ij} \boldsymbol{\beta}^j.$</span>
<span id="cb16-495"><a href="#cb16-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-496"><a href="#cb16-496" aria-hidden="true" tabindex="-1"></a>Note that we introduce this notation for simplicity of exposition, but, in</span>
<span id="cb16-497"><a href="#cb16-497" aria-hidden="true" tabindex="-1"></a>practice, the sparsity of the matrices $\boldsymbol D_{ij}$ allows a more</span>
<span id="cb16-498"><a href="#cb16-498" aria-hidden="true" tabindex="-1"></a>efficient implementation. Based on reformulation @eq-refpbm, we may apply</span>
<span id="cb16-499"><a href="#cb16-499" aria-hidden="true" tabindex="-1"></a>CONESTA to solve the objective of MGLasso for fixed $\lambda_1$ and $\lambda_2$.</span>
<span id="cb16-500"><a href="#cb16-500" aria-hidden="true" tabindex="-1"></a>The procedure is applied, for fixed $\lambda_1$, to a range of decreasing values</span>
<span id="cb16-501"><a href="#cb16-501" aria-hidden="true" tabindex="-1"></a>of $\lambda_2$ to obtain a hierarchical clustering. The corresponding</span>
<span id="cb16-502"><a href="#cb16-502" aria-hidden="true" tabindex="-1"></a>pseudo-code is given in the following algorithm where $(\mathbf{X}^i)^{\dagger}$</span>
<span id="cb16-503"><a href="#cb16-503" aria-hidden="true" tabindex="-1"></a>denotes the pseudo-inverse of $\mathbf{X}^i$ and $\epsilon_{fuse}$ the threshold</span>
<span id="cb16-504"><a href="#cb16-504" aria-hidden="true" tabindex="-1"></a>for merging clusters.</span>
<span id="cb16-505"><a href="#cb16-505" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;revision&gt;</span></span>
<span id="cb16-506"><a href="#cb16-506" aria-hidden="true" tabindex="-1"></a>We note here that problem in @eq-refpbm is of the same form as the optimization problem solved in the paper by @hadjselem2018: as they showed, CONESTA outperforms other optimization approaches such as the alternating direction method of multipliers <span class="co">[</span><span class="ot">ADMM, @Boyd2011</span><span class="co">]</span>, the excessive gap method <span class="co">[</span><span class="ot">EGM, @nesterov2005excessive</span><span class="co">]</span>, the classical FISTA with fixed smoothing and the inexact FISTA <span class="co">[</span><span class="ot">@schmidt2011convergence</span><span class="co">]</span>. Rather than repeating their experiments, we refer the reader to Section IV of their paper.</span>
<span id="cb16-507"><a href="#cb16-507" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/revision&gt;</span></span>
<span id="cb16-508"><a href="#cb16-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-509"><a href="#cb16-509" aria-hidden="true" tabindex="-1"></a><span class="in">```pseudocode</span></span>
<span id="cb16-510"><a href="#cb16-510" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: alg-mglasso</span></span>
<span id="cb16-511"><a href="#cb16-511" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-indent-size: "1.2em"</span></span>
<span id="cb16-512"><a href="#cb16-512" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-comment-delimiter: "//"</span></span>
<span id="cb16-513"><a href="#cb16-513" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-line-number: true</span></span>
<span id="cb16-514"><a href="#cb16-514" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-line-number-punc: ":"</span></span>
<span id="cb16-515"><a href="#cb16-515" aria-hidden="true" tabindex="-1"></a><span class="in">#| html-no-end: false</span></span>
<span id="cb16-516"><a href="#cb16-516" aria-hidden="true" tabindex="-1"></a><span class="in">#| pdf-placement: "htb!"</span></span>
<span id="cb16-517"><a href="#cb16-517" aria-hidden="true" tabindex="-1"></a><span class="in">#| pdf-line-number: true</span></span>
<span id="cb16-518"><a href="#cb16-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-519"><a href="#cb16-519" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb16-520"><a href="#cb16-520" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{MGLasso algorithm}</span></span>
<span id="cb16-521"><a href="#cb16-521" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb16-522"><a href="#cb16-522" aria-hidden="true" tabindex="-1"></a><span class="in">  \State \textbf{Inputs}: \\</span></span>
<span id="cb16-523"><a href="#cb16-523" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ Set of variables $\mathbf{X} = \{\mathbf{X}^1, \dots, \mathbf{X}^p \} \in \mathbb R^{n\times p}$ \\</span></span>
<span id="cb16-524"><a href="#cb16-524" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ Penalty parameters $\lambda_1 \ge 0, {\lambda_2}_{\operatorname{initial}} &gt; 0$ \\</span></span>
<span id="cb16-525"><a href="#cb16-525" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ Increasing factor $\eta &gt; 1$ for fusion penalties $\lambda_2$\\ </span></span>
<span id="cb16-526"><a href="#cb16-526" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ Fusion threshold $\epsilon_{fuse} \ge 0$</span></span>
<span id="cb16-527"><a href="#cb16-527" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb16-528"><a href="#cb16-528" aria-hidden="true" tabindex="-1"></a><span class="in">  \State \textbf{Outputs:} For $\lambda_1$ fixed and $\lambda_2$ from $0$ to ${\lambda_2}_{\operatorname{initial}} \times \eta^{(I)}$ with $I$ the number of iterations: \\</span></span>
<span id="cb16-529"><a href="#cb16-529" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ Regression vectors $\boldsymbol{\beta}(\lambda_1, \lambda_2) \in \mathbb R^{p \times (p-1)}$, \\</span></span>
<span id="cb16-530"><a href="#cb16-530" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ Clusters partition of variables indices in $K$ clusters: $C(\lambda_1, \lambda_2)$</span></span>
<span id="cb16-531"><a href="#cb16-531" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb16-532"><a href="#cb16-532" aria-hidden="true" tabindex="-1"></a><span class="in">  \State \textbf{Initializations:} \\</span></span>
<span id="cb16-533"><a href="#cb16-533" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ $\boldsymbol{\beta}^i = (\mathbf{X}^i)^{\dagger}\mathbf{X}^i$, $\forall i = 1, \dots, p$ for warm start in CONESTA solver \\</span></span>
<span id="cb16-534"><a href="#cb16-534" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ $C = \left \{\{1\}, \dots, \{p\}\right \}$ Initial clusters with one element per cluster. \\</span></span>
<span id="cb16-535"><a href="#cb16-535" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ Set $\lambda_2 = 0$ \\</span></span>
<span id="cb16-536"><a href="#cb16-536" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ Compute $\boldsymbol{\beta}$ using CONESTA solver \\</span></span>
<span id="cb16-537"><a href="#cb16-537" aria-hidden="true" tabindex="-1"></a><span class="in">    $\quad$ Update clusters $C$ with rule described in \textbf{while} loop.</span></span>
<span id="cb16-538"><a href="#cb16-538" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb16-539"><a href="#cb16-539" aria-hidden="true" tabindex="-1"></a><span class="in">  \State \textbf{Set:} $\lambda_2 = {\lambda_2}_{\operatorname{initial}}$ \\</span></span>
<span id="cb16-540"><a href="#cb16-540" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb16-541"><a href="#cb16-541" aria-hidden="true" tabindex="-1"></a><span class="in">  \Comment{Clustering path}</span></span>
<span id="cb16-542"><a href="#cb16-542" aria-hidden="true" tabindex="-1"></a><span class="in">  \While{$\operatorname{Card}(C) &gt; 1$}</span></span>
<span id="cb16-543"><a href="#cb16-543" aria-hidden="true" tabindex="-1"></a><span class="in">    \State Compute $\boldsymbol{\beta}$ using CONESTA solver with warm start from previous iteration \\</span></span>
<span id="cb16-544"><a href="#cb16-544" aria-hidden="true" tabindex="-1"></a><span class="in">    \Comment{Clusters update}</span></span>
<span id="cb16-545"><a href="#cb16-545" aria-hidden="true" tabindex="-1"></a><span class="in">    \State Compute pairwises distances $d(i,j)=\left \lVert \boldsymbol{\beta}^i - \boldsymbol \tau_{ij} \boldsymbol{\beta}^j \right \rVert_2$, $\forall i,j \in \{1, \dots, p\}$ \\</span></span>
<span id="cb16-546"><a href="#cb16-546" aria-hidden="true" tabindex="-1"></a><span class="in">    \State Determine clusters $C_k (k=1, \dots, K)$ with the rule $(i,j) \in C_k$ iff. $d(i,j) \le \epsilon_{fuse}$</span></span>
<span id="cb16-547"><a href="#cb16-547" aria-hidden="true" tabindex="-1"></a><span class="in">  </span></span>
<span id="cb16-548"><a href="#cb16-548" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $\lambda_2 = \lambda_2 \times \nu$</span></span>
<span id="cb16-549"><a href="#cb16-549" aria-hidden="true" tabindex="-1"></a><span class="in">  \EndWhile</span></span>
<span id="cb16-550"><a href="#cb16-550" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb16-551"><a href="#cb16-551" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb16-552"><a href="#cb16-552" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-553"><a href="#cb16-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-554"><a href="#cb16-554" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model selection {#model-selection}</span></span>
<span id="cb16-555"><a href="#cb16-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-556"><a href="#cb16-556" aria-hidden="true" tabindex="-1"></a>A crucial question for practical applications is the definition of a rule to</span>
<span id="cb16-557"><a href="#cb16-557" aria-hidden="true" tabindex="-1"></a>select the penalty parameters ($\lambda_1, \lambda_2$). This selection problem</span>
<span id="cb16-558"><a href="#cb16-558" aria-hidden="true" tabindex="-1"></a>operates at two levels: $\lambda_1$ controls the sparsity of the graphical</span>
<span id="cb16-559"><a href="#cb16-559" aria-hidden="true" tabindex="-1"></a>model, and $\lambda_2$ controls the number of clusters in the optimal clustering</span>
<span id="cb16-560"><a href="#cb16-560" aria-hidden="true" tabindex="-1"></a>partition. These two parameters are dealt with separately: the sparsity</span>
<span id="cb16-561"><a href="#cb16-561" aria-hidden="true" tabindex="-1"></a>parameter $\lambda_1$ is chosen via model selection, while the clustering</span>
<span id="cb16-562"><a href="#cb16-562" aria-hidden="true" tabindex="-1"></a>parameter $\lambda_2$ varies across a grid of values in order to obtain graphs</span>
<span id="cb16-563"><a href="#cb16-563" aria-hidden="true" tabindex="-1"></a>with different levels of granularity. The problem of model selection in</span>
<span id="cb16-564"><a href="#cb16-564" aria-hidden="true" tabindex="-1"></a>graphical models is difficult in the high dimensional case where the number of</span>
<span id="cb16-565"><a href="#cb16-565" aria-hidden="true" tabindex="-1"></a>samples is small compared to the number of variables, as classical Akaike</span>
<span id="cb16-566"><a href="#cb16-566" aria-hidden="true" tabindex="-1"></a>information criterion <span class="co">[</span><span class="ot">AIC, @akaike1998information</span><span class="co">]</span> and Bayesian information</span>
<span id="cb16-567"><a href="#cb16-567" aria-hidden="true" tabindex="-1"></a>criterion <span class="co">[</span><span class="ot">BIC, @schwarz1978estimating</span><span class="co">]</span> tend to perform poorly <span class="co">[</span><span class="ot">@Liu2010</span><span class="co">]</span>.</span>
<span id="cb16-568"><a href="#cb16-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-569"><a href="#cb16-569" aria-hidden="true" tabindex="-1"></a>In this paper, we focus on the StARS stability selection approach proposed by</span>
<span id="cb16-570"><a href="#cb16-570" aria-hidden="true" tabindex="-1"></a>@Liu2010 as suggested by some preliminary tests where we compared the Extended</span>
<span id="cb16-571"><a href="#cb16-571" aria-hidden="true" tabindex="-1"></a>BIC <span class="co">[</span><span class="ot">EBIC, @foygel2010extended</span><span class="co">]</span>, a model selection criterion calibrated with slope heuristics <span class="co">[</span><span class="ot">@baudry2012slope</span><span class="co">]</span>, the Rotation invariant criterion implemented in the Huge</span>
<span id="cb16-572"><a href="#cb16-572" aria-hidden="true" tabindex="-1"></a>package <span class="co">[</span><span class="ot">@zhao2012huge</span><span class="co">]</span>, the GGMSelect procedure <span class="co">[</span><span class="ot">@giraud2012graph</span><span class="co">]</span>,</span>
<span id="cb16-573"><a href="#cb16-573" aria-hidden="true" tabindex="-1"></a>cross-validation <span class="co">[</span><span class="ot">@bien2011sparse</span><span class="co">]</span> and StARS. The method uses $k$ subsamples of</span>
<span id="cb16-574"><a href="#cb16-574" aria-hidden="true" tabindex="-1"></a>data to estimate the associated graphs for a given range of $\lambda_1$ values.</span>
<span id="cb16-575"><a href="#cb16-575" aria-hidden="true" tabindex="-1"></a>For each value, a global instability of the graph edges is computed. The optimal</span>
<span id="cb16-576"><a href="#cb16-576" aria-hidden="true" tabindex="-1"></a>value of $\lambda_1$ is chosen so as to minimize the instability, as follows.</span>
<span id="cb16-577"><a href="#cb16-577" aria-hidden="true" tabindex="-1"></a>Let $\lambda^{(1)}_1, \dots, \lambda_1^{(K)}$ be a grid of sparsity</span>
<span id="cb16-578"><a href="#cb16-578" aria-hidden="true" tabindex="-1"></a>regularization parameters, and $S_1, \dots, S_N$ be the $N$ bootstrap samples</span>
<span id="cb16-579"><a href="#cb16-579" aria-hidden="true" tabindex="-1"></a>obtained by sampling the rows of the data set $\mathbf{X}$. For each</span>
<span id="cb16-580"><a href="#cb16-580" aria-hidden="true" tabindex="-1"></a>$k\in<span class="sc">\{</span>1,\ldots,K<span class="sc">\}</span>$ and for each $j\in<span class="sc">\{</span>1,\ldots, N<span class="sc">\}</span>$, we denote by</span>
<span id="cb16-581"><a href="#cb16-581" aria-hidden="true" tabindex="-1"></a>$\mathcal{A}^{k,j}(\mathbf{X})$ the adjacency matrix of the estimated graph</span>
<span id="cb16-582"><a href="#cb16-582" aria-hidden="true" tabindex="-1"></a>obtained by applying the inference algorithm to $S_n$ with regularization</span>
<span id="cb16-583"><a href="#cb16-583" aria-hidden="true" tabindex="-1"></a>parameter $\lambda_1^{(k)}$. For each possible edge $(s,t)\in<span class="sc">\{</span>1,\ldots,p<span class="sc">\}</span>^2$,</span>
<span id="cb16-584"><a href="#cb16-584" aria-hidden="true" tabindex="-1"></a>the probability of edge appearance is estimated empirically by </span>
<span id="cb16-585"><a href="#cb16-585" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-586"><a href="#cb16-586" aria-hidden="true" tabindex="-1"></a>\hat</span>
<span id="cb16-587"><a href="#cb16-587" aria-hidden="true" tabindex="-1"></a>\theta_{st}^{(k)} = \frac{1}{N} \sum_{j=1}^N \mathcal{A}^{k,j}_{st}.</span>
<span id="cb16-588"><a href="#cb16-588" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-589"><a href="#cb16-589" aria-hidden="true" tabindex="-1"></a>Define</span>
<span id="cb16-590"><a href="#cb16-590" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;old&gt;</span> $$\hat \xi_{st}(\Lambda) = 2 \hat \theta_{st} (\Lambda) \left ( 1 - \hat</span>
<span id="cb16-591"><a href="#cb16-591" aria-hidden="true" tabindex="-1"></a>\theta_{st} (\Lambda) \right )$$ </span>
<span id="cb16-592"><a href="#cb16-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-593"><a href="#cb16-593" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-594"><a href="#cb16-594" aria-hidden="true" tabindex="-1"></a>\hat \xi_{st}(\lambda_1^{(k)}) = 2 \hat \theta_{st}^{(k)}  \left ( 1 - \hat</span>
<span id="cb16-595"><a href="#cb16-595" aria-hidden="true" tabindex="-1"></a>\theta_{st}^{(k)} \right )</span>
<span id="cb16-596"><a href="#cb16-596" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-597"><a href="#cb16-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-598"><a href="#cb16-598" aria-hidden="true" tabindex="-1"></a>the empirical instability of edge $(s,t)$ (that</span>
<span id="cb16-599"><a href="#cb16-599" aria-hidden="true" tabindex="-1"></a>is, twice the variance of the Bernoulli indicator of edge $(s,t)$). The</span>
<span id="cb16-600"><a href="#cb16-600" aria-hidden="true" tabindex="-1"></a>instability level associated with $\lambda_1^{(k)}$ is given by</span>
<span id="cb16-601"><a href="#cb16-601" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-602"><a href="#cb16-602" aria-hidden="true" tabindex="-1"></a>\hat D(\lambda_1^{(k)}) = \frac{\sum_{s&lt;t} \hat \xi_{st}(\lambda_1^{(k)})}{</span>
<span id="cb16-603"><a href="#cb16-603" aria-hidden="true" tabindex="-1"></a>\binom{p}{2}}.</span>
<span id="cb16-604"><a href="#cb16-604" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-605"><a href="#cb16-605" aria-hidden="true" tabindex="-1"></a>StARS selects the optimal penalty parameter as follows</span>
<span id="cb16-606"><a href="#cb16-606" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-607"><a href="#cb16-607" aria-hidden="true" tabindex="-1"></a>\hat \lambda = \max_k\left<span class="sc">\{</span> \lambda_1^{(k)}: \hat D(\lambda_1^{(k)}) \le</span>
<span id="cb16-608"><a href="#cb16-608" aria-hidden="true" tabindex="-1"></a>\upsilon, k\in<span class="sc">\{</span>1,\ldots,K<span class="sc">\}</span> \right <span class="sc">\}</span>,</span>
<span id="cb16-609"><a href="#cb16-609" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-610"><a href="#cb16-610" aria-hidden="true" tabindex="-1"></a>where $\upsilon$ is the threshold chosen for the instability level.</span>
<span id="cb16-611"><a href="#cb16-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-612"><a href="#cb16-612" aria-hidden="true" tabindex="-1"></a><span class="fu"># Simulation experiments {#simulation-experiments}</span></span>
<span id="cb16-613"><a href="#cb16-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-614"><a href="#cb16-614" aria-hidden="true" tabindex="-1"></a>In this Section, we conduct a simulation study to evaluate the performance of</span>
<span id="cb16-615"><a href="#cb16-615" aria-hidden="true" tabindex="-1"></a>the MGLasso method, both in terms of clustering and support recovery. Receiver</span>
<span id="cb16-616"><a href="#cb16-616" aria-hidden="true" tabindex="-1"></a>Operating Characteristic (ROC) curves are used to evaluate the adequacy of the</span>
<span id="cb16-617"><a href="#cb16-617" aria-hidden="true" tabindex="-1"></a>inferred graphs with the  ground truth  for the MGLasso and GLasso in its</span>
<span id="cb16-618"><a href="#cb16-618" aria-hidden="true" tabindex="-1"></a>neighborhood selection version in the Erdös-Rényi <span class="co">[</span><span class="ot">@erdHos1960evolution</span><span class="co">]</span>, Scale-free <span class="co">[</span><span class="ot">@newman2001random</span><span class="co">]</span>, and Stochastic</span>
<span id="cb16-619"><a href="#cb16-619" aria-hidden="true" tabindex="-1"></a>Block Models <span class="co">[</span><span class="ot">SBM, @fienberg1981categorical</span><span class="co">]</span> frameworks. The Adjusted Rand indices are used to compare the</span>
<span id="cb16-620"><a href="#cb16-620" aria-hidden="true" tabindex="-1"></a>partitions obtained with MGLasso, hierarchical agglomerative clustering, and</span>
<span id="cb16-621"><a href="#cb16-621" aria-hidden="true" tabindex="-1"></a>K-means clustering in a stochastic block model framework.</span>
<span id="cb16-622"><a href="#cb16-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-623"><a href="#cb16-623" aria-hidden="true" tabindex="-1"></a><span class="fu">## Synthetic data models</span></span>
<span id="cb16-624"><a href="#cb16-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-625"><a href="#cb16-625" aria-hidden="true" tabindex="-1"></a>We consider three different synthetic network models: the Stochastic Block Model</span>
<span id="cb16-626"><a href="#cb16-626" aria-hidden="true" tabindex="-1"></a> <span class="co">[</span><span class="ot">@fienberg1981categorical</span><span class="co">]</span>, the Erdös-Renyi model <span class="co">[</span><span class="ot">@erdHos1960evolution</span><span class="co">]</span></span>
<span id="cb16-627"><a href="#cb16-627" aria-hidden="true" tabindex="-1"></a>and the Scale-Free model <span class="co">[</span><span class="ot">@newman2001random</span><span class="co">]</span>. In each case, Gaussian data is</span>
<span id="cb16-628"><a href="#cb16-628" aria-hidden="true" tabindex="-1"></a>generated by drawing $n$ independent realizations of a multivariate Gaussian</span>
<span id="cb16-629"><a href="#cb16-629" aria-hidden="true" tabindex="-1"></a>distribution $\mathcal N(0, \mathbf{\Sigma})$ where $\mathbf{\Sigma} \in</span>
<span id="cb16-630"><a href="#cb16-630" aria-hidden="true" tabindex="-1"></a>\mathbb{R}^{p \times p}$ and $\mathbf{\Omega} = \mathbf{\Sigma} ^{-1}$. The</span>
<span id="cb16-631"><a href="#cb16-631" aria-hidden="true" tabindex="-1"></a>support of $\mathbf{\Omega}$, equivalent to the network adjacency matrix, is</span>
<span id="cb16-632"><a href="#cb16-632" aria-hidden="true" tabindex="-1"></a>generated from the three different models. The difficulty level of the problem</span>
<span id="cb16-633"><a href="#cb16-633" aria-hidden="true" tabindex="-1"></a>is controlled by varying the ratio $\frac{n}{p}$ with $p$ fixed at $40$:</span>
<span id="cb16-634"><a href="#cb16-634" aria-hidden="true" tabindex="-1"></a>$\frac{n}{p}\in <span class="sc">\{</span>0.5,1,2<span class="sc">\}</span>$.</span>
<span id="cb16-635"><a href="#cb16-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-636"><a href="#cb16-636" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stochastic Block Model</span></span>
<span id="cb16-637"><a href="#cb16-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-638"><a href="#cb16-638" aria-hidden="true" tabindex="-1"></a>We construct a block-diagonal precision matrix $\mathbf{\Omega}$ as follows.</span>
<span id="cb16-639"><a href="#cb16-639" aria-hidden="true" tabindex="-1"></a>First, we generate the support of $\mathbf{\Omega}$ as shown in</span>
<span id="cb16-640"><a href="#cb16-640" aria-hidden="true" tabindex="-1"></a>@fig-model-sbm, denoted by $\boldsymbol A\in<span class="sc">\{</span>0,1<span class="sc">\}</span>^{p\times p}$. To do this,</span>
<span id="cb16-641"><a href="#cb16-641" aria-hidden="true" tabindex="-1"></a>the variables are first partitioned into $K = 5$ hidden groups, noted $C_1,</span>
<span id="cb16-642"><a href="#cb16-642" aria-hidden="true" tabindex="-1"></a>\dots, C_K$ described by a latent random variable $Z_i$, such that $Z_i = k$ if</span>
<span id="cb16-643"><a href="#cb16-643" aria-hidden="true" tabindex="-1"></a>$i = C_k$. $Z_i$ follows a multinomial distribution</span>
<span id="cb16-644"><a href="#cb16-644" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-645"><a href="#cb16-645" aria-hidden="true" tabindex="-1"></a>P(Z_i = k) = \pi_k, \quad</span>
<span id="cb16-646"><a href="#cb16-646" aria-hidden="true" tabindex="-1"></a>\forall k \in <span class="sc">\{</span>1, \dots, K<span class="sc">\}</span>,</span>
<span id="cb16-647"><a href="#cb16-647" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-648"><a href="#cb16-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-649"><a href="#cb16-649" aria-hidden="true" tabindex="-1"></a>where $\pi = (\pi_1, \dots, \pi_k)$ is the</span>
<span id="cb16-650"><a href="#cb16-650" aria-hidden="true" tabindex="-1"></a>vector of proportions of clusters whose sum is equal to one. The set of latent</span>
<span id="cb16-651"><a href="#cb16-651" aria-hidden="true" tabindex="-1"></a>variables is noted $\mathbf{Z} = <span class="sc">\{</span> Z_1, \dots, Z_K<span class="sc">\}</span>$. Conditionally to</span>
<span id="cb16-652"><a href="#cb16-652" aria-hidden="true" tabindex="-1"></a>$\mathbf{Z}$, $A_{ij}$ follows a Bernoulli distribution such that</span>
<span id="cb16-653"><a href="#cb16-653" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-654"><a href="#cb16-654" aria-hidden="true" tabindex="-1"></a>A_{ij}|Z_i =</span>
<span id="cb16-655"><a href="#cb16-655" aria-hidden="true" tabindex="-1"></a>k, Z_j = l \sim \mathcal{B}(\alpha_{kl}), \quad \forall k,l \in <span class="sc">\{</span>1, \dots,</span>
<span id="cb16-656"><a href="#cb16-656" aria-hidden="true" tabindex="-1"></a>K<span class="sc">\}</span>,</span>
<span id="cb16-657"><a href="#cb16-657" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-658"><a href="#cb16-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-659"><a href="#cb16-659" aria-hidden="true" tabindex="-1"></a>where $\alpha_{kl}$ is the probability of inter-cluster connectivity,</span>
<span id="cb16-660"><a href="#cb16-660" aria-hidden="true" tabindex="-1"></a>with $\alpha_{kl} = 0.01$ if $k\neq l$ and $\alpha_{ll} = 0,75$. For</span>
<span id="cb16-661"><a href="#cb16-661" aria-hidden="true" tabindex="-1"></a>$k\in<span class="sc">\{</span>1,\ldots, K<span class="sc">\}</span>$, we define $p_k = \sum_{i=1}^p \boldsymbol{1}_{<span class="sc">\{</span>Z_i =</span>
<span id="cb16-662"><a href="#cb16-662" aria-hidden="true" tabindex="-1"></a>k<span class="sc">\}</span>}$. The precision matrix $\mathbf{\Omega}$ of the graph is then calculated as</span>
<span id="cb16-663"><a href="#cb16-663" aria-hidden="true" tabindex="-1"></a>follows. We define $\Omega_{ij} = 0$ if $Z_i\neq Z_j$ ; otherwise, we define</span>
<span id="cb16-664"><a href="#cb16-664" aria-hidden="true" tabindex="-1"></a>$\Omega_{ij} = A_{ij}\omega_{ij}$ where, for all $i\in<span class="sc">\{</span>1,\ldots,p<span class="sc">\}</span>$ and for</span>
<span id="cb16-665"><a href="#cb16-665" aria-hidden="true" tabindex="-1"></a>all $j\in<span class="sc">\{</span>1,\ldots,p| Z_j = Z_i<span class="sc">\}</span>$, $\omega_{ij}$ is given by :</span>
<span id="cb16-666"><a href="#cb16-666" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-667"><a href="#cb16-667" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb16-668"><a href="#cb16-668" aria-hidden="true" tabindex="-1"></a>&amp;\omega_{ii} := \frac{1+\rho(p_{Z_i}-2)}{1+\rho(p_{Z_i}-2)-\rho^2(p_{Z_i}-1)};<span class="sc">\\</span></span>
<span id="cb16-669"><a href="#cb16-669" aria-hidden="true" tabindex="-1"></a>&amp;\omega_{ij} := \frac{-\rho}{1+\rho(p_{Z_i}-2)-\rho^2(p_{Z_i}-1)}.</span>
<span id="cb16-670"><a href="#cb16-670" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb16-671"><a href="#cb16-671" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-672"><a href="#cb16-672" aria-hidden="true" tabindex="-1"></a>If $\alpha_{ll}$ were to be equal to one, this construction of $\mathbf{\Omega}$</span>
<span id="cb16-673"><a href="#cb16-673" aria-hidden="true" tabindex="-1"></a>would make it possible to control the level of correlation between the variables</span>
<span id="cb16-674"><a href="#cb16-674" aria-hidden="true" tabindex="-1"></a>in each block to $\rho$. Introducing a more realistic scheme with</span>
<span id="cb16-675"><a href="#cb16-675" aria-hidden="true" tabindex="-1"></a>$\alpha_{ll}=0.75$ allows only to have an approximate control.</span>
<span id="cb16-676"><a href="#cb16-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-677"><a href="#cb16-677" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo = TRUE, message=FALSE, warning=FALSE}</span></span>
<span id="cb16-678"><a href="#cb16-678" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-model-sbm</span></span>
<span id="cb16-679"><a href="#cb16-679" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Adjacency matrix of a stochastic block model defined by $K=5$ classes with identical prior probabilities set to $\pi = 1/K$, inter-classes connection probability $\alpha_{kl}=0.01, k \neq l$, intra-classes connection probability $\alpha_{ll}=0.75$ and $p=40$ vertices.</span></span>
<span id="cb16-680"><a href="#cb16-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-681"><a href="#cb16-681" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mglasso)</span>
<span id="cb16-682"><a href="#cb16-682" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2020</span>)</span>
<span id="cb16-683"><a href="#cb16-683" aria-hidden="true" tabindex="-1"></a>sim_sbm <span class="ot">&lt;-</span> <span class="fu">sim_data</span>(</span>
<span id="cb16-684"><a href="#cb16-684" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> <span class="dv">40</span>,</span>
<span id="cb16-685"><a href="#cb16-685" aria-hidden="true" tabindex="-1"></a>  <span class="at">structure =</span> <span class="st">"block_diagonal"</span>,</span>
<span id="cb16-686"><a href="#cb16-686" aria-hidden="true" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="fu">rep</span>(<span class="dv">1</span> <span class="sc">/</span> <span class="dv">5</span>, <span class="dv">5</span>),</span>
<span id="cb16-687"><a href="#cb16-687" aria-hidden="true" tabindex="-1"></a>  <span class="at">prob_mat =</span> <span class="fu">diag</span>(<span class="fl">0.75</span>, <span class="dv">5</span>),</span>
<span id="cb16-688"><a href="#cb16-688" aria-hidden="true" tabindex="-1"></a>  <span class="at">rho =</span> <span class="fl">0.2</span>,</span>
<span id="cb16-689"><a href="#cb16-689" aria-hidden="true" tabindex="-1"></a>  <span class="at">inter_cluster_edge_prob =</span> <span class="fl">0.01</span></span>
<span id="cb16-690"><a href="#cb16-690" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-691"><a href="#cb16-691" aria-hidden="true" tabindex="-1"></a>gsbm <span class="ot">&lt;-</span> <span class="fu">adj_mat</span>(sim_sbm<span class="sc">$</span>graph)</span>
<span id="cb16-692"><a href="#cb16-692" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb16-693"><a href="#cb16-693" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as</span>(gsbm, <span class="st">"sparseMatrix"</span>),</span>
<span id="cb16-694"><a href="#cb16-694" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>,</span>
<span id="cb16-695"><a href="#cb16-695" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">""</span>,</span>
<span id="cb16-696"><a href="#cb16-696" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb16-697"><a href="#cb16-697" aria-hidden="true" tabindex="-1"></a>)   </span>
<span id="cb16-698"><a href="#cb16-698" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-699"><a href="#cb16-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-700"><a href="#cb16-700" aria-hidden="true" tabindex="-1"></a><span class="fu">### Erdös-Renyi Model</span></span>
<span id="cb16-701"><a href="#cb16-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-702"><a href="#cb16-702" aria-hidden="true" tabindex="-1"></a>The Erdös-Renyi model is a special case of the stochastic block model</span>
<span id="cb16-703"><a href="#cb16-703" aria-hidden="true" tabindex="-1"></a>where $\alpha_{kl} = \alpha_{ll} = \alpha$ is constant. We set the</span>
<span id="cb16-704"><a href="#cb16-704" aria-hidden="true" tabindex="-1"></a>density $\alpha$ of the graph to $0.1$; see @fig-model-erdos for an</span>
<span id="cb16-705"><a href="#cb16-705" aria-hidden="true" tabindex="-1"></a>example of the graph resulting from this model.</span>
<span id="cb16-706"><a href="#cb16-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-707"><a href="#cb16-707" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = TRUE}</span></span>
<span id="cb16-708"><a href="#cb16-708" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-model-erdos</span></span>
<span id="cb16-709"><a href="#cb16-709" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Adjacency matrix of an Erdös-Renyi model with probability of connection $\alpha = 0.1$ and $p=40$ vertices.</span></span>
<span id="cb16-710"><a href="#cb16-710" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2022</span>)</span>
<span id="cb16-711"><a href="#cb16-711" aria-hidden="true" tabindex="-1"></a>sim_erdos <span class="ot">&lt;-</span> <span class="fu">sim_data</span>(<span class="at">p =</span> <span class="dv">40</span>, <span class="at">structure =</span> <span class="st">"erdos"</span>, <span class="at">p_erdos =</span> <span class="fl">0.1</span>)</span>
<span id="cb16-712"><a href="#cb16-712" aria-hidden="true" tabindex="-1"></a>gerdos <span class="ot">&lt;-</span> <span class="fu">adj_mat</span>(sim_erdos<span class="sc">$</span>graph)</span>
<span id="cb16-713"><a href="#cb16-713" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(<span class="fu">as</span>(gerdos, <span class="st">"sparseMatrix"</span>), <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span>)</span>
<span id="cb16-714"><a href="#cb16-714" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-715"><a href="#cb16-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-716"><a href="#cb16-716" aria-hidden="true" tabindex="-1"></a><span class="fu">### Scale-free Model</span></span>
<span id="cb16-717"><a href="#cb16-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-718"><a href="#cb16-718" aria-hidden="true" tabindex="-1"></a>The Scale-free Model generates networks whose degree distributions</span>
<span id="cb16-719"><a href="#cb16-719" aria-hidden="true" tabindex="-1"></a>follow a power law. The graph starts with an initial chain graph of $2$</span>
<span id="cb16-720"><a href="#cb16-720" aria-hidden="true" tabindex="-1"></a>nodes. Then, new nodes are added to the graph one by one. Each new node</span>
<span id="cb16-721"><a href="#cb16-721" aria-hidden="true" tabindex="-1"></a>is connected to an existing node with a probability proportional to the</span>
<span id="cb16-722"><a href="#cb16-722" aria-hidden="true" tabindex="-1"></a>degree of the existing node. We set the number of edges in the graph to</span>
<span id="cb16-723"><a href="#cb16-723" aria-hidden="true" tabindex="-1"></a>$40$. An example of scale-free graph is shown in @fig-model-sfree.</span>
<span id="cb16-724"><a href="#cb16-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-725"><a href="#cb16-725" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = TRUE}</span></span>
<span id="cb16-726"><a href="#cb16-726" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-model-sfree</span></span>
<span id="cb16-727"><a href="#cb16-727" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Adjacency matrix of a Scale-free model with $40$ edges and $p=40$ nodes.</span></span>
<span id="cb16-728"><a href="#cb16-728" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2022</span>)</span>
<span id="cb16-729"><a href="#cb16-729" aria-hidden="true" tabindex="-1"></a>sim_sfree <span class="ot">&lt;-</span> <span class="fu">sim_data</span>(<span class="at">p =</span> <span class="dv">40</span>, <span class="at">structure =</span> <span class="st">"scale_free"</span>)</span>
<span id="cb16-730"><a href="#cb16-730" aria-hidden="true" tabindex="-1"></a>gsfree <span class="ot">&lt;-</span> <span class="fu">adj_mat</span>(sim_sfree<span class="sc">$</span>graph)</span>
<span id="cb16-731"><a href="#cb16-731" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(<span class="fu">as</span>(gsfree, <span class="st">"sparseMatrix"</span>), <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span>)</span>
<span id="cb16-732"><a href="#cb16-732" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-733"><a href="#cb16-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-734"><a href="#cb16-734" aria-hidden="true" tabindex="-1"></a><span class="fu">## Support recovery</span></span>
<span id="cb16-735"><a href="#cb16-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-736"><a href="#cb16-736" aria-hidden="true" tabindex="-1"></a>We compare the network structure learning performance of our approach to</span>
<span id="cb16-737"><a href="#cb16-737" aria-hidden="true" tabindex="-1"></a>that of GLasso in its neighborhood selection version using ROC curves.</span>
<span id="cb16-738"><a href="#cb16-738" aria-hidden="true" tabindex="-1"></a>In both GLasso and MGLasso, the sparsity is controlled by a</span>
<span id="cb16-739"><a href="#cb16-739" aria-hidden="true" tabindex="-1"></a>regularization parameter $\lambda_1$; however, MGLasso admits an</span>
<span id="cb16-740"><a href="#cb16-740" aria-hidden="true" tabindex="-1"></a>additional regularization parameter, $\lambda_2$, which controls the</span>
<span id="cb16-741"><a href="#cb16-741" aria-hidden="true" tabindex="-1"></a>strength of convex clustering. To compare the two methods, in each ROC</span>
<span id="cb16-742"><a href="#cb16-742" aria-hidden="true" tabindex="-1"></a>curve, we vary the parameter $\lambda_1$ while the parameter $\lambda_2$</span>
<span id="cb16-743"><a href="#cb16-743" aria-hidden="true" tabindex="-1"></a>(for MGLasso) is kept constant. We computed ROC curves for $4$ different</span>
<span id="cb16-744"><a href="#cb16-744" aria-hidden="true" tabindex="-1"></a>penalty levels for the $\lambda_2$ parameter; since GLasso does not</span>
<span id="cb16-745"><a href="#cb16-745" aria-hidden="true" tabindex="-1"></a>depend on $\lambda_2$, the GLasso ROC curves are replicated.</span>
<span id="cb16-746"><a href="#cb16-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-747"><a href="#cb16-747" aria-hidden="true" tabindex="-1"></a>In a decision rule associated with a sparsity penalty level $\lambda_1$, we recall the definition of the two following functions. The true positive rate is given by $\frac{TP(\lambda_1)}{TP(\lambda_1) + FN(\lambda_1)}.$ The false positive rate is defined as follows $1 - \frac{TN(\lambda_1)}{TN(\lambda_1) + FP(\lambda_1)}$, where $TP$ is the number of true positives, $TN$ the number of true negatives, $FN$ the number of false negatives and $FP$ the number of false positives. The ROC curve represents the true positive rate as a function of the false positive rate. For a given level of true positive rate, the best method minimizes the false positive rate.</span>
<span id="cb16-748"><a href="#cb16-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-749"><a href="#cb16-749" aria-hidden="true" tabindex="-1"></a>For each configuration ($n, p$ fixed), we generate $50$ replications and</span>
<span id="cb16-750"><a href="#cb16-750" aria-hidden="true" tabindex="-1"></a>their associated ROC curves, which are then averaged. The average ROC</span>
<span id="cb16-751"><a href="#cb16-751" aria-hidden="true" tabindex="-1"></a>curves for the three models are given in @fig-roc-erdos, @fig-roc-sfree</span>
<span id="cb16-752"><a href="#cb16-752" aria-hidden="true" tabindex="-1"></a>and @fig-roc-sbm by varying $\frac{n}{p}\in <span class="sc">\{</span>0.5,1,2<span class="sc">\}</span>$.</span>
<span id="cb16-753"><a href="#cb16-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-754"><a href="#cb16-754" aria-hidden="true" tabindex="-1"></a><span class="in">```{r roc_erdos, message=FALSE, echo = TRUE}</span></span>
<span id="cb16-755"><a href="#cb16-755" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-roc-erdos</span></span>
<span id="cb16-756"><a href="#cb16-756" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Mean ROC curves for MGLasso and GLasso graph inference in the Erdös-Renyi model. We varied the fusion penalty parameter of MGLasso $\lambda_2 \in \{0, 3.33, 10\}$ alongside the ratio $\frac{n}{p}\in \{0.5,1,2\}$. Within each panel, the ROC curve shows the True positive rate (y-axis) vs. the False positive rate (x-axis) for both MGLasso (blue) and GLasso (brown). Since GLasso does not have a fusion penalty, its ROC curves were replicated for panels belonging to the same row. We also plot the random classifier (dotted grey line). The results have been averaged over $50$ simulated datasets and suggest that MGLasso performs no worse than GLasso. For $\lambda_2 = 0$, the MGLasso approach is equivalent to GLasso in its neighborhood selection version.</span></span>
<span id="cb16-757"><a href="#cb16-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-758"><a href="#cb16-758" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb16-759"><a href="#cb16-759" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ghibli)</span>
<span id="cb16-760"><a href="#cb16-760" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"./data/roc_dtf_erdos.RData"</span>)</span>
<span id="cb16-761"><a href="#cb16-761" aria-hidden="true" tabindex="-1"></a>np.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"frac(n, p) == 0.5"</span>, <span class="st">"frac(n, p) == 1"</span>, <span class="st">"frac(n, p) == 2"</span>)</span>
<span id="cb16-762"><a href="#cb16-762" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(np.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0.5"</span>, <span class="st">"1"</span>, <span class="st">"2"</span>)</span>
<span id="cb16-763"><a href="#cb16-763" aria-hidden="true" tabindex="-1"></a>tv.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"lambda[2] == 0"</span>, <span class="st">"lambda[2] == 3.33"</span>, <span class="st">"lambda[2] == 10"</span>)</span>
<span id="cb16-764"><a href="#cb16-764" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(tv.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0"</span>, <span class="st">"3.33"</span>, <span class="st">"10"</span>)</span>
<span id="cb16-765"><a href="#cb16-765" aria-hidden="true" tabindex="-1"></a>roc_dtf_erdos <span class="ot">&lt;-</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(roc_dtf_erdos, tv <span class="sc">!=</span> <span class="fl">6.67</span>)</span>
<span id="cb16-766"><a href="#cb16-766" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(roc_dtf_erdos, <span class="fu">aes</span>(</span>
<span id="cb16-767"><a href="#cb16-767" aria-hidden="true" tabindex="-1"></a>  <span class="at">x     =</span> <span class="dv">100</span> <span class="sc">*</span> fpr,</span>
<span id="cb16-768"><a href="#cb16-768" aria-hidden="true" tabindex="-1"></a>  <span class="at">y     =</span> <span class="dv">100</span> <span class="sc">*</span> tpr,</span>
<span id="cb16-769"><a href="#cb16-769" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> method</span>
<span id="cb16-770"><a href="#cb16-770" aria-hidden="true" tabindex="-1"></a>)) <span class="sc">+</span></span>
<span id="cb16-771"><a href="#cb16-771" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb16-772"><a href="#cb16-772" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(np <span class="sc">~</span> tv, <span class="at">labeller =</span> <span class="fu">labeller</span>(</span>
<span id="cb16-773"><a href="#cb16-773" aria-hidden="true" tabindex="-1"></a>    <span class="at">np =</span> <span class="fu">as_labeller</span>(np.labs, label_parsed),</span>
<span id="cb16-774"><a href="#cb16-774" aria-hidden="true" tabindex="-1"></a>    <span class="at">tv =</span> <span class="fu">as_labeller</span>(tv.labs, label_parsed)</span>
<span id="cb16-775"><a href="#cb16-775" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb16-776"><a href="#cb16-776" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(</span>
<span id="cb16-777"><a href="#cb16-777" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> <span class="dv">0</span>,</span>
<span id="cb16-778"><a href="#cb16-778" aria-hidden="true" tabindex="-1"></a>    <span class="at">slope =</span> <span class="dv">1</span>,</span>
<span id="cb16-779"><a href="#cb16-779" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">"dashed"</span>,</span>
<span id="cb16-780"><a href="#cb16-780" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"grey"</span></span>
<span id="cb16-781"><a href="#cb16-781" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb16-782"><a href="#cb16-782" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"False Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb16-783"><a href="#cb16-783" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"True Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb16-784"><a href="#cb16-784" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb16-785"><a href="#cb16-785" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(</span>
<span id="cb16-786"><a href="#cb16-786" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Method"</span>,</span>
<span id="cb16-787"><a href="#cb16-787" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"GLasso"</span>, <span class="st">"MGLasso"</span>),</span>
<span id="cb16-788"><a href="#cb16-788" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> ghibli<span class="sc">::</span><span class="fu">ghibli_palette</span>(<span class="st">"MarnieMedium1"</span>)[<span class="dv">5</span><span class="sc">:</span><span class="dv">6</span>]</span>
<span id="cb16-789"><a href="#cb16-789" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb16-790"><a href="#cb16-790" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-791"><a href="#cb16-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-792"><a href="#cb16-792" aria-hidden="true" tabindex="-1"></a><span class="in">```{r roc_scale_free , message=FALSE, echo = TRUE}</span></span>
<span id="cb16-793"><a href="#cb16-793" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-roc-sfree</span></span>
<span id="cb16-794"><a href="#cb16-794" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Mean ROC curves for MGLasso and GLasso graph inference in the Scale-free model. We varied the fusion penalty parameter of MGLasso $\lambda_2 \in \{0, 3.33, 10\}$ alongside the ratio $\frac{n}{p}\in \{0.5,1,2\}$. Within each panel, the ROC curve shows the True positive rate (y-axis) vs. the False positive rate (x-axis) for both MGLasso (blue) and GLasso (brown). Since GLasso does not have a fusion penalty, its ROC curves were replicated for panels belonging to the same row. We also plot the random classifier (dotted grey line). The results have been averaged over $50$ simulated datasets and suggest that MGLasso performs no worse than GLasso. For $\lambda_2 = 0$, the MGLasso approach is equivalent to Glasso in its neighborhood selection version.</span></span>
<span id="cb16-795"><a href="#cb16-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-796"><a href="#cb16-796" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"./data/roc_dtf_sfree.RData"</span>)</span>
<span id="cb16-797"><a href="#cb16-797" aria-hidden="true" tabindex="-1"></a>np.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"frac(n, p) == 0.5"</span>, <span class="st">"frac(n, p) == 1"</span>, <span class="st">"frac(n, p) == 2"</span>)</span>
<span id="cb16-798"><a href="#cb16-798" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(np.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0.5"</span>, <span class="st">"1"</span>, <span class="st">"2"</span>)</span>
<span id="cb16-799"><a href="#cb16-799" aria-hidden="true" tabindex="-1"></a>tv.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"lambda[2] == 0"</span>, <span class="st">"lambda[2] == 3.33"</span>, <span class="st">"lambda[2] == 10"</span>)</span>
<span id="cb16-800"><a href="#cb16-800" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(tv.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0"</span>, <span class="st">"3.33"</span>, <span class="st">"10"</span>)</span>
<span id="cb16-801"><a href="#cb16-801" aria-hidden="true" tabindex="-1"></a>roc_dtf_sfree <span class="ot">&lt;-</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(roc_dtf_sfree, tv <span class="sc">!=</span> <span class="fl">6.67</span>)</span>
<span id="cb16-802"><a href="#cb16-802" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(roc_dtf_sfree, <span class="fu">aes</span>(</span>
<span id="cb16-803"><a href="#cb16-803" aria-hidden="true" tabindex="-1"></a>  <span class="at">x     =</span> <span class="dv">100</span> <span class="sc">*</span> fpr,</span>
<span id="cb16-804"><a href="#cb16-804" aria-hidden="true" tabindex="-1"></a>  <span class="at">y     =</span> <span class="dv">100</span> <span class="sc">*</span> tpr,</span>
<span id="cb16-805"><a href="#cb16-805" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> method</span>
<span id="cb16-806"><a href="#cb16-806" aria-hidden="true" tabindex="-1"></a>)) <span class="sc">+</span></span>
<span id="cb16-807"><a href="#cb16-807" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb16-808"><a href="#cb16-808" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(np <span class="sc">~</span> tv, <span class="at">labeller =</span> <span class="fu">labeller</span>(</span>
<span id="cb16-809"><a href="#cb16-809" aria-hidden="true" tabindex="-1"></a>    <span class="at">np =</span> <span class="fu">as_labeller</span>(np.labs, label_parsed),</span>
<span id="cb16-810"><a href="#cb16-810" aria-hidden="true" tabindex="-1"></a>    <span class="at">tv =</span> <span class="fu">as_labeller</span>(tv.labs, label_parsed)</span>
<span id="cb16-811"><a href="#cb16-811" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb16-812"><a href="#cb16-812" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(</span>
<span id="cb16-813"><a href="#cb16-813" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> <span class="dv">0</span>,</span>
<span id="cb16-814"><a href="#cb16-814" aria-hidden="true" tabindex="-1"></a>    <span class="at">slope =</span> <span class="dv">1</span>,</span>
<span id="cb16-815"><a href="#cb16-815" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">"dashed"</span>,</span>
<span id="cb16-816"><a href="#cb16-816" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"grey"</span></span>
<span id="cb16-817"><a href="#cb16-817" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb16-818"><a href="#cb16-818" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"False Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb16-819"><a href="#cb16-819" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"True Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb16-820"><a href="#cb16-820" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb16-821"><a href="#cb16-821" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(</span>
<span id="cb16-822"><a href="#cb16-822" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Method"</span>,</span>
<span id="cb16-823"><a href="#cb16-823" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"GLasso"</span>, <span class="st">"MGLasso"</span>),</span>
<span id="cb16-824"><a href="#cb16-824" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">ghibli_palette</span>(<span class="st">"MarnieMedium1"</span>)[<span class="dv">5</span><span class="sc">:</span><span class="dv">6</span>]</span>
<span id="cb16-825"><a href="#cb16-825" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb16-826"><a href="#cb16-826" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-827"><a href="#cb16-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-828"><a href="#cb16-828" aria-hidden="true" tabindex="-1"></a><span class="in">```{r roc_sbm , message=FALSE, echo = TRUE}</span></span>
<span id="cb16-829"><a href="#cb16-829" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-roc-sbm</span></span>
<span id="cb16-830"><a href="#cb16-830" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Mean ROC curves for MGLasso and GLasso graph inference in the stochastic block model. We varied the fusion penalty parameter of MGLasso $\lambda_2 \in \{0, 3.33, 10\}$ alongside the ratio $\frac{n}{p}\in \{0.5,1,2\}$. Within each panel, the ROC curve shows the True positive rate (y-axis) vs. the False positive rate (x-axis) for both MGLasso (blue) and GLasso (brown). Since GLasso does not have a fusion penalty, its ROC curves were replicated for panels belonging to the same row. We also plot the random classifier (dotted grey line). The results have been averaged over $50$ simulated datasets and suggest that MGLasso performs no worse than GLasso. For $\lambda_2 = 0$, the MGLasso approach is equivalent to Glasso in its neighborhood selection version.</span></span>
<span id="cb16-831"><a href="#cb16-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-832"><a href="#cb16-832" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"./data/roc_dtf_sbm.RData"</span>)</span>
<span id="cb16-833"><a href="#cb16-833" aria-hidden="true" tabindex="-1"></a>np.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"frac(n, p) == 0.5"</span>, <span class="st">"frac(n, p) == 1"</span>, <span class="st">"frac(n, p) == 2"</span>)</span>
<span id="cb16-834"><a href="#cb16-834" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(np.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0.5"</span>, <span class="st">"1"</span>, <span class="st">"2"</span>)</span>
<span id="cb16-835"><a href="#cb16-835" aria-hidden="true" tabindex="-1"></a>tv.labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"lambda[2] == 0"</span>, <span class="st">"lambda[2] == 3.33"</span>, <span class="st">"lambda[2] == 10"</span>)</span>
<span id="cb16-836"><a href="#cb16-836" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(tv.labs) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"0"</span>, <span class="st">"3.33"</span>, <span class="st">"10"</span>)</span>
<span id="cb16-837"><a href="#cb16-837" aria-hidden="true" tabindex="-1"></a>roc_dtf_sbm <span class="ot">&lt;-</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(roc_dtf_sbm, tv <span class="sc">!=</span> <span class="fl">6.67</span>)</span>
<span id="cb16-838"><a href="#cb16-838" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(roc_dtf_sbm, <span class="fu">aes</span>(</span>
<span id="cb16-839"><a href="#cb16-839" aria-hidden="true" tabindex="-1"></a>  <span class="at">x     =</span> <span class="dv">100</span> <span class="sc">*</span> fpr,</span>
<span id="cb16-840"><a href="#cb16-840" aria-hidden="true" tabindex="-1"></a>  <span class="at">y     =</span> <span class="dv">100</span> <span class="sc">*</span> tpr,</span>
<span id="cb16-841"><a href="#cb16-841" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> method</span>
<span id="cb16-842"><a href="#cb16-842" aria-hidden="true" tabindex="-1"></a>)) <span class="sc">+</span></span>
<span id="cb16-843"><a href="#cb16-843" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb16-844"><a href="#cb16-844" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(np <span class="sc">~</span> tv, <span class="at">labeller =</span> <span class="fu">labeller</span>(</span>
<span id="cb16-845"><a href="#cb16-845" aria-hidden="true" tabindex="-1"></a>    <span class="at">np =</span> <span class="fu">as_labeller</span>(np.labs, label_parsed),</span>
<span id="cb16-846"><a href="#cb16-846" aria-hidden="true" tabindex="-1"></a>    <span class="at">tv =</span> <span class="fu">as_labeller</span>(tv.labs, label_parsed)</span>
<span id="cb16-847"><a href="#cb16-847" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb16-848"><a href="#cb16-848" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(</span>
<span id="cb16-849"><a href="#cb16-849" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> <span class="dv">0</span>,</span>
<span id="cb16-850"><a href="#cb16-850" aria-hidden="true" tabindex="-1"></a>    <span class="at">slope =</span> <span class="dv">1</span>,</span>
<span id="cb16-851"><a href="#cb16-851" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">"dashed"</span>,</span>
<span id="cb16-852"><a href="#cb16-852" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"grey"</span></span>
<span id="cb16-853"><a href="#cb16-853" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb16-854"><a href="#cb16-854" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"False Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb16-855"><a href="#cb16-855" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"True Positive Rate"</span>) <span class="sc">+</span></span>
<span id="cb16-856"><a href="#cb16-856" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb16-857"><a href="#cb16-857" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(</span>
<span id="cb16-858"><a href="#cb16-858" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Method"</span>,</span>
<span id="cb16-859"><a href="#cb16-859" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"GLasso"</span>, <span class="st">"MGLasso"</span>),</span>
<span id="cb16-860"><a href="#cb16-860" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">ghibli_palette</span>(<span class="st">"MarnieMedium1"</span>)[<span class="dv">5</span><span class="sc">:</span><span class="dv">6</span>]</span>
<span id="cb16-861"><a href="#cb16-861" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb16-862"><a href="#cb16-862" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-863"><a href="#cb16-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-864"><a href="#cb16-864" aria-hidden="true" tabindex="-1"></a>Based on these empirical results, we first observe that, in all the considered</span>
<span id="cb16-865"><a href="#cb16-865" aria-hidden="true" tabindex="-1"></a>simulation models, MGLasso improves over GLasso in terms of support recovery in</span>
<span id="cb16-866"><a href="#cb16-866" aria-hidden="true" tabindex="-1"></a>the high-dimensional setting where $p&lt;n$. In addition, in the absence of</span>
<span id="cb16-867"><a href="#cb16-867" aria-hidden="true" tabindex="-1"></a>a fusion penalty, i.e., $\lambda_2 = 0$, MGLasso performs no worse than GLasso in</span>
<span id="cb16-868"><a href="#cb16-868" aria-hidden="true" tabindex="-1"></a>each of the $3$ models. However, for $\lambda_2&gt;0$, increasing penalty value</span>
<span id="cb16-869"><a href="#cb16-869" aria-hidden="true" tabindex="-1"></a>does not seem to significantly improve the support recovery performances for the</span>
<span id="cb16-870"><a href="#cb16-870" aria-hidden="true" tabindex="-1"></a>MGLasso, as we observe similar results for $\lambda_2=3.3,10$. Preliminary</span>
<span id="cb16-871"><a href="#cb16-871" aria-hidden="true" tabindex="-1"></a>analyses show that, as $\lambda_2$ increases, the estimates of the regression</span>
<span id="cb16-872"><a href="#cb16-872" aria-hidden="true" tabindex="-1"></a>vectors are shrunk towards $0$. This shrinkage effect of group-fused penalty</span>
<span id="cb16-873"><a href="#cb16-873" aria-hidden="true" tabindex="-1"></a>terms was also observed in <span class="co">[</span><span class="ot">@chu2021adaptive</span><span class="co">]</span>. Note that the performance of the</span>
<span id="cb16-874"><a href="#cb16-874" aria-hidden="true" tabindex="-1"></a>MGLasso deteriorates comparatively to GLasso when the inter-clusters edge</span>
<span id="cb16-875"><a href="#cb16-875" aria-hidden="true" tabindex="-1"></a>connection probability of the stochastic block model is high.</span>
<span id="cb16-876"><a href="#cb16-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-877"><a href="#cb16-877" aria-hidden="true" tabindex="-1"></a><span class="fu">## Clustering</span></span>
<span id="cb16-878"><a href="#cb16-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-879"><a href="#cb16-879" aria-hidden="true" tabindex="-1"></a>In order to study clustering performance, we compared the partitions estimated</span>
<span id="cb16-880"><a href="#cb16-880" aria-hidden="true" tabindex="-1"></a>by MGLasso, Hierarchical Agglomerative Clustering (HAC) with Ward's distance and</span>
<span id="cb16-881"><a href="#cb16-881" aria-hidden="true" tabindex="-1"></a>K-means to the true partition in a stochastic block model framework. Euclidean</span>
<span id="cb16-882"><a href="#cb16-882" aria-hidden="true" tabindex="-1"></a>distances between variables are used for HAC and K-means. The criterion used for</span>
<span id="cb16-883"><a href="#cb16-883" aria-hidden="true" tabindex="-1"></a>the comparison is the adjusted Rand index (ARI). We studied the influence of the</span>
<span id="cb16-884"><a href="#cb16-884" aria-hidden="true" tabindex="-1"></a>correlation level inside clusters on the clustering performances through two</span>
<span id="cb16-885"><a href="#cb16-885" aria-hidden="true" tabindex="-1"></a>different parameters: $\rho \in <span class="sc">\{</span> 0.1, 0.3 <span class="sc">\}</span>$; the vector of cluster</span>
<span id="cb16-886"><a href="#cb16-886" aria-hidden="true" tabindex="-1"></a>proportions is fixed at $\mathbf \pi = (1/5, \dots, 1/5)$. Hundred Gaussian data sets were then simulated for each  configuration ($\rho$, $n/p$</span>
<span id="cb16-887"><a href="#cb16-887" aria-hidden="true" tabindex="-1"></a>fixed).The optimal sparsity penalty for MGLasso was chosen by the Stability</span>
<span id="cb16-888"><a href="#cb16-888" aria-hidden="true" tabindex="-1"></a>Approach to Regularization Selection (StARS) method <span class="co">[</span><span class="ot">@Liu2010</span><span class="co">]</span>. In practice, we</span>
<span id="cb16-889"><a href="#cb16-889" aria-hidden="true" tabindex="-1"></a>estimated a stability-like parameter in a sample of graphs simulated via the</span>
<span id="cb16-890"><a href="#cb16-890" aria-hidden="true" tabindex="-1"></a>stochastic block model. This estimation of edge variability was then used as the</span>
<span id="cb16-891"><a href="#cb16-891" aria-hidden="true" tabindex="-1"></a>threshold for the StARS method. The parameter $\lambda_2$ has been varied. </span>
<span id="cb16-892"><a href="#cb16-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-893"><a href="#cb16-893" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo = TRUE}</span></span>
<span id="cb16-894"><a href="#cb16-894" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb16-895"><a href="#cb16-895" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-ari-low-cor</span></span>
<span id="cb16-896"><a href="#cb16-896" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Boxplots of Adjusted Rand Indices for the stochastic block model with $5$ classes and $p=40$ variables for a correlation level $\rho=0.1$. The number of estimated clusters $\{5,10,15,20\}$ vary alongside the ratio $\frac{n}{p}\in \{0.5,1,2\}$. Within each panel, the boxplots of ARI between true partition (with $5$ classes) and estimated clustering partitions on $100$ simulated datasets for $k$-means (blue), hierarchical agglomerative clustering (yellow), and MGLasso (brown) methods are plotted against the ratio $\frac{n}{p}.$  The cluster assignments of MGLasso are computed from a distance between estimated regression vectors for a given value of $\lambda_2.$ Missing boxplots for MGLasso thus mean computed partitions in the grid of values of $\lambda_2$ do not yield the fixed number of clusters. The higher the ARI values, the better the estimated clustering partition is.</span></span>
<span id="cb16-897"><a href="#cb16-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-898"><a href="#cb16-898" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"./data/rand_dt_lower_cor_sbm.RData"</span>)</span>
<span id="cb16-899"><a href="#cb16-899" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_res</span>(</span>
<span id="cb16-900"><a href="#cb16-900" aria-hidden="true" tabindex="-1"></a>  dt_rand,</span>
<span id="cb16-901"><a href="#cb16-901" aria-hidden="true" tabindex="-1"></a>  <span class="at">crit_ =</span> <span class="st">"rand"</span>,</span>
<span id="cb16-902"><a href="#cb16-902" aria-hidden="true" tabindex="-1"></a>  <span class="at">ncluster_ =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>),</span>
<span id="cb16-903"><a href="#cb16-903" aria-hidden="true" tabindex="-1"></a>  <span class="at">cor_ =</span> <span class="fl">0.25</span>,</span>
<span id="cb16-904"><a href="#cb16-904" aria-hidden="true" tabindex="-1"></a>  <span class="at">np_ =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb16-905"><a href="#cb16-905" aria-hidden="true" tabindex="-1"></a>  <span class="at">main =</span> <span class="st">""</span></span>
<span id="cb16-906"><a href="#cb16-906" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-907"><a href="#cb16-907" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-908"><a href="#cb16-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-909"><a href="#cb16-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-910"><a href="#cb16-910" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo = TRUE}</span></span>
<span id="cb16-911"><a href="#cb16-911" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb16-912"><a href="#cb16-912" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-ari-high-cor</span></span>
<span id="cb16-913"><a href="#cb16-913" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Boxplots of Adjusted Rand Indices for the stochastic block model with $5$ classes and $p=40$ variables for a correlation level $\rho=0.3$. The number of estimated clusters $\{5,10,15,20\}$ vary alongside the ratio $\frac{n}{p}\in \{0.5,1,2\}$. Within each panel, the boxplots of ARI between true partition (with $5$ classes) and estimated clustering partitions on $100$ simulated datasets for $k$-means (blue), hierarchical agglomerative clustering (yellow), and MGLasso (brown) methods are plotted against the ratio $\frac{n}{p}.$  The cluster assignments of MGLasso are computed from a distance between estimated regression vectors for a given value of $\lambda_2.$ The higher the ARI values, the better the estimated clustering partition is.</span></span>
<span id="cb16-914"><a href="#cb16-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-915"><a href="#cb16-915" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"./data/rand_dt_higher_cor_sbm.RData"</span>)</span>
<span id="cb16-916"><a href="#cb16-916" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_res</span>(</span>
<span id="cb16-917"><a href="#cb16-917" aria-hidden="true" tabindex="-1"></a>  dt_rand,</span>
<span id="cb16-918"><a href="#cb16-918" aria-hidden="true" tabindex="-1"></a>  <span class="at">crit_ =</span> <span class="st">"rand"</span>,</span>
<span id="cb16-919"><a href="#cb16-919" aria-hidden="true" tabindex="-1"></a>  <span class="at">ncluster_ =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>),</span>
<span id="cb16-920"><a href="#cb16-920" aria-hidden="true" tabindex="-1"></a>  <span class="at">cor_ =</span> <span class="fl">0.95</span>,</span>
<span id="cb16-921"><a href="#cb16-921" aria-hidden="true" tabindex="-1"></a>  <span class="at">np_ =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb16-922"><a href="#cb16-922" aria-hidden="true" tabindex="-1"></a>  <span class="at">main =</span> <span class="st">""</span></span>
<span id="cb16-923"><a href="#cb16-923" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb16-924"><a href="#cb16-924" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-925"><a href="#cb16-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-926"><a href="#cb16-926" aria-hidden="true" tabindex="-1"></a>The expected empirical evidence that MGLasso would work reasonably well for strongly correlated variables is somehow highlighted in @fig-ari-low-cor and @fig-ari-high-cor. The performances of MGLasso slightly improve when going from @fig-ari-low-cor to @fig-ari-high-cor, which corresponds to correlation levels of 0.1 and 0.3 between variables belonging to the same block, respectively. We observe the same trend for the HAC and the k-means. Compared to these two approaches, the MGLasso presents the lowest values of adjusted Rand indices, thus suggesting a lower quality of clustering. It should be noted that the performance of MGLasso can be sensitive to the selection of the Lasso penalty parameter and the threshold fixed to determine clusters' fusion. In practice, this fusion threshold is varied in a grid of values close to zero and lower than $10^{-3}$. The value leading to the maximum number of intermediate clusters in the clustering path is chosen. Using non-trivial weights could also improve the overall performance of MGLasso.</span>
<span id="cb16-927"><a href="#cb16-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-928"><a href="#cb16-928" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;revision&gt;</span> </span>
<span id="cb16-929"><a href="#cb16-929" aria-hidden="true" tabindex="-1"></a>During the revision of this paper, an interesting question was raised regarding the behavior of the algorithm in a phylogenetic-based model. To investigate this, extensive numerical experiments were conducted on a phylogenetic-based model that evaluates only clustering performances. The results showed that the MGLASSO algorithm's performance improves, and the method performs as well as some state-of-the-art clustering approaches, including vanilla convex clustering and spectral clustering. In phylogenetic-based models, adjusted Rand indices can be computed between the estimated partition with $k$ clusters and the true partition in $k$ clusters computed from the tree used for the simulation procedure. This differs from the clustering performance evaluation scheme applied in the stochastic block model, where the true partition is considered fixed.</span>
<span id="cb16-930"><a href="#cb16-930" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/revision&gt;</span></span>
<span id="cb16-931"><a href="#cb16-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-932"><a href="#cb16-932" aria-hidden="true" tabindex="-1"></a><span class="fu"># Applications </span></span>
<span id="cb16-933"><a href="#cb16-933" aria-hidden="true" tabindex="-1"></a>To illustrate the proposed simultaneous graphs and clusters inference approach,</span>
<span id="cb16-934"><a href="#cb16-934" aria-hidden="true" tabindex="-1"></a>we present analyses where the MGLasso model is applied to microbial association</span>
<span id="cb16-935"><a href="#cb16-935" aria-hidden="true" tabindex="-1"></a>data for the study of multiscale networks between operational taxonomic units</span>
<span id="cb16-936"><a href="#cb16-936" aria-hidden="true" tabindex="-1"></a>and to transcriptomic and methylation genotypes for multi-omics data</span>
<span id="cb16-937"><a href="#cb16-937" aria-hidden="true" tabindex="-1"></a>integration.</span>
<span id="cb16-938"><a href="#cb16-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-939"><a href="#cb16-939" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application to microbial associations in gut data</span></span>
<span id="cb16-940"><a href="#cb16-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-941"><a href="#cb16-941" aria-hidden="true" tabindex="-1"></a>We analyze microbial associations in human gut microbiome data acquired from the</span>
<span id="cb16-942"><a href="#cb16-942" aria-hidden="true" tabindex="-1"></a>round $1$ of the American Gut Project (AGP, @mcdonald2018american) for $p = 127$</span>
<span id="cb16-943"><a href="#cb16-943" aria-hidden="true" tabindex="-1"></a>operational taxonomic units (OTUs) and $n = 289$ individuals samples. The count</span>
<span id="cb16-944"><a href="#cb16-944" aria-hidden="true" tabindex="-1"></a>of microbial OTUs is an indicator of the abundance of underlying microbial</span>
<span id="cb16-945"><a href="#cb16-945" aria-hidden="true" tabindex="-1"></a>populations. Here, we investigate the network and clustering structures of the</span>
<span id="cb16-946"><a href="#cb16-946" aria-hidden="true" tabindex="-1"></a>OTUs for different levels of granularity on the processed data included in the</span>
<span id="cb16-947"><a href="#cb16-947" aria-hidden="true" tabindex="-1"></a>SpiecEasi R package (see @Kurtz2015 for details). The data is first normalized</span>
<span id="cb16-948"><a href="#cb16-948" aria-hidden="true" tabindex="-1"></a>to have a unit-sum per sample and to remove biases. Then, a centered log-ratio</span>
<span id="cb16-949"><a href="#cb16-949" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">clr, @aitchison1982statistical</span><span class="co">]</span> transformation with an added unit pseudo-count</span>
<span id="cb16-950"><a href="#cb16-950" aria-hidden="true" tabindex="-1"></a>is applied to come back to an unconstrained Euclidean space. For fitting the</span>
<span id="cb16-951"><a href="#cb16-951" aria-hidden="true" tabindex="-1"></a>MGLasso model, we select the Lasso penalty parameter $\lambda_1$ via the StARS</span>
<span id="cb16-952"><a href="#cb16-952" aria-hidden="true" tabindex="-1"></a>approach with threshold $\upsilon = 0.05$ and vary the fusion penalty</span>
<span id="cb16-953"><a href="#cb16-953" aria-hidden="true" tabindex="-1"></a>$\lambda_2$ in the interval $<span class="co">[</span><span class="ot">0, 20</span><span class="co">]</span>$ with irregular steps. The CPU time taken</span>
<span id="cb16-954"><a href="#cb16-954" aria-hidden="true" tabindex="-1"></a>for $20$ values of $\lambda_2$ is about $8$ hours with parallel evaluations on a</span>
<span id="cb16-955"><a href="#cb16-955" aria-hidden="true" tabindex="-1"></a>computation cluster with as many cores as $\lambda_2$ values. The maximal number</span>
<span id="cb16-956"><a href="#cb16-956" aria-hidden="true" tabindex="-1"></a>of iterations is set to $10000$ and the solver precision to $0.01$.</span>
<span id="cb16-957"><a href="#cb16-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-958"><a href="#cb16-958" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;old&gt;</span>We finally illustrate our new method of inferring the multiscale</span>
<span id="cb16-959"><a href="#cb16-959" aria-hidden="true" tabindex="-1"></a>Gaussian graphical model, with an application to the analysis of</span>
<span id="cb16-960"><a href="#cb16-960" aria-hidden="true" tabindex="-1"></a>microbial associations in the American Gut Project. The data used are</span>
<span id="cb16-961"><a href="#cb16-961" aria-hidden="true" tabindex="-1"></a>count data that have been previously normalized by applying the</span>
<span id="cb16-962"><a href="#cb16-962" aria-hidden="true" tabindex="-1"></a>log-centered ratio technique as used in <span class="co">[</span><span class="ot">@Kurtz2015</span><span class="co">]</span>. After some</span>
<span id="cb16-963"><a href="#cb16-963" aria-hidden="true" tabindex="-1"></a>filtering steps <span class="co">[</span><span class="ot">@Kurtz2015</span><span class="co">]</span> on the operational taxonomic units (OTUs)</span>
<span id="cb16-964"><a href="#cb16-964" aria-hidden="true" tabindex="-1"></a>counts (removed if present in less than $37\%$ of the samples) and the</span>
<span id="cb16-965"><a href="#cb16-965" aria-hidden="true" tabindex="-1"></a>samples (removed if sequencing depth below 2700), the top OTUs are</span>
<span id="cb16-966"><a href="#cb16-966" aria-hidden="true" tabindex="-1"></a>grouped in a dataset composed of $n = 289$ for $127$ OTUs.</span>
<span id="cb16-967"><a href="#cb16-967" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;old&gt;</span></span>
<span id="cb16-968"><a href="#cb16-968" aria-hidden="true" tabindex="-1"></a>As a preliminary analysis, we perform a hierarchical agglomerative clustering</span>
<span id="cb16-969"><a href="#cb16-969" aria-hidden="true" tabindex="-1"></a>(HAC) on the OTUs, which allows us to identify four significant groups.</span>
<span id="cb16-970"><a href="#cb16-970" aria-hidden="true" tabindex="-1"></a>The correlation matrix of the dataset is given in fig-emp-cor;</span>
<span id="cb16-971"><a href="#cb16-971" aria-hidden="true" tabindex="-1"></a>variables have been rearranged according to the HAC partition.</span>
<span id="cb16-972"><a href="#cb16-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-973"><a href="#cb16-973" aria-hidden="true" tabindex="-1"></a>Using these settings, we compute a clustering path of the solutions and</span>
<span id="cb16-974"><a href="#cb16-974" aria-hidden="true" tabindex="-1"></a>estimated graphs for $5$ values of $\lambda_2$ corresponding to $5$ different</span>
<span id="cb16-975"><a href="#cb16-975" aria-hidden="true" tabindex="-1"></a>clusters partitions. The @fig-clusterpath shows how the predicted</span>
<span id="cb16-976"><a href="#cb16-976" aria-hidden="true" tabindex="-1"></a>$\hat{\boldsymbol X}$ evolves through $\lambda_2.$ The $\hat{\boldsymbol X}$ are</span>
<span id="cb16-977"><a href="#cb16-977" aria-hidden="true" tabindex="-1"></a>computed from estimated centroids $\hat{\boldsymbol \beta}$ and projected onto</span>
<span id="cb16-978"><a href="#cb16-978" aria-hidden="true" tabindex="-1"></a>two principal components of the original data. The path is not always</span>
<span id="cb16-979"><a href="#cb16-979" aria-hidden="true" tabindex="-1"></a>agglomerative, but the clusters' splits observed ensure optimal solutions.</span>
<span id="cb16-980"><a href="#cb16-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-983"><a href="#cb16-983" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb16-984"><a href="#cb16-984" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb16-985"><a href="#cb16-985" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb16-986"><a href="#cb16-986" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-clusterpath</span></span>
<span id="cb16-987"><a href="#cb16-987" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Clustering path of the MGLasso convex clustering solutions on microbiome data with $127$ OTUs. The predicted data are projected onto the two principal components of the original data, while the fusion penalty varies. As $\lambda_2$ increases, it reaches a value for which all the estimated centroids are equal; thus, the branches of the path converge to a unique point in the center of the graph. OTUs are colored according to their phylum classification. The path displays abrupt merges. The pure cluster on the graph's left side (down) corresponds to the phylum Bacteroidetes.</span></span>
<span id="cb16-988"><a href="#cb16-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-989"><a href="#cb16-989" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(SpiecEasi)</span>
<span id="cb16-990"><a href="#cb16-990" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(colorspace)</span>
<span id="cb16-991"><a href="#cb16-991" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggrepel)</span>
<span id="cb16-992"><a href="#cb16-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-993"><a href="#cb16-993" aria-hidden="true" tabindex="-1"></a>path_data <span class="ot">&lt;-</span> <span class="st">"./data/"</span></span>
<span id="cb16-994"><a href="#cb16-994" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">paste0</span>(path_data, <span class="st">"mgl_amgut_rev_l2_seq0to1_20val.RData"</span>))</span>
<span id="cb16-995"><a href="#cb16-995" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">paste0</span>(path_data, <span class="st">"mgl_amgut_rev_l2_seq1to20_20val.RData"</span>))</span>
<span id="cb16-996"><a href="#cb16-996" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">paste0</span>(path_data, <span class="st">"mgl_amgut_rev_l2_seq0to4_20val.RData"</span>))</span>
<span id="cb16-997"><a href="#cb16-997" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">paste0</span>(path_data, <span class="st">"amgut1.filt.phy.rda"</span>)) <span class="co"># Data for the phylum taxonomic classifier loaded from supplementary files of the SpiecEasi package. See https://github.com/zdk123/SpiecEasi/blob/master/data/amgut2.filt.phy.rda</span></span>
<span id="cb16-998"><a href="#cb16-998" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">paste0</span>(path_data, <span class="st">"amgut1.filt.rda"</span>))</span>
<span id="cb16-999"><a href="#cb16-999" aria-hidden="true" tabindex="-1"></a>amgut1.filt <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">clr</span>(amgut1.filt <span class="sc">+</span> <span class="dv">1</span> , <span class="dv">1</span>))</span>
<span id="cb16-1000"><a href="#cb16-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1001"><a href="#cb16-1001" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb16-1002"><a href="#cb16-1002" aria-hidden="true" tabindex="-1"></a>rank2_table <span class="ot">&lt;-</span> <span class="fu">table</span>(taxas[,<span class="st">"Rank2"</span>])</span>
<span id="cb16-1003"><a href="#cb16-1003" aria-hidden="true" tabindex="-1"></a>col_leaves <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">rep</span>(<span class="fu">rainbow_hcl</span>(<span class="dv">6</span>, <span class="at">c=</span><span class="dv">90</span>, <span class="at">l=</span><span class="dv">50</span>), <span class="at">times =</span> rank2_table))</span>
<span id="cb16-1004"><a href="#cb16-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1005"><a href="#cb16-1005" aria-hidden="true" tabindex="-1"></a>plot_clusterpath <span class="ot">&lt;-</span> <span class="cf">function</span>(X, mglasso_res, <span class="at">colnames_ =</span> <span class="cn">NULL</span>, max.overlaps, <span class="at">cut_k_vars =</span> <span class="dv">5</span>, colors_) {</span>
<span id="cb16-1006"><a href="#cb16-1006" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Initialisations</span></span>
<span id="cb16-1007"><a href="#cb16-1007" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb16-1008"><a href="#cb16-1008" aria-hidden="true" tabindex="-1"></a>  df.paths <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">c</span>(),<span class="at">y=</span><span class="fu">c</span>(), <span class="at">group=</span><span class="fu">c</span>())</span>
<span id="cb16-1009"><a href="#cb16-1009" aria-hidden="true" tabindex="-1"></a>  nlevel <span class="ot">&lt;-</span> <span class="fu">length</span>(mglasso_res)</span>
<span id="cb16-1010"><a href="#cb16-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1011"><a href="#cb16-1011" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Principal component analysis</span></span>
<span id="cb16-1012"><a href="#cb16-1012" aria-hidden="true" tabindex="-1"></a>  svdX <span class="ot">&lt;-</span> <span class="fu">svd</span>(X)                <span class="do">## singular value decomposition</span></span>
<span id="cb16-1013"><a href="#cb16-1013" aria-hidden="true" tabindex="-1"></a>  pc <span class="ot">&lt;-</span> svdX<span class="sc">$</span>u[,<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>,drop<span class="ot">=</span><span class="cn">FALSE</span>] <span class="do">## singular vectors</span></span>
<span id="cb16-1014"><a href="#cb16-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1015"><a href="#cb16-1015" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> cut_k_vars<span class="sc">:</span>nlevel) {</span>
<span id="cb16-1016"><a href="#cb16-1016" aria-hidden="true" tabindex="-1"></a>    Beta <span class="ot">&lt;-</span> mglasso_res[[j]]<span class="sc">$</span>selected_Theta</span>
<span id="cb16-1017"><a href="#cb16-1017" aria-hidden="true" tabindex="-1"></a>    Xpred <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>p, <span class="cf">function</span>(i){X <span class="sc">%*%</span> Beta[i,]})</span>
<span id="cb16-1018"><a href="#cb16-1018" aria-hidden="true" tabindex="-1"></a>    pcs <span class="ot">&lt;-</span> <span class="fu">t</span>(pc)<span class="sc">%*%</span>Xpred</span>
<span id="cb16-1019"><a href="#cb16-1019" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> pcs[<span class="dv">1</span>,]</span>
<span id="cb16-1020"><a href="#cb16-1020" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> pcs[<span class="dv">2</span>,]</span>
<span id="cb16-1021"><a href="#cb16-1021" aria-hidden="true" tabindex="-1"></a>    df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>pcs[<span class="dv">1</span>,], <span class="at">y=</span>pcs[<span class="dv">2</span>,], <span class="at">group=</span><span class="dv">1</span><span class="sc">:</span>p, <span class="at">Rank2 =</span> colors_)</span>
<span id="cb16-1022"><a href="#cb16-1022" aria-hidden="true" tabindex="-1"></a>    df.paths <span class="ot">&lt;-</span> <span class="fu">rbind</span>(df.paths,df)</span>
<span id="cb16-1023"><a href="#cb16-1023" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-1024"><a href="#cb16-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1025"><a href="#cb16-1025" aria-hidden="true" tabindex="-1"></a>  <span class="co"># X_data &lt;- as.data.frame(t(X) %*% pc) ## PCA projections (scores)</span></span>
<span id="cb16-1026"><a href="#cb16-1026" aria-hidden="true" tabindex="-1"></a>  X_data <span class="ot">&lt;-</span> df.paths[<span class="dv">1</span><span class="sc">:</span>p,]</span>
<span id="cb16-1027"><a href="#cb16-1027" aria-hidden="true" tabindex="-1"></a>  <span class="co">#colnames(X_data) &lt;- c("x", "y")</span></span>
<span id="cb16-1028"><a href="#cb16-1028" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ifelse</span>(<span class="fu">is.null</span>(colnames_),</span>
<span id="cb16-1029"><a href="#cb16-1029" aria-hidden="true" tabindex="-1"></a>         X_data<span class="sc">$</span>Name <span class="ot">&lt;-</span> <span class="fu">colnames</span>(X),</span>
<span id="cb16-1030"><a href="#cb16-1030" aria-hidden="true" tabindex="-1"></a>         X_data<span class="sc">$</span>Name <span class="ot">&lt;-</span> colnames_)</span>
<span id="cb16-1031"><a href="#cb16-1031" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> df.paths, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb16-1032"><a href="#cb16-1032" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb16-1033"><a href="#cb16-1033" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_path</span>(<span class="fu">aes</span>(<span class="at">group =</span> group,  <span class="at">colour =</span> Rank2), <span class="at">alpha =</span> <span class="fl">0.5</span>)</span>
<span id="cb16-1034"><a href="#cb16-1034" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb16-1035"><a href="#cb16-1035" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_text_repel</span>(<span class="at">data =</span> X_data,</span>
<span id="cb16-1036"><a href="#cb16-1036" aria-hidden="true" tabindex="-1"></a>                                <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">label =</span> Name),</span>
<span id="cb16-1037"><a href="#cb16-1037" aria-hidden="true" tabindex="-1"></a>                                <span class="at">max.overlaps =</span> max.overlaps)</span>
<span id="cb16-1038"><a href="#cb16-1038" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb16-1039"><a href="#cb16-1039" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">data =</span> X_data, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">colour =</span> Rank2), <span class="at">size =</span> <span class="fl">1.5</span>)</span>
<span id="cb16-1040"><a href="#cb16-1040" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb16-1041"><a href="#cb16-1041" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">'Principal Component 3'</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">'Principal Component 4'</span>)</span>
<span id="cb16-1042"><a href="#cb16-1042" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="sc">+</span> <span class="fu">theme_bw</span>()</span>
<span id="cb16-1043"><a href="#cb16-1043" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-1044"><a href="#cb16-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1045"><a href="#cb16-1045" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_clusterpath</span>(amgut1.filt, <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3), <span class="at">max.overlaps =</span> <span class="dv">10</span>, <span class="at">cut_k_vars =</span> <span class="dv">1</span>, <span class="at">colors_ =</span> taxas[,<span class="st">"Rank2"</span>])</span>
<span id="cb16-1046"><a href="#cb16-1046" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1047"><a href="#cb16-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1048"><a href="#cb16-1048" aria-hidden="true" tabindex="-1"></a>The @fig-meta-graphs displays graphs and clusters for different levels of</span>
<span id="cb16-1049"><a href="#cb16-1049" aria-hidden="true" tabindex="-1"></a>granularity: $127$, $63$, $31$, $15$ and $2$ clusters. For computing the</span>
<span id="cb16-1050"><a href="#cb16-1050" aria-hidden="true" tabindex="-1"></a>clusters' assignment of nodes, the fusion threshold has been set to</span>
<span id="cb16-1051"><a href="#cb16-1051" aria-hidden="true" tabindex="-1"></a>$\epsilon_{fuse} = 0.001$. Variables that belong to the same cluster share the</span>
<span id="cb16-1052"><a href="#cb16-1052" aria-hidden="true" tabindex="-1"></a>same neighborhood; thus, the neighboring information is summarized into a single</span>
<span id="cb16-1053"><a href="#cb16-1053" aria-hidden="true" tabindex="-1"></a>variable representative of the group. The subfigures show graphs at multiple</span>
<span id="cb16-1054"><a href="#cb16-1054" aria-hidden="true" tabindex="-1"></a>levels of granularity which are built on the meta-variables or representative</span>
<span id="cb16-1055"><a href="#cb16-1055" aria-hidden="true" tabindex="-1"></a>variables.</span>
<span id="cb16-1056"><a href="#cb16-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1059"><a href="#cb16-1059" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb16-1060"><a href="#cb16-1060" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb16-1061"><a href="#cb16-1061" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb16-1062"><a href="#cb16-1062" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-meta-graphs</span></span>
<span id="cb16-1063"><a href="#cb16-1063" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Estimated graphs at multiple levels of granularity. The first graph shows a network inferred when $\lambda_2=0$.The number of clusters is equal to the number of OTUs. Increasing the fusion penalty makes it possible to uncover graphs built on the representative variable of each cluster. OTUs are colored according to their phylum taxonomic classifier. The number of clusters is computed from the regression vectors with a fixed fusion threshold.</span></span>
<span id="cb16-1064"><a href="#cb16-1064" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap: </span></span>
<span id="cb16-1065"><a href="#cb16-1065" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "127 clusters graph" </span></span>
<span id="cb16-1066"><a href="#cb16-1066" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Meta-variables graph with 63 clusters"</span></span>
<span id="cb16-1067"><a href="#cb16-1067" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Meta-variables graph with 31 clusters"</span></span>
<span id="cb16-1068"><a href="#cb16-1068" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Meta-variables graph with 15 clusters"</span></span>
<span id="cb16-1069"><a href="#cb16-1069" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Meta-variables graph with 2 clusters"</span></span>
<span id="cb16-1070"><a href="#cb16-1070" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 2</span></span>
<span id="cb16-1071"><a href="#cb16-1071" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(igraph)</span>
<span id="cb16-1072"><a href="#cb16-1072" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(phyloseq)</span>
<span id="cb16-1073"><a href="#cb16-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1074"><a href="#cb16-1074" aria-hidden="true" tabindex="-1"></a>all_clusters_partition <span class="ot">&lt;-</span></span>
<span id="cb16-1075"><a href="#cb16-1075" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(<span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3), <span class="cf">function</span>(x)</span>
<span id="cb16-1076"><a href="#cb16-1076" aria-hidden="true" tabindex="-1"></a>    <span class="fu">get_clusters_mgl</span>(x<span class="sc">$</span>selected_Theta))</span>
<span id="cb16-1077"><a href="#cb16-1077" aria-hidden="true" tabindex="-1"></a>all_num_clusters <span class="ot">&lt;-</span></span>
<span id="cb16-1078"><a href="#cb16-1078" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unlist</span>(<span class="fu">lapply</span>(all_clusters_partition, <span class="cf">function</span>(x)</span>
<span id="cb16-1079"><a href="#cb16-1079" aria-hidden="true" tabindex="-1"></a>    <span class="fu">length</span>(<span class="fu">unique</span>(x))))</span>
<span id="cb16-1080"><a href="#cb16-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1081"><a href="#cb16-1081" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">127</span>)[<span class="dv">1</span>]</span>
<span id="cb16-1082"><a href="#cb16-1082" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb16-1083"><a href="#cb16-1083" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">extract_meta</span>(<span class="at">clusters =</span> all_clusters_partition[[ind]])</span>
<span id="cb16-1084"><a href="#cb16-1084" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb16-1085"><a href="#cb16-1085" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]<span class="sc">$</span>selected_Theta[vec, vec]</span>
<span id="cb16-1086"><a href="#cb16-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1087"><a href="#cb16-1087" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span> <span class="fu">symmetrize</span>(metaG)</span>
<span id="cb16-1088"><a href="#cb16-1088" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb16-1089"><a href="#cb16-1089" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj2igraph</span>(metaG, <span class="at">vertex.attr =</span> <span class="fu">list</span>(<span class="at">name =</span> <span class="fu">taxa_names</span>(amgut1.filt.phy)[vec]))</span>
<span id="cb16-1090"><a href="#cb16-1090" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(metaG)<span class="sc">$</span>weight <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">E</span>(metaG)<span class="sc">$</span>weight)</span>
<span id="cb16-1091"><a href="#cb16-1091" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb16-1092"><a href="#cb16-1092" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> <span class="fu">cbind</span>(clusters, taxas)</span>
<span id="cb16-1093"><a href="#cb16-1093" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> taxas[vec, ]</span>
<span id="cb16-1094"><a href="#cb16-1094" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_network</span>(</span>
<span id="cb16-1095"><a href="#cb16-1095" aria-hidden="true" tabindex="-1"></a>  metaG,</span>
<span id="cb16-1096"><a href="#cb16-1096" aria-hidden="true" tabindex="-1"></a>  taxas,</span>
<span id="cb16-1097"><a href="#cb16-1097" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"taxa"</span>,</span>
<span id="cb16-1098"><a href="#cb16-1098" aria-hidden="true" tabindex="-1"></a>  <span class="at">layout.method =</span> layout_with_fr,</span>
<span id="cb16-1099"><a href="#cb16-1099" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="st">"Rank2"</span></span>
<span id="cb16-1100"><a href="#cb16-1100" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-1101"><a href="#cb16-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1102"><a href="#cb16-1102" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">63</span>)[<span class="dv">1</span>]</span>
<span id="cb16-1103"><a href="#cb16-1103" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb16-1104"><a href="#cb16-1104" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">extract_meta</span>(<span class="at">clusters =</span> all_clusters_partition[[ind]])</span>
<span id="cb16-1105"><a href="#cb16-1105" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb16-1106"><a href="#cb16-1106" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]<span class="sc">$</span>selected_Theta[vec, vec]</span>
<span id="cb16-1107"><a href="#cb16-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1108"><a href="#cb16-1108" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span> <span class="fu">symmetrize</span>(metaG)</span>
<span id="cb16-1109"><a href="#cb16-1109" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb16-1110"><a href="#cb16-1110" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj2igraph</span>(metaG, <span class="at">vertex.attr =</span> <span class="fu">list</span>(<span class="at">name =</span> <span class="fu">taxa_names</span>(amgut1.filt.phy)[vec]))</span>
<span id="cb16-1111"><a href="#cb16-1111" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(metaG)<span class="sc">$</span>weight <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">E</span>(metaG)<span class="sc">$</span>weight)</span>
<span id="cb16-1112"><a href="#cb16-1112" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb16-1113"><a href="#cb16-1113" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> <span class="fu">cbind</span>(clusters, taxas)</span>
<span id="cb16-1114"><a href="#cb16-1114" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> taxas[vec, ]</span>
<span id="cb16-1115"><a href="#cb16-1115" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_network</span>(</span>
<span id="cb16-1116"><a href="#cb16-1116" aria-hidden="true" tabindex="-1"></a>  metaG,</span>
<span id="cb16-1117"><a href="#cb16-1117" aria-hidden="true" tabindex="-1"></a>  taxas,</span>
<span id="cb16-1118"><a href="#cb16-1118" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"taxa"</span>,</span>
<span id="cb16-1119"><a href="#cb16-1119" aria-hidden="true" tabindex="-1"></a>  <span class="at">layout.method =</span> layout_with_fr,</span>
<span id="cb16-1120"><a href="#cb16-1120" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="st">"Rank2"</span></span>
<span id="cb16-1121"><a href="#cb16-1121" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-1122"><a href="#cb16-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1123"><a href="#cb16-1123" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">31</span>)[<span class="dv">1</span>]</span>
<span id="cb16-1124"><a href="#cb16-1124" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb16-1125"><a href="#cb16-1125" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">extract_meta</span>(<span class="at">clusters =</span> all_clusters_partition[[ind]])</span>
<span id="cb16-1126"><a href="#cb16-1126" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb16-1127"><a href="#cb16-1127" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]<span class="sc">$</span>selected_Theta[vec, vec]</span>
<span id="cb16-1128"><a href="#cb16-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1129"><a href="#cb16-1129" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span> <span class="fu">symmetrize</span>(metaG)</span>
<span id="cb16-1130"><a href="#cb16-1130" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb16-1131"><a href="#cb16-1131" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj2igraph</span>(metaG, <span class="at">vertex.attr =</span> <span class="fu">list</span>(<span class="at">name =</span> <span class="fu">taxa_names</span>(amgut1.filt.phy)[vec]))</span>
<span id="cb16-1132"><a href="#cb16-1132" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(metaG)<span class="sc">$</span>weight <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">E</span>(metaG)<span class="sc">$</span>weight)</span>
<span id="cb16-1133"><a href="#cb16-1133" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb16-1134"><a href="#cb16-1134" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> <span class="fu">cbind</span>(clusters, taxas)</span>
<span id="cb16-1135"><a href="#cb16-1135" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> taxas[vec, ]</span>
<span id="cb16-1136"><a href="#cb16-1136" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_network</span>(</span>
<span id="cb16-1137"><a href="#cb16-1137" aria-hidden="true" tabindex="-1"></a>  metaG,</span>
<span id="cb16-1138"><a href="#cb16-1138" aria-hidden="true" tabindex="-1"></a>  taxas,</span>
<span id="cb16-1139"><a href="#cb16-1139" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"taxa"</span>,</span>
<span id="cb16-1140"><a href="#cb16-1140" aria-hidden="true" tabindex="-1"></a>  <span class="at">layout.method =</span> layout_with_dh,</span>
<span id="cb16-1141"><a href="#cb16-1141" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="st">"Rank2"</span></span>
<span id="cb16-1142"><a href="#cb16-1142" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-1143"><a href="#cb16-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1144"><a href="#cb16-1144" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">15</span>)[<span class="dv">1</span>]</span>
<span id="cb16-1145"><a href="#cb16-1145" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb16-1146"><a href="#cb16-1146" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">extract_meta</span>(<span class="at">clusters =</span> all_clusters_partition[[ind]])</span>
<span id="cb16-1147"><a href="#cb16-1147" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb16-1148"><a href="#cb16-1148" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]<span class="sc">$</span>selected_Theta[vec, vec]</span>
<span id="cb16-1149"><a href="#cb16-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1150"><a href="#cb16-1150" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span> <span class="fu">symmetrize</span>(metaG)</span>
<span id="cb16-1151"><a href="#cb16-1151" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb16-1152"><a href="#cb16-1152" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj2igraph</span>(metaG, <span class="at">vertex.attr =</span> <span class="fu">list</span>(<span class="at">name =</span> <span class="fu">taxa_names</span>(amgut1.filt.phy)[vec]))</span>
<span id="cb16-1153"><a href="#cb16-1153" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(metaG)<span class="sc">$</span>weight <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">E</span>(metaG)<span class="sc">$</span>weight)</span>
<span id="cb16-1154"><a href="#cb16-1154" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb16-1155"><a href="#cb16-1155" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> <span class="fu">cbind</span>(clusters, taxas)</span>
<span id="cb16-1156"><a href="#cb16-1156" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> taxas[vec, ]</span>
<span id="cb16-1157"><a href="#cb16-1157" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_network</span>(</span>
<span id="cb16-1158"><a href="#cb16-1158" aria-hidden="true" tabindex="-1"></a>  metaG,</span>
<span id="cb16-1159"><a href="#cb16-1159" aria-hidden="true" tabindex="-1"></a>  taxas,</span>
<span id="cb16-1160"><a href="#cb16-1160" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"taxa"</span>,</span>
<span id="cb16-1161"><a href="#cb16-1161" aria-hidden="true" tabindex="-1"></a>  <span class="at">layout.method =</span> layout_with_dh,</span>
<span id="cb16-1162"><a href="#cb16-1162" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="st">"Rank2"</span></span>
<span id="cb16-1163"><a href="#cb16-1163" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-1164"><a href="#cb16-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1165"><a href="#cb16-1165" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">2</span>)[<span class="dv">1</span>]</span>
<span id="cb16-1166"><a href="#cb16-1166" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb16-1167"><a href="#cb16-1167" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">extract_meta</span>(<span class="at">clusters =</span> all_clusters_partition[[ind]])</span>
<span id="cb16-1168"><a href="#cb16-1168" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb16-1169"><a href="#cb16-1169" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]<span class="sc">$</span>selected_Theta[vec, vec]</span>
<span id="cb16-1170"><a href="#cb16-1170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1171"><a href="#cb16-1171" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span> <span class="fu">symmetrize</span>(metaG)</span>
<span id="cb16-1172"><a href="#cb16-1172" aria-hidden="true" tabindex="-1"></a>metaG <span class="ot">&lt;-</span></span>
<span id="cb16-1173"><a href="#cb16-1173" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj2igraph</span>(metaG, <span class="at">vertex.attr =</span> <span class="fu">list</span>(<span class="at">name =</span> <span class="fu">taxa_names</span>(amgut1.filt.phy)[vec]))</span>
<span id="cb16-1174"><a href="#cb16-1174" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(metaG)<span class="sc">$</span>weight <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">E</span>(metaG)<span class="sc">$</span>weight)</span>
<span id="cb16-1175"><a href="#cb16-1175" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb16-1176"><a href="#cb16-1176" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> <span class="fu">cbind</span>(clusters, taxas)</span>
<span id="cb16-1177"><a href="#cb16-1177" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> taxas[vec, ]</span>
<span id="cb16-1178"><a href="#cb16-1178" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_network</span>(</span>
<span id="cb16-1179"><a href="#cb16-1179" aria-hidden="true" tabindex="-1"></a>  metaG,</span>
<span id="cb16-1180"><a href="#cb16-1180" aria-hidden="true" tabindex="-1"></a>  taxas,</span>
<span id="cb16-1181"><a href="#cb16-1181" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"taxa"</span>,</span>
<span id="cb16-1182"><a href="#cb16-1182" aria-hidden="true" tabindex="-1"></a>  <span class="at">layout.method =</span> layout_with_dh,</span>
<span id="cb16-1183"><a href="#cb16-1183" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="st">"Rank2"</span></span>
<span id="cb16-1184"><a href="#cb16-1184" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-1185"><a href="#cb16-1185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1186"><a href="#cb16-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1187"><a href="#cb16-1187" aria-hidden="true" tabindex="-1"></a>To assess the relevance of the inferred clusters, they are compared to known</span>
<span id="cb16-1188"><a href="#cb16-1188" aria-hidden="true" tabindex="-1"></a>taxonomic ranks (phylum, class, order, family, genera, or species). The phylum</span>
<span id="cb16-1189"><a href="#cb16-1189" aria-hidden="true" tabindex="-1"></a>classification is used. For example, for a clustering partition in $2$ groups,</span>
<span id="cb16-1190"><a href="#cb16-1190" aria-hidden="true" tabindex="-1"></a>the MGLasso clustering partition is composed of $120$ variables versus $7$</span>
<span id="cb16-1191"><a href="#cb16-1191" aria-hidden="true" tabindex="-1"></a>variables. The cluster $2$ is exclusively composed of OTUs</span>
<span id="cb16-1192"><a href="#cb16-1192" aria-hidden="true" tabindex="-1"></a>belonging to the Proteobacteria phylum. The cluster $1$ also contains</span>
<span id="cb16-1193"><a href="#cb16-1193" aria-hidden="true" tabindex="-1"></a>Proteobacteria OTUs, so those identified in cluster $2$ might share more</span>
<span id="cb16-1194"><a href="#cb16-1194" aria-hidden="true" tabindex="-1"></a>intimate characteristics.</span>
<span id="cb16-1195"><a href="#cb16-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1198"><a href="#cb16-1198" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb16-1199"><a href="#cb16-1199" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(all_num_clusters <span class="sc">==</span> <span class="dv">2</span>)[<span class="dv">1</span>]</span>
<span id="cb16-1200"><a href="#cb16-1200" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">as.character</span>(all_clusters_partition[[ind]])</span>
<span id="cb16-1201"><a href="#cb16-1201" aria-hidden="true" tabindex="-1"></a>taxas <span class="ot">&lt;-</span> amgut1.filt.phy<span class="sc">@</span>tax_table<span class="sc">@</span>.Data</span>
<span id="cb16-1202"><a href="#cb16-1202" aria-hidden="true" tabindex="-1"></a>taxonomic.classification <span class="ot">&lt;-</span> taxas[,<span class="st">"Rank2"</span>]</span>
<span id="cb16-1203"><a href="#cb16-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1204"><a href="#cb16-1204" aria-hidden="true" tabindex="-1"></a><span class="do">## remove "p__" characters in species names</span></span>
<span id="cb16-1205"><a href="#cb16-1205" aria-hidden="true" tabindex="-1"></a>taxonomic.classification <span class="ot">&lt;-</span> <span class="fu">sub</span>(<span class="st">"p__"</span>, <span class="st">""</span>, taxonomic.classification)</span>
<span id="cb16-1206"><a href="#cb16-1206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1207"><a href="#cb16-1207" aria-hidden="true" tabindex="-1"></a>tables<span class="sc">::</span><span class="fu">as.tabular</span>(<span class="fu">table</span>(clusters, taxonomic.classification))</span>
<span id="cb16-1208"><a href="#cb16-1208" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1209"><a href="#cb16-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1210"><a href="#cb16-1210" aria-hidden="true" tabindex="-1"></a>Adjusted Rand indices are not calculated for comparisons as the unitary weights</span>
<span id="cb16-1211"><a href="#cb16-1211" aria-hidden="true" tabindex="-1"></a>in the convex clustering problem can be suboptimal. The abundance of OTUs</span>
<span id="cb16-1212"><a href="#cb16-1212" aria-hidden="true" tabindex="-1"></a>belonging to cluster $1$, mainly composed of Bacteroidetes and Firmicutes phyla,</span>
<span id="cb16-1213"><a href="#cb16-1213" aria-hidden="true" tabindex="-1"></a>is seemingly dependent on the abundance of OTUS in cluster $2$, i.e.,</span>
<span id="cb16-1214"><a href="#cb16-1214" aria-hidden="true" tabindex="-1"></a>Proteobacteria phylum.</span>
<span id="cb16-1215"><a href="#cb16-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1216"><a href="#cb16-1216" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application to methylation and transcriptomic genotypes in poplar  </span></span>
<span id="cb16-1217"><a href="#cb16-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1218"><a href="#cb16-1218" aria-hidden="true" tabindex="-1"></a>Next, we investigate interactions between European poplar genotypes for</span>
<span id="cb16-1219"><a href="#cb16-1219" aria-hidden="true" tabindex="-1"></a>transcriptomic and DNA methylation data extracted from the Evolutionary and</span>
<span id="cb16-1220"><a href="#cb16-1220" aria-hidden="true" tabindex="-1"></a>functional impact of EPIgenetic variation in forest TREEs project [EPITREE,</span>
<span id="cb16-1221"><a href="#cb16-1221" aria-hidden="true" tabindex="-1"></a>@maury2019epigenetics]. The analysis was purposefully applied to the samples and</span>
<span id="cb16-1222"><a href="#cb16-1222" aria-hidden="true" tabindex="-1"></a>not the genes in order to highlight the MGLasso clustering performance and show</span>
<span id="cb16-1223"><a href="#cb16-1223" aria-hidden="true" tabindex="-1"></a>some potential relationships between DNA methylation and gene expression levels</span>
<span id="cb16-1224"><a href="#cb16-1224" aria-hidden="true" tabindex="-1"></a>for some genotypes.</span>
<span id="cb16-1225"><a href="#cb16-1225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1226"><a href="#cb16-1226" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Classic correlation approaches can lead to spurious relationships between</span></span>
<span id="cb16-1227"><a href="#cb16-1227" aria-hidden="true" tabindex="-1"></a><span class="co">variables. Through the gaussian graphical framework of MGLasso, one can focus on</span></span>
<span id="cb16-1228"><a href="#cb16-1228" aria-hidden="true" tabindex="-1"></a><span class="co">the conditional dependency structure which gets rid of confusion effects. We</span></span>
<span id="cb16-1229"><a href="#cb16-1229" aria-hidden="true" tabindex="-1"></a><span class="co">refer to @akalin2020computational for a broader definition of the central dogma</span></span>
<span id="cb16-1230"><a href="#cb16-1230" aria-hidden="true" tabindex="-1"></a><span class="co">of molecular biology (DNA-RNA-proteins). --&gt;</span></span>
<span id="cb16-1231"><a href="#cb16-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1232"><a href="#cb16-1232" aria-hidden="true" tabindex="-1"></a>Poplar (_Populus_) is often used as a model tree for the study of drought</span>
<span id="cb16-1233"><a href="#cb16-1233" aria-hidden="true" tabindex="-1"></a>response. Natural populations of black poplars (_Populus nigra_) have been</span>
<span id="cb16-1234"><a href="#cb16-1234" aria-hidden="true" tabindex="-1"></a>planted in common gardens in France, Italy, and Germany (see</span>
<span id="cb16-1235"><a href="#cb16-1235" aria-hidden="true" tabindex="-1"></a>@fig-context-epitree) with control on some environmental variables such as water</span>
<span id="cb16-1236"><a href="#cb16-1236" aria-hidden="true" tabindex="-1"></a>availability <span class="co">[</span><span class="ot">@sow2018narrow</span><span class="co">]</span>. The poplar has economic importance and is one of</span>
<span id="cb16-1237"><a href="#cb16-1237" aria-hidden="true" tabindex="-1"></a>the most endangered species as a result of global climate change. The drought</span>
<span id="cb16-1238"><a href="#cb16-1238" aria-hidden="true" tabindex="-1"></a>response can be studied via DNA methylation, which is a necessary process in</span>
<span id="cb16-1239"><a href="#cb16-1239" aria-hidden="true" tabindex="-1"></a>plant development and response to environmental variations</span>
<span id="cb16-1240"><a href="#cb16-1240" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@amaral2020advances</span><span class="co">]</span>. It consists of the addition of a Methyl group to a</span>
<span id="cb16-1241"><a href="#cb16-1241" aria-hidden="true" tabindex="-1"></a>cytosine (C) in the genome and occurs in three contexts (CG, CHG, and CHH, where</span>
<span id="cb16-1242"><a href="#cb16-1242" aria-hidden="true" tabindex="-1"></a>H $\in <span class="sc">\{</span> A, C, T<span class="sc">\}</span>$). Methylation can be measured on two regions of the gene.</span>
<span id="cb16-1243"><a href="#cb16-1243" aria-hidden="true" tabindex="-1"></a>Methylation in promoters is linked to gene silencing, and methylation in the</span>
<span id="cb16-1244"><a href="#cb16-1244" aria-hidden="true" tabindex="-1"></a>body of the gene can be related to tissue-specific expression or alternative</span>
<span id="cb16-1245"><a href="#cb16-1245" aria-hidden="true" tabindex="-1"></a>splicing <span class="co">[</span><span class="ot">@sow2019role</span><span class="co">]</span>.</span>
<span id="cb16-1246"><a href="#cb16-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1247"><a href="#cb16-1247" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Epigenetic is the study of heritable changes which are not the result of a</span></span>
<span id="cb16-1248"><a href="#cb16-1248" aria-hidden="true" tabindex="-1"></a><span class="co">modification in the DNA sequence [@plomion2016forest]. Epigenetic marks in</span></span>
<span id="cb16-1249"><a href="#cb16-1249" aria-hidden="true" tabindex="-1"></a><span class="co">forest trees can be studied via  --&gt;</span></span>
<span id="cb16-1250"><a href="#cb16-1250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1251"><a href="#cb16-1251" aria-hidden="true" tabindex="-1"></a>::: {#fig-context-epitree layout-ncol=2}</span>
<span id="cb16-1252"><a href="#cb16-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1253"><a href="#cb16-1253" aria-hidden="true" tabindex="-1"></a><span class="al">![Black poplar (C. Fischer Wikimedia)](./figures/peuplier-noir-Christian-Fischer.jpeg)</span></span>
<span id="cb16-1254"><a href="#cb16-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1255"><a href="#cb16-1255" aria-hidden="true" tabindex="-1"></a><span class="al">![Map of genotypes](./figures/carte-genotypes.png)</span></span>
<span id="cb16-1256"><a href="#cb16-1256" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1257"><a href="#cb16-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1258"><a href="#cb16-1258" aria-hidden="true" tabindex="-1"></a>The collected DNA methylation and expression data are counts data. Details on</span>
<span id="cb16-1259"><a href="#cb16-1259" aria-hidden="true" tabindex="-1"></a>the plant material and experimental design can be found in @sow2019role and</span>
<span id="cb16-1260"><a href="#cb16-1260" aria-hidden="true" tabindex="-1"></a>@chateigner2020gene. The transcriptomic data were measured via RNA-Seq and</span>
<span id="cb16-1261"><a href="#cb16-1261" aria-hidden="true" tabindex="-1"></a>normalized using Trimmed Mean of M-Values combined with a Best linear unbiased</span>
<span id="cb16-1262"><a href="#cb16-1262" aria-hidden="true" tabindex="-1"></a>predictor (BLUP) correction as described in @chateigner2020gene. The methylation</span>
<span id="cb16-1263"><a href="#cb16-1263" aria-hidden="true" tabindex="-1"></a>data were measured through whole-genome bisulfite sequencing (WGBS) and are</span>
<span id="cb16-1264"><a href="#cb16-1264" aria-hidden="true" tabindex="-1"></a>normalized via the read per density approach then passed to a logarithm function</span>
<span id="cb16-1265"><a href="#cb16-1265" aria-hidden="true" tabindex="-1"></a>$log_2(x+1)$ with $x \in \mathbb R$. For each one of the $10$</span>
<span id="cb16-1266"><a href="#cb16-1266" aria-hidden="true" tabindex="-1"></a>populations (see @fig-context-epitree), DNA methylation in CG, CHG, and CHH</span>
<span id="cb16-1267"><a href="#cb16-1267" aria-hidden="true" tabindex="-1"></a>contexts for promoters and gene-body and RNA sequencing data are observed on</span>
<span id="cb16-1268"><a href="#cb16-1268" aria-hidden="true" tabindex="-1"></a>genotypes. A mean measure is computed from two replicates per population. The</span>
<span id="cb16-1269"><a href="#cb16-1269" aria-hidden="true" tabindex="-1"></a>analysis has been restricted to a set of $151$ target genes which explains the</span>
<span id="cb16-1270"><a href="#cb16-1270" aria-hidden="true" tabindex="-1"></a>most variability in the omics data and the subsequent number of samples from</span>
<span id="cb16-1271"><a href="#cb16-1271" aria-hidden="true" tabindex="-1"></a>different omic variables, which is $70.$</span>
<span id="cb16-1272"><a href="#cb16-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1273"><a href="#cb16-1273" aria-hidden="true" tabindex="-1"></a>The MGLasso model is fitted with fusion penalty values chosen in $<span class="co">[</span><span class="ot">0, 30.94</span><span class="co">]</span>$</span>
<span id="cb16-1274"><a href="#cb16-1274" aria-hidden="true" tabindex="-1"></a>and a Lasso penalty $\lambda_1$ parameter chosen via the StARS approach with</span>
<span id="cb16-1275"><a href="#cb16-1275" aria-hidden="true" tabindex="-1"></a>threshold $0.05$. In the resulting clustering path (see</span>
<span id="cb16-1276"><a href="#cb16-1276" aria-hidden="true" tabindex="-1"></a>@fig-clusterpath-poplar), we can identify three distinct and coherent clusters,</span>
<span id="cb16-1277"><a href="#cb16-1277" aria-hidden="true" tabindex="-1"></a>which are samples corresponding to gene expression genotypes, gene-body</span>
<span id="cb16-1278"><a href="#cb16-1278" aria-hidden="true" tabindex="-1"></a>methylation samples, and gene promoter samples.</span>
<span id="cb16-1279"><a href="#cb16-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1282"><a href="#cb16-1282" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb16-1283"><a href="#cb16-1283" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-clusterpath-poplar</span></span>
<span id="cb16-1284"><a href="#cb16-1284" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Clustering path of solutions on DNA methylation and transcriptomic samples. The figure shows $3$ distinct clusters which correspond to omics data of different natures: transcriptomic (right), methylation on the promoter (bottom), and methylation on gene-body (top left)."</span></span>
<span id="cb16-1285"><a href="#cb16-1285" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb16-1286"><a href="#cb16-1286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1287"><a href="#cb16-1287" aria-hidden="true" tabindex="-1"></a>mglasso_genot <span class="ot">&lt;-</span></span>
<span id="cb16-1288"><a href="#cb16-1288" aria-hidden="true" tabindex="-1"></a>  <span class="fu">readRDS</span>(<span class="fu">paste0</span>(path_data, <span class="st">"mgl_epit_sparse_geno.rds"</span>))</span>
<span id="cb16-1289"><a href="#cb16-1289" aria-hidden="true" tabindex="-1"></a>epit_sparse <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="fu">paste0</span>(path_data, <span class="st">"epit-spca-select.rds"</span>))</span>
<span id="cb16-1290"><a href="#cb16-1290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1291"><a href="#cb16-1291" aria-hidden="true" tabindex="-1"></a><span class="co"># Shorten columns' names</span></span>
<span id="cb16-1292"><a href="#cb16-1292" aria-hidden="true" tabindex="-1"></a><span class="co"># To do: add colors to cluster path for known groups</span></span>
<span id="cb16-1293"><a href="#cb16-1293" aria-hidden="true" tabindex="-1"></a>names_epit <span class="ot">&lt;-</span> epit_sparse <span class="sc">%&gt;%</span> <span class="fu">colnames</span>()</span>
<span id="cb16-1294"><a href="#cb16-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1295"><a href="#cb16-1295" aria-hidden="true" tabindex="-1"></a>cut_names <span class="ot">&lt;-</span> names_epit <span class="sc">%&gt;%</span></span>
<span id="cb16-1296"><a href="#cb16-1296" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sapply</span>(<span class="cf">function</span>(x)</span>
<span id="cb16-1297"><a href="#cb16-1297" aria-hidden="true" tabindex="-1"></a>    <span class="fu">gsub</span>(<span class="st">"log2_rpd."</span>, <span class="st">""</span>, x)) <span class="sc">%&gt;%</span></span>
<span id="cb16-1298"><a href="#cb16-1298" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sapply</span>(<span class="cf">function</span>(x)</span>
<span id="cb16-1299"><a href="#cb16-1299" aria-hidden="true" tabindex="-1"></a>    <span class="fu">gsub</span>(<span class="st">"new_"</span>, <span class="st">""</span>, x)) <span class="sc">%&gt;%</span></span>
<span id="cb16-1300"><a href="#cb16-1300" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.character</span>()</span>
<span id="cb16-1301"><a href="#cb16-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1302"><a href="#cb16-1302" aria-hidden="true" tabindex="-1"></a><span class="do">####</span></span>
<span id="cb16-1303"><a href="#cb16-1303" aria-hidden="true" tabindex="-1"></a>order_omics <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb16-1304"><a href="#cb16-1304" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"exp"</span>, cut_names),</span>
<span id="cb16-1305"><a href="#cb16-1305" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"gbM.CG"</span>, cut_names),</span>
<span id="cb16-1306"><a href="#cb16-1306" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"gbM.CHG"</span>, cut_names),</span>
<span id="cb16-1307"><a href="#cb16-1307" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"gbM.CHH"</span>, cut_names),</span>
<span id="cb16-1308"><a href="#cb16-1308" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"prom.CG"</span>, cut_names),</span>
<span id="cb16-1309"><a href="#cb16-1309" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"prom.CHG"</span>, cut_names),</span>
<span id="cb16-1310"><a href="#cb16-1310" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grep</span>(<span class="st">"prom.CHH"</span>, cut_names))</span>
<span id="cb16-1311"><a href="#cb16-1311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1312"><a href="#cb16-1312" aria-hidden="true" tabindex="-1"></a>col_leaves <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">rep</span>(<span class="fu">rainbow_hcl</span>(<span class="dv">7</span>, <span class="at">c=</span><span class="dv">90</span>, <span class="at">l=</span><span class="dv">50</span>), <span class="at">each =</span> <span class="dv">10</span>))</span>
<span id="cb16-1313"><a href="#cb16-1313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1314"><a href="#cb16-1314" aria-hidden="true" tabindex="-1"></a>col_leaves <span class="ot">&lt;-</span> col_leaves[<span class="fu">order</span>(order_omics)]</span>
<span id="cb16-1315"><a href="#cb16-1315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1316"><a href="#cb16-1316" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(col_leaves) <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="st">"RNA-Seq"</span> <span class="ot">=</span> <span class="st">"#0093A9"</span>,</span>
<span id="cb16-1317"><a href="#cb16-1317" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CpG-Body"</span> <span class="ot">=</span> <span class="st">"#00944F"</span>,</span>
<span id="cb16-1318"><a href="#cb16-1318" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CHG-Body"</span> <span class="ot">=</span> <span class="st">"#4473D7"</span>,</span>
<span id="cb16-1319"><a href="#cb16-1319" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CHH-Body"</span> <span class="ot">=</span> <span class="st">"#5D8400"</span>,</span>
<span id="cb16-1320"><a href="#cb16-1320" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CpG-Promoter"</span> <span class="ot">=</span> <span class="st">"#A86B00"</span>,</span>
<span id="cb16-1321"><a href="#cb16-1321" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CHG-Promoter"</span> <span class="ot">=</span> <span class="st">"#C03FBE"</span>,</span>
<span id="cb16-1322"><a href="#cb16-1322" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"CHH-Promoter"</span> <span class="ot">=</span> <span class="st">"#CC476B"</span>)</span>
<span id="cb16-1323"><a href="#cb16-1323" aria-hidden="true" tabindex="-1"></a><span class="do">####</span></span>
<span id="cb16-1324"><a href="#cb16-1324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1325"><a href="#cb16-1325" aria-hidden="true" tabindex="-1"></a>plot_clusterpath <span class="ot">&lt;-</span> <span class="cf">function</span>(X, mglasso_res, <span class="at">colnames_ =</span> <span class="cn">NULL</span>, max.overlaps, <span class="at">cut_k_vars =</span> <span class="dv">5</span>, colors_) {</span>
<span id="cb16-1326"><a href="#cb16-1326" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Initialisations</span></span>
<span id="cb16-1327"><a href="#cb16-1327" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb16-1328"><a href="#cb16-1328" aria-hidden="true" tabindex="-1"></a>  df.paths <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">c</span>(),<span class="at">y=</span><span class="fu">c</span>(), <span class="at">group=</span><span class="fu">c</span>())</span>
<span id="cb16-1329"><a href="#cb16-1329" aria-hidden="true" tabindex="-1"></a>  nlevel <span class="ot">&lt;-</span> <span class="fu">length</span>(mglasso_res)</span>
<span id="cb16-1330"><a href="#cb16-1330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1331"><a href="#cb16-1331" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Principal component analysis</span></span>
<span id="cb16-1332"><a href="#cb16-1332" aria-hidden="true" tabindex="-1"></a>  svdX <span class="ot">&lt;-</span> <span class="fu">svd</span>(X)                <span class="do">## singular value decomposition</span></span>
<span id="cb16-1333"><a href="#cb16-1333" aria-hidden="true" tabindex="-1"></a>  pc <span class="ot">&lt;-</span> svdX<span class="sc">$</span>u[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,drop<span class="ot">=</span><span class="cn">FALSE</span>] <span class="do">## singular vectors</span></span>
<span id="cb16-1334"><a href="#cb16-1334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1335"><a href="#cb16-1335" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> cut_k_vars<span class="sc">:</span>nlevel) {</span>
<span id="cb16-1336"><a href="#cb16-1336" aria-hidden="true" tabindex="-1"></a>    Beta <span class="ot">&lt;-</span> mglasso_res[[j]]<span class="sc">$</span>selected_Theta</span>
<span id="cb16-1337"><a href="#cb16-1337" aria-hidden="true" tabindex="-1"></a>    Xpred <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>p, <span class="cf">function</span>(i){X <span class="sc">%*%</span> Beta[i,]})</span>
<span id="cb16-1338"><a href="#cb16-1338" aria-hidden="true" tabindex="-1"></a>    pcs <span class="ot">&lt;-</span> <span class="fu">t</span>(pc)<span class="sc">%*%</span>Xpred</span>
<span id="cb16-1339"><a href="#cb16-1339" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> pcs[<span class="dv">1</span>,]</span>
<span id="cb16-1340"><a href="#cb16-1340" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> pcs[<span class="dv">2</span>,]</span>
<span id="cb16-1341"><a href="#cb16-1341" aria-hidden="true" tabindex="-1"></a>    df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>pcs[<span class="dv">1</span>,], <span class="at">y=</span>pcs[<span class="dv">2</span>,], <span class="at">group=</span><span class="dv">1</span><span class="sc">:</span>p, <span class="at">Data =</span> colors_)</span>
<span id="cb16-1342"><a href="#cb16-1342" aria-hidden="true" tabindex="-1"></a>    df.paths <span class="ot">&lt;-</span> <span class="fu">rbind</span>(df.paths,df)</span>
<span id="cb16-1343"><a href="#cb16-1343" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-1344"><a href="#cb16-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1345"><a href="#cb16-1345" aria-hidden="true" tabindex="-1"></a>  <span class="co"># X_data &lt;- as.data.frame(t(X) %*% pc) ## PCA projections (scores)</span></span>
<span id="cb16-1346"><a href="#cb16-1346" aria-hidden="true" tabindex="-1"></a>  X_data <span class="ot">&lt;-</span> df.paths[<span class="dv">1</span><span class="sc">:</span>p,]</span>
<span id="cb16-1347"><a href="#cb16-1347" aria-hidden="true" tabindex="-1"></a>  <span class="co">#colnames(X_data) &lt;- c("x", "y")</span></span>
<span id="cb16-1348"><a href="#cb16-1348" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ifelse</span>(<span class="fu">is.null</span>(colnames_),</span>
<span id="cb16-1349"><a href="#cb16-1349" aria-hidden="true" tabindex="-1"></a>         X_data<span class="sc">$</span>Name <span class="ot">&lt;-</span> <span class="fu">colnames</span>(X),</span>
<span id="cb16-1350"><a href="#cb16-1350" aria-hidden="true" tabindex="-1"></a>         X_data<span class="sc">$</span>Name <span class="ot">&lt;-</span> colnames_)</span>
<span id="cb16-1351"><a href="#cb16-1351" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> df.paths, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb16-1352"><a href="#cb16-1352" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb16-1353"><a href="#cb16-1353" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_path</span>(<span class="fu">aes</span>(<span class="at">group =</span> group,  <span class="at">colour =</span> Data), <span class="at">alpha =</span> <span class="fl">0.5</span>)</span>
<span id="cb16-1354"><a href="#cb16-1354" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb16-1355"><a href="#cb16-1355" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_text_repel</span>(<span class="at">data =</span> X_data,</span>
<span id="cb16-1356"><a href="#cb16-1356" aria-hidden="true" tabindex="-1"></a>                                <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">label =</span> Name),</span>
<span id="cb16-1357"><a href="#cb16-1357" aria-hidden="true" tabindex="-1"></a>                                <span class="at">max.overlaps =</span> max.overlaps)</span>
<span id="cb16-1358"><a href="#cb16-1358" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb16-1359"><a href="#cb16-1359" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">data =</span> X_data, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">colour =</span> Data), <span class="at">size =</span> <span class="fl">1.5</span>)</span>
<span id="cb16-1360"><a href="#cb16-1360" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="ot">&lt;-</span></span>
<span id="cb16-1361"><a href="#cb16-1361" aria-hidden="true" tabindex="-1"></a>    data_plot <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">'Principal Component 1'</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">'Principal Component 2'</span>)</span>
<span id="cb16-1362"><a href="#cb16-1362" aria-hidden="true" tabindex="-1"></a>  data_plot <span class="sc">+</span> <span class="fu">theme_bw</span>()</span>
<span id="cb16-1363"><a href="#cb16-1363" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-1364"><a href="#cb16-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1365"><a href="#cb16-1365" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_clusterpath</span>(<span class="fu">as.matrix</span>(epit_sparse), mglasso_genot, cut_names, <span class="at">max.overlaps =</span> <span class="dv">20</span>, <span class="at">cut_k_vars =</span> <span class="dv">1</span>, <span class="at">colors_ =</span> col_leaves)</span>
<span id="cb16-1366"><a href="#cb16-1366" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1367"><a href="#cb16-1367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1368"><a href="#cb16-1368" aria-hidden="true" tabindex="-1"></a>The results of the MGLasso can also be represented in the expanded way where</span>
<span id="cb16-1369"><a href="#cb16-1369" aria-hidden="true" tabindex="-1"></a>meta-variables are not computed from clusters. In @fig-graphpath-poplar, a focus</span>
<span id="cb16-1370"><a href="#cb16-1370" aria-hidden="true" tabindex="-1"></a>is put on the effect of the fusion penalty. Clusters partitions are not</span>
<span id="cb16-1371"><a href="#cb16-1371" aria-hidden="true" tabindex="-1"></a>presented. The higher the fusion penalty, variables are encouraged to share the</span>
<span id="cb16-1372"><a href="#cb16-1372" aria-hidden="true" tabindex="-1"></a>same neighborhood structure. Note that an equivalent graph over meta-variables</span>
<span id="cb16-1373"><a href="#cb16-1373" aria-hidden="true" tabindex="-1"></a>can be computed after choosing a fusion threshold as in @fig-meta-graphs.</span>
<span id="cb16-1374"><a href="#cb16-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1377"><a href="#cb16-1377" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb16-1378"><a href="#cb16-1378" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-graphpath-poplar</span></span>
<span id="cb16-1379"><a href="#cb16-1379" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Adjacency matrices for different fusion penalty parameters. The first graph shows the inferred network when no fusion penalty is added to the model. In that graph, the first block of size $10 \times 10$ variables corresponds to RNA-Seq samples. The second sparser block of size $30 \times 30$ corresponds to gene-body DNA methylation data in the three methylation contexts. The last sparse block of the same size corresponds to promoter methylation. The edge bands suggest a relationship between DNA methylation measurements that belong to the same context. For example, the Loire methylation sample in the CpG context is likely related to the Loire samples in the CHG and CHH contexts. The graphs also suggest some relationships between expression and methylation for some natural populations. As the merging penalty increases, the blocks corresponding to the three methylation contexts merge first, then follow the upper left block corresponding to the expression data. For $\lambda_2 = 30.94,$ all natural populations merge into a single cluster and complete graph.</span></span>
<span id="cb16-1380"><a href="#cb16-1380" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb16-1381"><a href="#cb16-1381" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Full graph with $\lambda_2$ = 0</span></span>
<span id="cb16-1382"><a href="#cb16-1382" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Full graph with $\lambda_2$ = 1.63</span></span>
<span id="cb16-1383"><a href="#cb16-1383" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Full graph with $\lambda_2$ = 3.26</span></span>
<span id="cb16-1384"><a href="#cb16-1384" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Full graph with $\lambda_2$ = 4.89</span></span>
<span id="cb16-1385"><a href="#cb16-1385" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Full graph with $\lambda_2$ = 30.94</span></span>
<span id="cb16-1386"><a href="#cb16-1386" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 3</span></span>
<span id="cb16-1387"><a href="#cb16-1387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1388"><a href="#cb16-1388" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot adjacency matrices for some levels </span></span>
<span id="cb16-1389"><a href="#cb16-1389" aria-hidden="true" tabindex="-1"></a><span class="co"># Selection based on network interpretability  </span></span>
<span id="cb16-1390"><a href="#cb16-1390" aria-hidden="true" tabindex="-1"></a><span class="co">#' symmetrize matrix of regression vectors pxp</span></span>
<span id="cb16-1391"><a href="#cb16-1391" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb16-1392"><a href="#cb16-1392" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj_mat</span>(mglasso_genot<span class="sc">$</span><span class="st">`</span><span class="at">1</span><span class="st">`</span><span class="sc">$</span>selected_Theta[order_omics, order_omics]),</span>
<span id="cb16-1393"><a href="#cb16-1393" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb16-1394"><a href="#cb16-1394" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-1395"><a href="#cb16-1395" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb16-1396"><a href="#cb16-1396" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj_mat</span>(mglasso_genot<span class="sc">$</span><span class="st">`</span><span class="at">2</span><span class="st">`</span><span class="sc">$</span>selected_Theta[order_omics, order_omics]),</span>
<span id="cb16-1397"><a href="#cb16-1397" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb16-1398"><a href="#cb16-1398" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-1399"><a href="#cb16-1399" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb16-1400"><a href="#cb16-1400" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj_mat</span>(mglasso_genot<span class="sc">$</span><span class="st">`</span><span class="at">3</span><span class="st">`</span><span class="sc">$</span>selected_Theta[order_omics, order_omics]),</span>
<span id="cb16-1401"><a href="#cb16-1401" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb16-1402"><a href="#cb16-1402" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-1403"><a href="#cb16-1403" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb16-1404"><a href="#cb16-1404" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj_mat</span>(mglasso_genot<span class="sc">$</span><span class="st">`</span><span class="at">4</span><span class="st">`</span><span class="sc">$</span>selected_Theta[order_omics, order_omics]),</span>
<span id="cb16-1405"><a href="#cb16-1405" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb16-1406"><a href="#cb16-1406" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-1407"><a href="#cb16-1407" aria-hidden="true" tabindex="-1"></a>Matrix<span class="sc">::</span><span class="fu">image</span>(</span>
<span id="cb16-1408"><a href="#cb16-1408" aria-hidden="true" tabindex="-1"></a>  <span class="fu">adj_mat</span>(mglasso_genot<span class="sc">$</span><span class="st">`</span><span class="at">20</span><span class="st">`</span><span class="sc">$</span>selected_Theta[order_omics, order_omics]),</span>
<span id="cb16-1409"><a href="#cb16-1409" aria-hidden="true" tabindex="-1"></a>  <span class="at">sub =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span></span>
<span id="cb16-1410"><a href="#cb16-1410" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-1411"><a href="#cb16-1411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1412"><a href="#cb16-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1413"><a href="#cb16-1413" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusion</span></span>
<span id="cb16-1414"><a href="#cb16-1414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1415"><a href="#cb16-1415" aria-hidden="true" tabindex="-1"></a>We proposed a new technique that combines Gaussian Graphical Model inference and</span>
<span id="cb16-1416"><a href="#cb16-1416" aria-hidden="true" tabindex="-1"></a>hierarchical clustering called MGLasso. The method proceeds via convex</span>
<span id="cb16-1417"><a href="#cb16-1417" aria-hidden="true" tabindex="-1"></a>optimization and minimizes the neighborhood selection objective penalized by a</span>
<span id="cb16-1418"><a href="#cb16-1418" aria-hidden="true" tabindex="-1"></a>hybrid regularization combining a sparsity-inducing norm and a convex clustering</span>
<span id="cb16-1419"><a href="#cb16-1419" aria-hidden="true" tabindex="-1"></a>penalty. We developed a complete numerical scheme to apply MGLasso in practice,</span>
<span id="cb16-1420"><a href="#cb16-1420" aria-hidden="true" tabindex="-1"></a>with an optimization algorithm based on CONESTA and a model selection procedure.</span>
<span id="cb16-1421"><a href="#cb16-1421" aria-hidden="true" tabindex="-1"></a>Our simulations results over synthetic and real datasets showed that MGLasso can</span>
<span id="cb16-1422"><a href="#cb16-1422" aria-hidden="true" tabindex="-1"></a>perform better than GLasso in network support recovery in the presence of groups</span>
<span id="cb16-1423"><a href="#cb16-1423" aria-hidden="true" tabindex="-1"></a>of correlated variables, and we illustrated the method with the analysis of</span>
<span id="cb16-1424"><a href="#cb16-1424" aria-hidden="true" tabindex="-1"></a>microbial associations data and methylation mixed with transcriptomic data. </span>
<span id="cb16-1425"><a href="#cb16-1425" aria-hidden="true" tabindex="-1"></a>The present work paves the way for future</span>
<span id="cb16-1426"><a href="#cb16-1426" aria-hidden="true" tabindex="-1"></a>improvements: first, by incorporating prior knowledge through more flexible</span>
<span id="cb16-1427"><a href="#cb16-1427" aria-hidden="true" tabindex="-1"></a>weighted regularization; second, by studying the theoretical properties of the</span>
<span id="cb16-1428"><a href="#cb16-1428" aria-hidden="true" tabindex="-1"></a>method in terms of statistical guarantees for the MGLasso estimator. Moreover,</span>
<span id="cb16-1429"><a href="#cb16-1429" aria-hidden="true" tabindex="-1"></a>the node-wise regression approach on which our method is based can be extended</span>
<span id="cb16-1430"><a href="#cb16-1430" aria-hidden="true" tabindex="-1"></a>to a broader family of non-Gaussian distributions belonging to the exponential</span>
<span id="cb16-1431"><a href="#cb16-1431" aria-hidden="true" tabindex="-1"></a>family as outlined by @yang2012graphical.  Our MGLasso approach can be easily</span>
<span id="cb16-1432"><a href="#cb16-1432" aria-hidden="true" tabindex="-1"></a>extended to non-Gaussian distributions belonging to the exponential family and</span>
<span id="cb16-1433"><a href="#cb16-1433" aria-hidden="true" tabindex="-1"></a>mixed graphical models. </span>
<span id="cb16-1434"><a href="#cb16-1434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1435"><a href="#cb16-1435" aria-hidden="true" tabindex="-1"></a><span class="fu"># Appendix {.appendix .unnumbered}  </span></span>
<span id="cb16-1436"><a href="#cb16-1436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1437"><a href="#cb16-1437" aria-hidden="true" tabindex="-1"></a>The scripts to reproduce the simulations are available at <span class="ot">&lt;https://github.com/computorg/published-202306-sanou-multiscale_glasso/tree/main/scripts/simulation-experiments&gt;</span>.</span>
<span id="cb16-1438"><a href="#cb16-1438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1439"><a href="#cb16-1439" aria-hidden="true" tabindex="-1"></a><span class="fu"># Acknowledgments {.appendix .unnumbered}  </span></span>
<span id="cb16-1440"><a href="#cb16-1440" aria-hidden="true" tabindex="-1"></a>The authors would like to thank the Editors and referees for comments that led to substantial improvements in the manuscript.</span>
<span id="cb16-1441"><a href="#cb16-1441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1442"><a href="#cb16-1442" aria-hidden="true" tabindex="-1"></a><span class="fu"># Session information {.appendix .unnumbered}</span></span>
<span id="cb16-1443"><a href="#cb16-1443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1444"><a href="#cb16-1444" aria-hidden="true" tabindex="-1"></a><span class="in">```{r session-info, echo = TRUE}</span></span>
<span id="cb16-1445"><a href="#cb16-1445" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span>
<span id="cb16-1446"><a href="#cb16-1446" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>
<script>
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>



</body></html>