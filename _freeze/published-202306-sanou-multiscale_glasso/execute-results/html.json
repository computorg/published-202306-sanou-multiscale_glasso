{
  "hash": "eee31bbf1b475e525613bf93fd80015a",
  "result": {
    "markdown": "---\ntitle: \"Inference of Multiscale Gaussian Graphical Models\"\nsubtitle: \"\"\nauthor:\n  - name: Edmond Sanou\n    corresponding: true\n    email: doedmond.sanou@univ-evry.fr \n    url: https://desanou.github.io/\n    affiliations:\n      - name: Université Paris-Saclay, CNRS, Univ Evry, Laboratoire de Mathématiques et Modélisation d'Evry\n        url: http://www.math-evry.cnrs.fr/\n  - name: Christophe Ambroise\n    email: christophe.ambroise@univ-evry.fr\n    url: https://cambroise.github.io/\n    affiliations:\n      - name: Université Paris-Saclay, CNRS, Univ Evry, Laboratoire de Mathématiques et Modélisation d'Evry\n        url: http://www.math-evry.cnrs.fr/\n  - name: Geneviève Robin\n    email: genevievelrobin@gmail.com\n    url: https://genevieverobin.wordpress.com/\n    affiliations:\n      - name: Université Paris-Saclay, CNRS, Univ Evry, Laboratoire de Mathématiques et Modélisation d'Evry\n        url: http://www.math-evry.cnrs.fr/\ndate: last-modified\ndate-modified: last-modified\nabstract: >+\n  Gaussian Graphical Models (GGMs) are widely used in high-dimensional data \n  analysis to synthesize the interaction between variables. In many \n  applications, such as genomics or image analysis, graphical models \n  rely on sparsity and clustering to reduce dimensionality and improve \n  performances. This paper explores a slightly different paradigm where \n  clustering is not knowledge-driven but performed simultaneously with \n  the graph inference task. We introduce a novel Multiscale Graphical \n  Lasso (MGLasso) to improve networks interpretability by proposing graphs \n  at different granularity levels. The method estimates clusters through a \n  convex clustering approach --- a relaxation of $k$-means, and hierarchical \n  clustering. The conditional independence graph is simultaneously inferred \n  through a neighborhood selection scheme for undirected graphical models. \n  MGLasso extends and generalizes the sparse group fused lasso problem to \n  undirected graphical models. We use continuation with Nesterov smoothing \n  in a shrinkage-thresholding algorithm (CONESTA) to propose a regularization \n  path of solutions along the group fused Lasso penalty, \n  while the Lasso penalty is kept constant. Extensive \n  experiments on synthetic data compare the performances of our model \n  to state-of-the-art clustering methods and network inference models. \n  Applications to gut microbiome data and poplar's methylation mixed with \n  transcriptomic data are presented.\nkeywords: [Neighborhood selection, Convex hierarchical clustering, Gaussian graphical models]\ncitation:\n  type: article-journal\n  container-title: \"Computo\"\n  doi: \"10.57750/1f4p-7955\"\n  url: https://computo.sfds.asso.fr/published-202306-sanou-multiscale_glasso\n  publisher: \"Société Française de Statistique\"\n  issn: \"2824-7795\"\nbibliography: references.bib\ngithub-user: computorg\nrepo: \"published-202306-sanou-multiscale_glasso\"\ndraft: false\npublished: true\nformat:\n  computo-html: default\n  computo-pdf: default\nexecute:\n  freeze: auto  # re-render only when source changes\n---\n\n\n# Introduction \n\nProbabilistic graphical models [@Lauritzen1996; @Koller2009] are widely used in\nhigh-dimensional data analysis to synthesize the interaction between variables.\nIn many applications, such as genomics or image analysis, graphical models\nreduce the number of parameters by selecting the most relevant interactions\nbetween variables. Undirected _Gaussian Graphical Models_ (GGMs) are a class of\ngraphical models used in Gaussian settings. In the context of high-dimensional\nstatistics, graphical models are generally assumed sparse, meaning that a small\nnumber of variables interact compared to the total number of possible\ninteractions. This assumption has been shown to provide both statistical and\ncomputational advantages by simplifying the structure of dependence between\nvariables [@Dempster1972] and allowing efficient algorithms [@Meinshausen2006].\nSee, for instance, @Fan2016 for a review of sparse graphical models inference.\n\nIn GGMs, it is well known [@Lauritzen1996] that inferring the graphical model\nor, equivalently, the _conditional independence graph_ (CIG) boils down to\ninferring the support of the precision matrix $\\mathbf{\\Omega}$ (the inverse of\nthe variance-covariance matrix). Several $\\ell_1$ penalized methods have been\nproposed in the literature to learn the CIG of GGMs. For instance, _the\nneighborhood selection_ [MB,@Meinshausen2006] based on a nodewise regression\napproach via the _least absolute shrinkage and selection operator_ [Lasso,\n@tibshirani1996] is a popular method. Each variable is regressed on the others,\ntaking advantage of the link between the so-obtained regression coefficients and\npartial correlations. The MB method has generated a long line of work in\nnodewise regression methods. For instance, @Rocha2008 and @Ambroise2009 showed\nthat nodewise regression could be seen as a pseudo-likelihood approximation and\n@Peng2009 extended the MB method to estimate sparse partial correlations using a\nsingle regression problem. Other inference methods similar to nodewise\nregression include a method based on the Dantzig selector [@Yuan2010] and the\nintroduction of the Clime estimator [@Cai2011]. Another family of sparse CIG\ninference methods directly estimates $\\mathbf{\\Omega}$ via direct minimization\nof the $\\ell_1$-penalized negative log-likelihood [@Banerjee2008], without\nresorting to the auxiliary regression problem. This method called the _graphical\nLasso_ [GLasso, @Friedman2007], benefits from many optimization algorithms\n[@Yuan2007; @Rothman2008; @Banerjee2008; @Hsieh2014].\n\nSuch inference methods are widely used and enjoy many favorable theoretical and\nempirical properties, including robustness to high-dimensional problems.\nHowever, some limitations have been observed, particularly in the presence of\nstrongly correlated variables. Known impairments of Lasso-type regularization\ncause these limitations in this context [@Buhlmann2012; @Park2007]. To overcome\nthis, in addition to sparsity, several previous works attempt to estimate CIG by\nintegrating clustering structures among variables for statistical sanity and\ninterpretability. A non-exhaustive list of works that integrate a clustering\nstructure to speed up or improve the estimation procedure includes @Honorio2009,\n@Ambroise2009, @Mazumder2012, @Tan2013, @Devijver2018, @Yao2019.\n\nThe above methods exploit the group structure to simplify the graph inference\nproblem and infer the CIG between single variables. Another question that has\nreceived less attention is the inference of the CIG between the groups of\nvariables, i.e., between the meta-variables representative of the group\nstructure. A recent work introducing inference of graphical models on multiple\ngrouping levels is @Cheng2017. They proposed inferring the CIG of gene data on\ntwo levels corresponding to genes and pathways, respectively. Note that pathways\nare considered as groups of functionally related genes known in advance. The\ninference is achieved by optimizing a penalized maximum likelihood that\nestimates a sparse network at both gene and group levels. Our work is also part\nof this dynamic. We introduce a penalty term allowing parsimonious networks to\nbe built at different clustering levels. The main difference with the procedure\nof @Cheng2017 is that we do not require prior knowledge of the group structure,\nwhich makes the problem significantly more complex. In addition, our method has\nthe advantage of proposing CIGs at more than two levels of granularity.\n\nWe introduce the Multiscale Graphical Lasso (MGLasso), a novel method to\nestimate simultaneously a hierarchical clustering structure and graphical models\ndepicting the conditional independence structure between clusters of variables\nat each level of the hierarchy. Our approach is based on neighborhood selection\n[@Meinshausen2006] and considers an additional fused-Lasso type penalty for\nclustering [@pelckmans2005convex; @Hocking2011; @Lindsten2011].\n\nThe use of fusion penalties in Gaussian graphical model inference is a\nwell-studied area. Some prior works on learning sparse GGMs with a fusion\npenalty term have focused on penalized likelihood. Among those, a line of works\n[@danaher2014joint; @yang2015fused] infers multiple graphs across several\nclasses while assuming the observations belong to different known clusters.\nAnother line of research [@Honorio2009; @Yao2019; @lin2020estimation]\ninvestigates fusion penalties for enforcing local constancy in the nodes of the\ninferred network. Variables belonging to the same clusters are thus more likely\nto share the same neighborhood. These ordinary likelihood-based models are\ncomputationally challenging compared to pseudo-likelihood approximations. The\nunpublished manuscript of @ganguly2014 introduces a fusion-like penalty in the\nneighborhood selection framework. However, the problem is solved in a node-wise\nregression fashion where the $p$ regressions problems are not combined.\n\nFusion penalties have also been used in simple regression problems\n[@tibshirani2005sparsity] and multivariate regression analysis (multitask\nlearning) with multiple outcomes [see, e.g., @chen2010graph; @degras2021sparse;\n@dondelinger2020joint; @hallac2015network; @chu2021adaptive]. The defined\npenalties encourage fusion between predictors in simple regression, or outcomes\nthat share similar model coefficients in multitask learning. Fusions can be\nformulated in a general form assuming no order on the variables as in convex\nclustering [@Hoefling2010; @petry2011pairwise] or assuming the availability of\nprior information about clusters [@rudin1992nonlinear; @hallac2015network].\n\nThe multitask learning framework can be extended to the learning of GGMs.\n@chiquet2011inferring introduced a multitask inference for multiple graphical\nmodels when observations belong to different clusters. In MGLasso, the multitask\nlearning framework is combined with a novel general fusion penalty to uncover\nclustering over variables. In the defined fusion term, we consider reordering\nthe regression coefficients to match common predictors and symmetric\ncoefficients. That results in enforcing the grouping property by encouraging\nvariables belonging to the same cluster to have the same neighborhood. MGLasso\nexploits the multitask learning framework for GGMs inference coupled with a\nconvex clustering problem over the nodes to infer multiscale networks and\nclusters simultaneously. To our knowledge, this is the first attempt in the\nliterature of undirected GGMs. MGLasso can also be seen as an extension of sparse\ngroup fused Lasso for graphical models and be straightforwardly extended to\nprobability distributions belonging to the exponential family\n[@yang2012graphical]. The MGLasso algorithm is implemented in the R package _mglasso_ available at\n<https://CRAN.R-project.org/package=mglasso>. The remainder of this paper is\norganized as follows. In Section [2](#multiscale-graphical-lasso) and Section\n[3](#numerical-scheme), we formally introduce the Multiscale Graphical Lasso and\nits optimization algorithm. Section [4](#simulation-experiments) presents\nsimulated and real data numerical results.\n\n# Multiscale Graphical Lasso {#multiscale-graphical-lasso}\n\nLet $\\mathbf X = (X^1, \\dots, X^p)^T$ be a $p$-dimensional Gaussian random\nvector, with mean vector $\\boldsymbol \\mu \\in \\mathbb R^p$ and positive definite\ncovariance matrix $\\mathbf \\Sigma \\in \\mathbb R^{p \\times p}$. Let $G = (V, E)$\nbe a graph encoding the conditional independence structure of the normal\ndistribution $\\mathcal N(\\boldsymbol \\mu, \\mathbf \\Sigma),$ where \n$V = \\{1,\\ldots p\\}$ is the set of vertices and $E$ the set of edges. The graph $G$\nis uniquely determined by the support of the precision matrix \n$\\mathbf{\\Omega} = \\mathbf{\\Sigma}^{-1}$ [@Dempster1972]. \nSpecifically, for any two vertices $i \\neq j\\in V$, \nthe edge $(i,j)$ belongs to the set $E$ if and only if\n$\\Omega_{ij} \\neq 0.$ On the contrary, if $\\Omega_{ij} = 0$,\nthe variables $X^i$ and $X^j$ are said to be independent\nconditionally to the remaining variables $X^{\\setminus (i, j)}$. We note, \n$$\nX^i\n\\perp \\!\\!\\! \\perp X^j |X^{\\setminus (i, j)} \\Leftrightarrow \\Omega_{ij} = 0.\n$$\n\nLet $\\boldsymbol X = \\left( \\boldsymbol X_1^T, \\dots, \\boldsymbol X_n^T\n\\right )^T$ be the $n \\times p$-dimensional data matrix composed of $n$ i.i.d\nsamples of the Gaussian random vector $\\mathbf X$. To perform graphical model\ninference, @Meinshausen2006 consider $p$ separate linear regressions of the\nform: \n$$\n\\hat{\\boldsymbol{\\beta}^i}(\\lambda) = \\underset{\\boldsymbol{\\beta}^i\n\\in \\mathbb{R}^{p-1}}{\\operatorname{argmin}} \\frac{1}{n} \\left \\lVert\n\\mathbf{X}^i - \\mathbf{X}^{\\setminus i} \\boldsymbol{\\beta}^i \\right \\rVert_2 ^2\n+ \\lambda \\left \\lVert \\boldsymbol{\\beta}^i \\right \\rVert_1,\n$${#eq-neighborhood} \nwhere $\\lambda$ is a non-negative regularization parameter,\n$\\mathbf{X}^{\\setminus i}$ denotes the matrix $\\mathbf{X}$ deprived of column\n$i$, $\\boldsymbol{\\beta}^i = (\\beta^i_j)_{j \\in \\{1,\\dots,p\\} \\backslash i}$ \nis a vector of $p-1$ regression coefficients and\n$\\left \\lVert . \\right \\rVert_1$ is the $\\ell_1-$norm. These Lasso regularized\nproblems estimate the neighborhoods, one variable at a time. The final edge set\nestimates $\\hat E$ can be deduced from the union of the estimated neighborhoods\nusing an AND or OR rule (@Meinshausen2006). The MB approach is based on the\ncentral relationship between simple linear regression and precision matrix\ncoefficients. It can be shown that $\\beta^i_j =\n-\\frac{\\Omega_{ij}}{\\Omega_{ii}}$ [@Lauritzen1996].\n\nOn the other hand, let us now consider the clustering analysis of the $p$\nvariables in $\\mathbb R^n.$ The convex clustering problem [@Hocking2011; @Lindsten2011;\n@pelckmans2005convex] is the minimization of the quantity \n$$\n\\frac{1}{2}\n\\sum_{i=1}^p \\left \\lVert \\boldsymbol X^i - \\boldsymbol  \\alpha^i \\right\n\\rVert_2^2 + \\lambda \\sum_{i < j} w_{ij} \\left \\lVert \\boldsymbol  \\alpha^i -\n\\boldsymbol  \\alpha^j \\right \\rVert_q \n$$ {#eq-clusterpath} \nwith respect to the\nmatrix $\\boldsymbol \\alpha \\in \\mathbb R^{p \\times n}$, where $\\lambda$ is a\nsparsity penalization parameter, $\\{ w_{ij} \\}$ are symmetric positive weights,\n$\\boldsymbol \\alpha^i \\in \\mathbb R^n$ is the centroid to which $\\boldsymbol\nX^i$ is assigned to, and $\\left \\lVert . \\right \\rVert_q$ is the $\\ell_q$-norm\non $\\mathbb R^p$ with $q \\ge 1.$ Points $\\boldsymbol X^i$ and $\\boldsymbol X^j$\nare assigned to the same cluster if $\\hat{\\boldsymbol \\alpha^i} \\approx\n\\hat{\\boldsymbol \\alpha^j}.$ The regularization path of solutions to problem in\n@eq-clusterpath can be represented as a dendrogram. The path properties have\nbeen studied in @chi2015splitting and @chiquet2017fast, among others.\nNote that these approaches rely on geometric properties of matrix $\\boldsymbol X,$\nand do not require any assumption on the distribution of the covariates.\n\nWe propose to combine the MB and convex clustering approaches.\nSpecifically, the $p$ independent Lasso regressions of the MB approach are merged\ninto a single optimization criterion where a convex clustering fusion penalty in\n$\\ell_2$ is applied on the regression vectors considered as cluster centers.\nNamely, the _Multiscale Graphical Lasso_ (MGLasso) pseudo-likelihood problem\nminimizes in a Gaussian framework the following quantity: \n$$\nJ_{\\lambda_1,\n\\lambda_2}(\\boldsymbol{\\beta}; \\mathbf{X} ) = \\frac{1}{2} \\sum_{i=1}^p \\left\n\\lVert \\mathbf{X}^i - \\mathbf{X}^{\\setminus i} \\boldsymbol{\\beta}^i \\right\n\\rVert_2 ^2  + \\lambda_1 \\sum_{i = 1}^p  \\left \\lVert \\boldsymbol{\\beta}^i\n\\right \\rVert_1 + \\lambda_2 \\sum_{i < j} \\left \\lVert \\boldsymbol{\\beta}^i -\n\\boldsymbol \\tau_{ij}\\boldsymbol{\\beta}^j \\right \\rVert_2,\n$$ {#eq-cost-fct}\nwith respect to $\\boldsymbol{\\beta} := [{\\boldsymbol{\\beta}^1}, \\ldots,\n{\\boldsymbol{\\beta}^p}] \\in \\mathbb{R}^{(p-1) \\times p},$ where\n$\\mathbf{X}^{i}\\in \\mathbb{R}^n$ denotes the $i$-th column of $\\mathbf{X}$,\n$\\lambda_1$ and $\\lambda_2$ are penalization parameters, $\\boldsymbol \\tau_{ij}\n\\in \\mathbb R^{(p-1)\\times(p-1)}$ is a permutation matrix, which permutes the\ncoefficients in the regression vector $\\boldsymbol \\beta^j$ such as \n$$\n\\left\n\\lVert \\boldsymbol{\\beta}^i - \\boldsymbol \\tau_{ij}\\boldsymbol{\\beta}^j \\right\n\\rVert_2 = \\sqrt{\\sum_{k \\in \\{1, \\dots,p \\} \\backslash \\{i,j\\}} (\\beta^i_k -\n\\beta^j_k)^2 + (\\beta^i_j - \\beta^j_i)^2 },\n$$ \nas illustrated in\n@fig-permute-beta. The coefficient $\\beta^i_k$ is to be read as the multiple\nregression coefficients of $\\boldsymbol X^i$ on $\\boldsymbol X^k.$\n\nThe MGLasso criterion can be seen as a multitask regression problem where the\nset of responses is identical to the set of predictors. The Lasso penalty term\nencourages sparsity in the estimated coefficients while the group-fused term\nencourages fusion in the regression vectors $\\boldsymbol{\\beta}^i$ and\n$\\boldsymbol{\\beta}^j$.\n\nLet us illustrate by an example the effect of the fusion term in the proposed\napproach. Two variables $i$ and $j$ are in the same group when\n$\\|\\boldsymbol{\\beta}^i - \\boldsymbol \\tau_{ij} \\boldsymbol{\\beta}^j\\|_2 \\approx\n0$. Considering a cluster $\\mathcal C$ of $q$ variables, it is straightforward\nto show that $\\forall (i,j) \\in \\mathcal C^2$, we have $\\hat\n{\\beta^i_j}=\\beta_{\\mathcal C}$, where $\\beta_{\\mathcal C}$ is a scalar. Thus\nthe algorithm is likely to produce precision matrices with blocks of constant\nentries for a given value of $\\lambda_2,$ each block corresponding to a cluster.\nIn the same vein as @Park2007, a cluster composed of variables that share the\nsame coefficients can be summarized by a representative variable.\n\nA component-wise difference between two regression vectors without reordering\nthe coefficients would not necesarily cluster variables which share the same\nneighborhood. The permutation $\\boldsymbol \\tau_{ij}$ reoders coefficients in\nsuch a way that differences are taken between symmetric coeffecients and those\ncorresponding to the same set of predictors. The model is thus likely to cluster\ntogether variables that share the same neighboring structure and encourages\nsymmetric graph structures.\n\n:::{#fig-permute-beta} \n\n![](./figures/permute-beta.png)  \n\nIllustration of the permutation between regression coefficients in the MGLasso\nmodel. \n:::\n\n\nIn practice, when external information about the clustering structure is\navailable, the problem can be generalized into: \n$$\n\\min_{\\boldsymbol{\\beta}}\n\\sum_{i=1}^p\\frac{1}{2} \\left \\lVert \\mathbf{X}^i - \\mathbf{X}^{\\setminus i}\n\\boldsymbol{\\beta}^i \\right \\rVert_2 ^2  + \\lambda_1 \\sum_{i = 1}^p \\left \\lVert\n\\boldsymbol{\\beta}^i \\right \\rVert_1 + \\lambda_2 \\sum_{i < j}  w_{ij} \\left\n\\lVert \\boldsymbol{\\beta}^i - \\boldsymbol \\tau_{ij}\\boldsymbol{\\beta}^j \\right\n\\rVert_2,\n$$ {#eq-cost-fct-general} \nwhere $w_{ij}$ is a positive weight. In the\nremainder of the paper, we will assume that $w_{ij} = 1$ for simplicity.\n\n\n# Numerical scheme\n\nThis Section introduces a complete numerical scheme of the Multiscale Graphical\nLasso via convex optimization and a model selection procedure. Section\n[3.1](#optimization-via-conesta-algorithm) reviews the principles of the\nContinuation with Nesterov smoothing in a shrinkage-thresholding algorithm\n[CONESTA, @hadjselem2018]. Section\n[3.2](#reformulation-of-mglasso-for-conesta-algorithm) details a reformulation\nof the MGLasso criterion, which eases the use of CONESTA as a solver. Finally,\nSection [3.3](#model-selection) presents the procedure for selecting the\nregularization parameters.\n\n## Optimization via CONESTA algorithm {#optimization-via-conesta-algorithm}\n\nThe optimization problem for Multiscale Graphical Lasso is convex but not\nstraightforward to solve using classical algorithms because of the fused-lasso\ntype penalty, which is non-separable and admits no closed-form solution for the\nproximal gradient. We rely on the Continuation with Nesterov smoothing in a\nshrinkage-thresholding algorithm [@hadjselem2018] dedicated to high-dimensional\nregression problems with structured sparsity, such as group structures.\n\nThe CONESTA solver, initially introduced for neuro-imaging problems, addresses a\ngeneral class of convex optimization problems that include group-wise penalties.\nThe algorithm solves problems in the form  \n$$\n\\operatorname{minimize \\ w.r.t. }\n\\boldsymbol{\\theta} \\quad f(\\boldsymbol{\\theta}) = g(\\boldsymbol{\\theta}) +\n\\lambda_1 h(\\boldsymbol{\\theta}) + \\lambda_2 s(\\boldsymbol{\\theta}),\n$$ {#eq-conesta-criterion}  \nwhere $\\boldsymbol{\\theta}\\in \\mathbb{R}^d$ and\n$\\lambda_1$ and $\\lambda_2$ are penalty parameters.\n\nIn the original paper [@hadjselem2018], $g(\\boldsymbol{\\theta})$ is a\ndifferentiable function, $h(\\boldsymbol{\\theta})$ is a penalty function whose\nproximal operator $\\operatorname{prox}_{\\lambda_1 h}$ is known in closed-form.\n\nGiven $\\phi \\subseteq \\{1,\\ldots, d\\},$ let $\\boldsymbol{\\theta}_\\phi =\n(\\theta_i)_{i \\in \\phi}$ denote the subvector of $\\boldsymbol{\\theta}$\nreferenced by the indices in $\\phi.$ Denote $\\Phi = \\{ \\phi_1, \\dots,\n\\phi_{\\operatorname{Card}(\\Phi)}\\}$ a collection with $\\phi_i \\subseteq\n\\{1,\\ldots, d\\}.$ Let the matrix $\\mathbf{A}_\\phi \\in \\mathbb{R}^{m \\times\n\\operatorname{Card}(\\Phi) }$ define a linear map from\n$\\mathbb{R}^{\\operatorname{Card}(\\phi)}$ to $\\mathbb{R}^m$ by sending the column\nvector $\\boldsymbol{\\theta}_\\phi \\in \\mathbb{R}^{\\operatorname{Card}(\\phi)}$ to\nthe column vector $\\mathbf{A}_\\phi \\boldsymbol{\\theta}_\\phi \\in \\mathbb{R}^m.$\nThe function $s(\\boldsymbol{\\theta})$ is assumed to be an $\\ell_{1,2}$-norm\ni.e., the sum of the group-wise $\\ell_2$-norms of the elements $\\mathbf{A}_\\phi\n\\boldsymbol{\\theta}_\\phi, \\phi \\in \\Phi.$ Namely, \n$$\ns(\\boldsymbol{\\theta}) =\n\\sum_{\\phi \\in \\Phi} \\|\\mathbf{A}_\\phi \\boldsymbol{\\theta}_\\phi\\|_2.\n$$ \nWhen\n$\\mathbf{A}_\\phi$ is the identity operator, the penalty function $s$ is the\noverlapping group-lasso and $m = \\operatorname{Card}(\\phi)$. When it is a\ndiscrete derivative operator,  $s$ is a total variation penalty, and $m$ can be\nseen as the number of neighborhood relationships.\n\nThe non-smooth $\\ell_{1,2}$-norm penalty can be approximated by a smooth\nfunction with known gradient computed using Nesterov's smoothing\n[@nesterov2005smooth]. Given a smoothness parameter $\\mu>0$, let us define the\nsmooth approximation \n$$\ns_{\\mu}(\\boldsymbol{\\theta}) = \\max_{\\boldsymbol{\\alpha}\n\\in \\mathcal{K}} \\left \\{ \\boldsymbol{\\alpha}^T \\mathbf{A} \\boldsymbol{\\theta} -\n\\frac{\\mu}{2} \\| \\boldsymbol{\\alpha} \\|_2^2 \\right \\},\n$$\nwhere $\\mathcal{K}$ is\nthe cartesian product of $\\ell_2$-unit balls, $\\mathbf{A}$ is the vertical\nconcatenation of the matrices $\\mathbf{A}_\\phi$ and $\\boldsymbol{\\alpha}$ is an\nauxiliary variable resulting from the dual reformulation of\n$s(\\boldsymbol{\\theta})$. Note that $\\lim_{\\mu \\rightarrow 0}\ns_{\\mu}(\\boldsymbol{\\theta}) = s(\\boldsymbol{\\theta}).$ A Fast Iterative\nShrinkage-Thresholding Algorithm [FISTA, @Beck2009] step can then be applied\nafter computing the gradient of the smooth part i.e. $g(\\boldsymbol{\\theta}) +\n\\lambda_2 s_{\\mu}(\\boldsymbol{\\theta})$ of the approximated criterion.\n\nThe main ingredient of CONESTA remains in the determination of the optimal\nsmoothness parameter using the duality gap, which minimizes the number of FISTA\niterations for a given precision $\\epsilon.$ The specification of $\\mu$ is\nsubject to dynamic update. A sequence of decreasing optimal smoothness\nparameters is generated in order to dynamically adapt the FISTA algorithm\nstepsize towards $\\epsilon.$ Namely, $\\mu^k = \\mu_{opt}(\\epsilon^k).$ The\nsmoothness parameter decreases as one gets closer to $\\boldsymbol{\\theta}\n^\\star$, the solution of the problem defined in @eq-conesta-criterion. Since\n$\\boldsymbol{\\theta} ^\\star$ is unknown; the approximation of the distance to\nthe minimum is achieved via the duality gap. Indeed \n$$\n\\operatorname{GAP}_{\\mu^k}(\\boldsymbol{\\theta}^k) \\ge\nf_{\\mu^k}(\\boldsymbol{\\theta}^k) - f(\\boldsymbol{\\theta}^\\star) \\ge 0.\n$$ \nWe\nrefer the reader to the seminal paper for more details on the formulation of\n$\\operatorname{GAP}_{\\mu^k}(\\boldsymbol{\\theta}^k).$ The CONESTA routine is\nspelled out in the algorithm CONESTA solver where $L(g + \\lambda_2 s_{\\mu})$ is\nthe Lipschitz constant of $\\nabla(g + \\lambda_2 s_{\\mu}),$ $k$ is the iteration\ncounter for the inner FISTA updates and $i$ is the iteration counter for CONESTA\nupdates.\n\n::: {#conesta}\n```pseudocode\n\\begin{algorithm}\n\\caption{CONESTA solver}\n\\begin{algorithmic}\n  \\State \\textbf{Inputs}: \\\\\n    $\\quad$ functions $g(\\boldsymbol{\\theta}), h(\\boldsymbol{\\theta}), s(\\boldsymbol{\\theta})$ \\\\\n    $\\quad$ precision $\\epsilon$ \\\\\n    $\\quad$ penalty parameters $\\lambda_1, \\lambda_2$ \\\\\n    $\\quad$ decreasing factor $\\boldsymbol \\tau \\in (0,1)$ for sequence of precisions\n    \n  \\State \\textbf{Output:} \\\\\n    $\\quad$ $\\boldsymbol{\\theta}^{i+1} \\in \\mathbb{R}^d$\n\n  \\State \\textbf{Initializations:} \\\\\n    $\\quad \\boldsymbol{\\theta}^0 \\in \\mathbb{R}^d$ \\\\\n    $\\quad \\epsilon^0 = \\boldsymbol \\tau \\operatorname{GAP}_{\\mu = 10^{-8}}(\\boldsymbol{\\theta}^0)$ \\\\\n    $\\quad \\mu^0 = \\mu_{opt}(\\epsilon^0)$\n\n  \\Repeat\n    \\State $\\epsilon^i_{\\mu} = \\epsilon^i - \\mu^i \\lambda_2 \\frac{d}{2}$ \\\\\n    \\Comment{FISTA}\n    \\State $k=2$ \\Comment{new iterator}\n    \\State $\\boldsymbol{\\theta}_{\\operatorname{FISTA}}^1 = \\boldsymbol{\\theta}_{\\operatorname{FISTA}}^0 = \\boldsymbol{\\theta}^i$ \\Comment{Initial parameters value}\n    \\State $t_{\\mu} = \\frac{1}{L(g + \\lambda_2 s_{\\mu})}$ \\Comment{Compute stepsize with $L(g + \\lambda_2 s_{\\mu})$ the Lipschitz constant of $\\nabla(g + \\lambda_2 s_{\\mu})$}\n    \n    \\Repeat\n      \\State $\\boldsymbol{z} = \\boldsymbol{\\theta}_{\\operatorname{FISTA}}^{k-1} + \\frac{k-2}{k+1}(\\boldsymbol{\\theta}_{\\operatorname{FISTA}}^{k-1} - \\boldsymbol{\\theta}_{\\operatorname{FISTA}}^{k-2})$\n      \\State $\\boldsymbol{\\theta}_{\\operatorname{FISTA}}^k = \\operatorname{prox}_{\\lambda_1 h}(\\boldsymbol{z} - t_{\\mu} \\nabla(g + \\lambda_2 s_{\\mu})(\\boldsymbol{z}))$\n    \\Until{$\\operatorname{GAP}_{\\mu}(\\boldsymbol{\\theta}_{\\operatorname{FISTA}}^k) \\le \\epsilon_{\\mu}^i$} \n    \n  \\State $\\boldsymbol{\\theta}^{i+1} = \\boldsymbol{\\theta}_{\\operatorname{FISTA}}^k$ \\\\\n  \\State $\\epsilon^i = \\operatorname{GAP}_{\\mu = \\mu_i} \\boldsymbol{\\theta}^{i+1} + \\mu^i \\lambda_2 \\frac{d}{2}$ \\\\\n  \\State $\\epsilon^{i+1} = \\boldsymbol \\tau \\epsilon^{i}$ \\\\\n  \\State $\\mu^{i+1} = \\mu_{opt}(\\epsilon^{i+1})$\n  \\Until{$\\epsilon^i \\le \\epsilon$}\n  \n\\end{algorithmic}\n\\end{algorithm}\n```\n:::\n\n## Reformulation of MGLasso for CONESTA algorithm {#reformulation-of-mglasso-for-conesta-algorithm}\n\nUsing CONESTA for solving the MGLasso problem requires a reformulation in order \nto comply with the form of loss function required by CONESTA. \nThe objective of MGLasso can be written as\n$$ \n\\operatorname{argmin} \\frac{1}{2} ||\\mathbf{Y} - \\tilde{\\mathbf{X}}\n\\tilde{\\boldsymbol{\\beta}}||_2^2 + \\lambda_1 ||\\tilde{\\boldsymbol{\\beta}}||_1 +\n\\lambda_2 \\sum_{i<j} ||\\boldsymbol D_{ij} \\tilde{\\boldsymbol{\\beta}}||_2, \n$$ {#eq-refpbm}\n\nwhere $\\mathbf{Y} = \\operatorname{Vec}(\\mathbf{X}) \\in \\mathbb{R}^{np},\n\\tilde{\\boldsymbol{\\beta}} = \\operatorname{Vec(\\boldsymbol{\\beta})} \\in\n\\mathbb{R}^{p (p-1)}, \\tilde{\\mathbf{X}}$ is a $\\mathbb{R}^{[np]\\times [p \\times\n(p-1)]}$ block-diagonal matrix with $\\mathbf{X}^{\\setminus i}$ on the $i$-th\nblock. The matrix $\\boldsymbol D_{ij}$ is a $(p-1)\\times p(p-1)$ matrix chosen\nso that $\\boldsymbol D_{ij} \\tilde{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta}^i -\n\\boldsymbol \\tau_{ij} \\boldsymbol{\\beta}^j.$\n\nNote that we introduce this notation for simplicity of exposition, but, in\npractice, the sparsity of the matrices $\\boldsymbol D_{ij}$ allows a more\nefficient implementation. Based on reformulation @eq-refpbm, we may apply\nCONESTA to solve the objective of MGLasso for fixed $\\lambda_1$ and $\\lambda_2$.\nThe procedure is applied, for fixed $\\lambda_1$, to a range of decreasing values\nof $\\lambda_2$ to obtain a hierarchical clustering. The corresponding\npseudo-code is given in the following algorithm where $(\\mathbf{X}^i)^{\\dagger}$\ndenotes the pseudo-inverse of $\\mathbf{X}^i$ and $\\epsilon_{fuse}$ the threshold\nfor merging clusters.\n<revision>\nWe note here that problem in @eq-refpbm is of the same form as the optimization problem solved in the paper by @hadjselem2018: as they showed, CONESTA outperforms other optimization approaches such as the alternating direction method of multipliers [ADMM, @Boyd2011], the excessive gap method [EGM, @nesterov2005excessive], the classical FISTA with fixed smoothing and the inexact FISTA [@schmidt2011convergence]. Rather than repeating their experiments, we refer the reader to Section IV of their paper.\n</revision>\n\n::: {#algo-mglasso}\n```pseudocode\n\\begin{algorithm}\n\\caption{MGLasso algorithm}\n\\begin{algorithmic}\n  \\State \\textbf{Inputs}: \\\\\n    $\\quad$ Set of variables $\\mathbf{X} = \\{\\mathbf{X}^1, \\dots, \\mathbf{X}^p \\} \\in \\mathbb R^{n\\times p}$ \\\\\n    $\\quad$ Penalty parameters $\\lambda_1 \\ge 0, {\\lambda_2}_{\\operatorname{initial}} > 0$ \\\\\n    $\\quad$ Increasing factor $\\eta > 1$ for fusion penalties $\\lambda_2$\\\\ \n    $\\quad$ Fusion threshold $\\epsilon_{fuse} \\ge 0$\n  \n  \\State \\textbf{Outputs:} For $\\lambda_1$ fixed and $\\lambda_2$ from $0$ to ${\\lambda_2}_{\\operatorname{initial}} \\times \\eta^{(I)}$ with $I$ the number of iterations: \\\\\n    $\\quad$ Regression vectors $\\boldsymbol{\\beta}(\\lambda_1, \\lambda_2) \\in \\mathbb R^{p \\times (p-1)}$, \\\\\n    $\\quad$ Clusters partition of variables indices in $K$ clusters: $C(\\lambda_1, \\lambda_2)$\n    \n  \\State \\textbf{Initializations:} \\\\\n    $\\quad$ $\\boldsymbol{\\beta}^i = (\\mathbf{X}^i)^{\\dagger}\\mathbf{X}^i$, $\\forall i = 1, \\dots, p$ for warm start in CONESTA solver \\\\\n    $\\quad$ $C = \\left \\{\\{1\\}, \\dots, \\{p\\}\\right \\}$ Initial clusters with one element per cluster. \\\\\n    $\\quad$ Set $\\lambda_2 = 0$ \\\\\n    $\\quad$ Compute $\\boldsymbol{\\beta}$ using CONESTA solver \\\\\n    $\\quad$ Update clusters $C$ with rule described in \\textbf{while} loop.\n  \n  \\State \\textbf{Set:} $\\lambda_2 = {\\lambda_2}_{\\operatorname{initial}}$ \\\\\n  \n  \\Comment{Clustering path}\n  \\While{$\\operatorname{Card}(C) > 1$}\n    \\State Compute $\\boldsymbol{\\beta}$ using CONESTA solver with warm start from previous iteration \\\\\n    \\Comment{Clusters update}\n    \\State Compute pairwises distances $d(i,j)=\\left \\lVert \\boldsymbol{\\beta}^i - \\boldsymbol \\tau_{ij} \\boldsymbol{\\beta}^j \\right \\rVert_2$, $\\forall i,j \\in \\{1, \\dots, p\\}$ \\\\\n    \\State Determine clusters $C_k (k=1, \\dots, K)$ with the rule $(i,j) \\in C_k$ iff. $d(i,j) \\le \\epsilon_{fuse}$\n  \n    \\State $\\lambda_2 = \\lambda_2 \\times \\nu$\n  \\EndWhile\n\\end{algorithmic}\n\\end{algorithm}\n```\n:::\n\n## Model selection {#model-selection}\n\nA crucial question for practical applications is the definition of a rule to\nselect the penalty parameters ($\\lambda_1, \\lambda_2$). This selection problem\noperates at two levels: $\\lambda_1$ controls the sparsity of the graphical\nmodel, and $\\lambda_2$ controls the number of clusters in the optimal clustering\npartition. These two parameters are dealt with separately: the sparsity\nparameter $\\lambda_1$ is chosen via model selection, while the clustering\nparameter $\\lambda_2$ varies across a grid of values in order to obtain graphs\nwith different levels of granularity. The problem of model selection in\ngraphical models is difficult in the high dimensional case where the number of\nsamples is small compared to the number of variables, as classical Akaike\ninformation criterion [AIC, @akaike1998information] and Bayesian information\ncriterion [BIC, @schwarz1978estimating] tend to perform poorly [@Liu2010].\n\nIn this paper, we focus on the StARS stability selection approach proposed by\n@Liu2010 as suggested by some preliminary tests where we compared the Extended\nBIC [EBIC, @foygel2010extended], a model selection criterion calibrated with slope heuristics [@baudry2012slope], the Rotation invariant criterion implemented in the Huge\npackage [@zhao2012huge], the GGMSelect procedure [@giraud2012graph],\ncross-validation [@bien2011sparse] and StARS. The method uses $k$ subsamples of\ndata to estimate the associated graphs for a given range of $\\lambda_1$ values.\nFor each value, a global instability of the graph edges is computed. The optimal\nvalue of $\\lambda_1$ is chosen so as to minimize the instability, as follows.\nLet $\\lambda^{(1)}_1, \\dots, \\lambda_1^{(K)}$ be a grid of sparsity\nregularization parameters, and $S_1, \\dots, S_N$ be the $N$ bootstrap samples\nobtained by sampling the rows of the data set $\\mathbf{X}$. For each\n$k\\in\\{1,\\ldots,K\\}$ and for each $j\\in\\{1,\\ldots, N\\}$, we denote by\n$\\mathcal{A}^{k,j}(\\mathbf{X})$ the adjacency matrix of the estimated graph\nobtained by applying the inference algorithm to $S_n$ with regularization\nparameter $\\lambda_1^{(k)}$. For each possible edge $(s,t)\\in\\{1,\\ldots,p\\}^2$,\nthe probability of edge appearance is estimated empirically by \n$$\n\\hat\n\\theta_{st}^{(k)} = \\frac{1}{N} \\sum_{j=1}^N \\mathcal{A}^{k,j}_{st}.\n$$ \nDefine\n<old> $$\\hat \\xi_{st}(\\Lambda) = 2 \\hat \\theta_{st} (\\Lambda) \\left ( 1 - \\hat\n\\theta_{st} (\\Lambda) \\right )$$ \n\n$$\n\\hat \\xi_{st}(\\lambda_1^{(k)}) = 2 \\hat \\theta_{st}^{(k)}  \\left ( 1 - \\hat\n\\theta_{st}^{(k)} \\right )\n$$ \n\nthe empirical instability of edge $(s,t)$ (that\nis, twice the variance of the Bernoulli indicator of edge $(s,t)$). The\ninstability level associated with $\\lambda_1^{(k)}$ is given by\n$$\n\\hat D(\\lambda_1^{(k)}) = \\frac{\\sum_{s<t} \\hat \\xi_{st}(\\lambda_1^{(k)})}{\n\\binom{p}{2}}.\n$$\nStARS selects the optimal penalty parameter as follows\n$$\n\\hat \\lambda = \\max_k\\left\\{ \\lambda_1^{(k)}: \\hat D(\\lambda_1^{(k)}) \\le\n\\upsilon, k\\in\\{1,\\ldots,K\\} \\right \\},\n$$\nwhere $\\upsilon$ is the threshold chosen for the instability level.\n\n# Simulation experiments {#simulation-experiments}\n\nIn this Section, we conduct a simulation study to evaluate the performance of\nthe MGLasso method, both in terms of clustering and support recovery. Receiver\nOperating Characteristic (ROC) curves are used to evaluate the adequacy of the\ninferred graphs with the  ground truth  for the MGLasso and GLasso in its\nneighborhood selection version in the Erdös-Rényi [@erdHos1960evolution], Scale-free [@newman2001random], and Stochastic\nBlock Models [SBM, @fienberg1981categorical] frameworks. The Adjusted Rand indices are used to compare the\npartitions obtained with MGLasso, hierarchical agglomerative clustering, and\nK-means clustering in a stochastic block model framework.\n\n## Synthetic data models\n\nWe consider three different synthetic network models: the Stochastic Block Model\n [@fienberg1981categorical], the Erdös-Renyi model [@erdHos1960evolution]\nand the Scale-Free model [@newman2001random]. In each case, Gaussian data is\ngenerated by drawing $n$ independent realizations of a multivariate Gaussian\ndistribution $\\mathcal N(0, \\mathbf{\\Sigma})$ where $\\mathbf{\\Sigma} \\in\n\\mathbb{R}^{p \\times p}$ and $\\mathbf{\\Omega} = \\mathbf{\\Sigma} ^{-1}$. The\nsupport of $\\mathbf{\\Omega}$, equivalent to the network adjacency matrix, is\ngenerated from the three different models. The difficulty level of the problem\nis controlled by varying the ratio $\\frac{n}{p}$ with $p$ fixed at $40$:\n$\\frac{n}{p}\\in \\{0.5,1,2\\}$.\n\n### Stochastic Block Model\n\nWe construct a block-diagonal precision matrix $\\mathbf{\\Omega}$ as follows.\nFirst, we generate the support of $\\mathbf{\\Omega}$ as shown in\n@fig-model-sbm, denoted by $\\boldsymbol A\\in\\{0,1\\}^{p\\times p}$. To do this,\nthe variables are first partitioned into $K = 5$ hidden groups, noted $C_1,\n\\dots, C_K$ described by a latent random variable $Z_i$, such that $Z_i = k$ if\n$i = C_k$. $Z_i$ follows a multinomial distribution\n$$\nP(Z_i = k) = \\pi_k, \\quad\n\\forall k \\in \\{1, \\dots, K\\},\n$$ \n\nwhere $\\pi = (\\pi_1, \\dots, \\pi_k)$ is the\nvector of proportions of clusters whose sum is equal to one. The set of latent\nvariables is noted $\\mathbf{Z} = \\{ Z_1, \\dots, Z_K\\}$. Conditionally to\n$\\mathbf{Z}$, $A_{ij}$ follows a Bernoulli distribution such that\n$$\nA_{ij}|Z_i =\nk, Z_j = l \\sim \\mathcal{B}(\\alpha_{kl}), \\quad \\forall k,l \\in \\{1, \\dots,\nK\\},\n$$ \n\nwhere $\\alpha_{kl}$ is the probability of inter-cluster connectivity,\nwith $\\alpha_{kl} = 0.01$ if $k\\neq l$ and $\\alpha_{ll} = 0,75$. For\n$k\\in\\{1,\\ldots, K\\}$, we define $p_k = \\sum_{i=1}^p \\boldsymbol{1}_{\\{Z_i =\nk\\}}$. The precision matrix $\\mathbf{\\Omega}$ of the graph is then calculated as\nfollows. We define $\\Omega_{ij} = 0$ if $Z_i\\neq Z_j$ ; otherwise, we define\n$\\Omega_{ij} = A_{ij}\\omega_{ij}$ where, for all $i\\in\\{1,\\ldots,p\\}$ and for\nall $j\\in\\{1,\\ldots,p| Z_j = Z_i\\}$, $\\omega_{ij}$ is given by :\n$$\n\\begin{aligned}\n&\\omega_{ii} := \\frac{1+\\rho(p_{Z_i}-2)}{1+\\rho(p_{Z_i}-2)-\\rho^2(p_{Z_i}-1)};\\\\\n&\\omega_{ij} := \\frac{-\\rho}{1+\\rho(p_{Z_i}-2)-\\rho^2(p_{Z_i}-1)}.\n\\end{aligned}\n$$\nIf $\\alpha_{ll}$ were to be equal to one, this construction of $\\mathbf{\\Omega}$\nwould make it possible to control the level of correlation between the variables\nin each block to $\\rho$. Introducing a more realistic scheme with\n$\\alpha_{ll}=0.75$ allows only to have an approximate control.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mglasso)\nset.seed(2020)\nsim_sbm <- sim_data(\n  p = 40,\n  structure = \"block_diagonal\",\n  alpha = rep(1 / 5, 5),\n  prob_mat = diag(0.75, 5),\n  rho = 0.2,\n  inter_cluster_edge_prob = 0.01\n)\ngsbm <- adj_mat(sim_sbm$graph)\nMatrix::image(\n  as(gsbm, \"sparseMatrix\"),\n  sub = \"\",\n  xlab = \"\",\n  ylab = \"\"\n)   \n```\n\n::: {.cell-output-display}\n![Adjacency matrix of a stochastic block model defined by $K=5$ classes with identical prior probabilities set to $\\pi = 1/K$, inter-classes connection probability $\\alpha_{kl}=0.01, k \\neq l$, intra-classes connection probability $\\alpha_{ll}=0.75$ and $p=40$ vertices.](published-202306-sanou-multiscale_glasso_files/figure-html/fig-model-sbm-1.svg){#fig-model-sbm}\n:::\n:::\n\n\n### Erdös-Renyi Model\n\nThe Erdös-Renyi model is a special case of the stochastic block model\nwhere $\\alpha_{kl} = \\alpha_{ll} = \\alpha$ is constant. We set the\ndensity $\\alpha$ of the graph to $0.1$; see @fig-model-erdos for an\nexample of the graph resulting from this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\nsim_erdos <- sim_data(p = 40, structure = \"erdos\", p_erdos = 0.1)\ngerdos <- adj_mat(sim_erdos$graph)\nMatrix::image(as(gerdos, \"sparseMatrix\"), sub = \"\", xlab = \"\", ylab = \"\")\n```\n\n::: {.cell-output-display}\n![Adjacency matrix of an Erdös-Renyi model with probability of connection $\\alpha = 0.1$ and $p=40$ vertices.](published-202306-sanou-multiscale_glasso_files/figure-html/fig-model-erdos-1.svg){#fig-model-erdos}\n:::\n:::\n\n\n### Scale-free Model\n\nThe Scale-free Model generates networks whose degree distributions\nfollow a power law. The graph starts with an initial chain graph of $2$\nnodes. Then, new nodes are added to the graph one by one. Each new node\nis connected to an existing node with a probability proportional to the\ndegree of the existing node. We set the number of edges in the graph to\n$40$. An example of scale-free graph is shown in @fig-model-sfree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\nsim_sfree <- sim_data(p = 40, structure = \"scale_free\")\ngsfree <- adj_mat(sim_sfree$graph)\nMatrix::image(as(gsfree, \"sparseMatrix\"), sub = \"\", xlab = \"\", ylab = \"\")\n```\n\n::: {.cell-output-display}\n![Adjacency matrix of a Scale-free model with $40$ edges and $p=40$ nodes.](published-202306-sanou-multiscale_glasso_files/figure-html/fig-model-sfree-1.svg){#fig-model-sfree}\n:::\n:::\n\n\n## Support recovery\n\nWe compare the network structure learning performance of our approach to\nthat of GLasso in its neighborhood selection version using ROC curves.\nIn both GLasso and MGLasso, the sparsity is controlled by a\nregularization parameter $\\lambda_1$; however, MGLasso admits an\nadditional regularization parameter, $\\lambda_2$, which controls the\nstrength of convex clustering. To compare the two methods, in each ROC\ncurve, we vary the parameter $\\lambda_1$ while the parameter $\\lambda_2$\n(for MGLasso) is kept constant. We computed ROC curves for $4$ different\npenalty levels for the $\\lambda_2$ parameter; since GLasso does not\ndepend on $\\lambda_2$, the GLasso ROC curves are replicated.\n\nIn a decision rule associated with a sparsity penalty level $\\lambda_1$, we recall the definition of the two following functions. The true positive rate is given by $\\frac{TP(\\lambda_1)}{TP(\\lambda_1) + FN(\\lambda_1)}.$ The false positive rate is defined as follows $1 - \\frac{TN(\\lambda_1)}{TN(\\lambda_1) + FP(\\lambda_1)}$, where $TP$ is the number of true positives, $TN$ the number of true negatives, $FN$ the number of false negatives and $FP$ the number of false positives. The ROC curve represents the true positive rate as a function of the false positive rate. For a given level of true positive rate, the best method minimizes the false positive rate.\n\nFor each configuration ($n, p$ fixed), we generate $50$ replications and\ntheir associated ROC curves, which are then averaged. The average ROC\ncurves for the three models are given in @fig-roc-erdos, @fig-roc-sfree\nand @fig-roc-sbm by varying $\\frac{n}{p}\\in \\{0.5,1,2\\}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(ghibli)\nload(\"./data/roc_dtf_erdos.RData\")\nnp.labs <- c(\"frac(n, p) == 0.5\", \"frac(n, p) == 1\", \"frac(n, p) == 2\")\nnames(np.labs) <- c(\"0.5\", \"1\", \"2\")\ntv.labs <- c(\"lambda[2] == 0\", \"lambda[2] == 3.33\", \"lambda[2] == 10\")\nnames(tv.labs) <- c(\"0\", \"3.33\", \"10\")\nroc_dtf_erdos <- dplyr::filter(roc_dtf_erdos, tv != 6.67)\nggplot(roc_dtf_erdos, aes(\n  x     = 100 * fpr,\n  y     = 100 * tpr,\n  color = method\n)) +\n  geom_line(linewidth = 0.7) +\n  facet_grid(np ~ tv, labeller = labeller(\n    np = as_labeller(np.labs, label_parsed),\n    tv = as_labeller(tv.labs, label_parsed)\n  )) +\n  geom_abline(\n    intercept = 0,\n    slope = 1,\n    linetype = \"dashed\",\n    color = \"grey\"\n  ) +\n  xlab(\"False Positive Rate\") +\n  ylab(\"True Positive Rate\") +\n  ggtitle(\"\") +\n  scale_colour_manual(\n    name = \"Method\",\n    labels = c(\"GLasso\", \"MGLasso\"),\n    values = ghibli::ghibli_palette(\"MarnieMedium1\")[5:6]\n  ) +\n  theme(text = element_text(size = 14))\n```\n\n::: {.cell-output-display}\n![Mean ROC curves for MGLasso and GLasso graph inference in the Erdös-Renyi model. We varied the fusion penalty parameter of MGLasso $\\lambda_2 \\in \\{0, 3.33, 10\\}$ alongside the ratio $\\frac{n}{p}\\in \\{0.5,1,2\\}$. Within each panel, the ROC curve shows the True positive rate (y-axis) vs. the False positive rate (x-axis) for both MGLasso (blue) and GLasso (brown). Since GLasso does not have a fusion penalty, its ROC curves were replicated for panels belonging to the same row. We also plot the random classifier (dotted grey line). The results have been averaged over $50$ simulated datasets and suggest that MGLasso performs no worse than GLasso. For $\\lambda_2 = 0$, the MGLasso approach is equivalent to GLasso in its neighborhood selection version.](published-202306-sanou-multiscale_glasso_files/figure-html/fig-roc-erdos-1.svg){#fig-roc-erdos}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"./data/roc_dtf_sfree.RData\")\nnp.labs <- c(\"frac(n, p) == 0.5\", \"frac(n, p) == 1\", \"frac(n, p) == 2\")\nnames(np.labs) <- c(\"0.5\", \"1\", \"2\")\ntv.labs <- c(\"lambda[2] == 0\", \"lambda[2] == 3.33\", \"lambda[2] == 10\")\nnames(tv.labs) <- c(\"0\", \"3.33\", \"10\")\nroc_dtf_sfree <- dplyr::filter(roc_dtf_sfree, tv != 6.67)\nggplot(roc_dtf_sfree, aes(\n  x     = 100 * fpr,\n  y     = 100 * tpr,\n  color = method\n)) +\n  geom_line() +\n  facet_grid(np ~ tv, labeller = labeller(\n    np = as_labeller(np.labs, label_parsed),\n    tv = as_labeller(tv.labs, label_parsed)\n  )) +\n  geom_abline(\n    intercept = 0,\n    slope = 1,\n    linetype = \"dashed\",\n    color = \"grey\"\n  ) +\n  xlab(\"False Positive Rate\") +\n  ylab(\"True Positive Rate\") +\n  ggtitle(\"\") +\n  scale_colour_manual(\n    name = \"Method\",\n    labels = c(\"GLasso\", \"MGLasso\"),\n    values = ghibli_palette(\"MarnieMedium1\")[5:6]\n  ) +\n  theme(text = element_text(size = 14))\n```\n\n::: {.cell-output-display}\n![Mean ROC curves for MGLasso and GLasso graph inference in the Scale-free model. We varied the fusion penalty parameter of MGLasso $\\lambda_2 \\in \\{0, 3.33, 10\\}$ alongside the ratio $\\frac{n}{p}\\in \\{0.5,1,2\\}$. Within each panel, the ROC curve shows the True positive rate (y-axis) vs. the False positive rate (x-axis) for both MGLasso (blue) and GLasso (brown). Since GLasso does not have a fusion penalty, its ROC curves were replicated for panels belonging to the same row. We also plot the random classifier (dotted grey line). The results have been averaged over $50$ simulated datasets and suggest that MGLasso performs no worse than GLasso. For $\\lambda_2 = 0$, the MGLasso approach is equivalent to Glasso in its neighborhood selection version.](published-202306-sanou-multiscale_glasso_files/figure-html/fig-roc-sfree-1.svg){#fig-roc-sfree}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"./data/roc_dtf_sbm.RData\")\nnp.labs <- c(\"frac(n, p) == 0.5\", \"frac(n, p) == 1\", \"frac(n, p) == 2\")\nnames(np.labs) <- c(\"0.5\", \"1\", \"2\")\ntv.labs <- c(\"lambda[2] == 0\", \"lambda[2] == 3.33\", \"lambda[2] == 10\")\nnames(tv.labs) <- c(\"0\", \"3.33\", \"10\")\nroc_dtf_sbm <- dplyr::filter(roc_dtf_sbm, tv != 6.67)\nggplot(roc_dtf_sbm, aes(\n  x     = 100 * fpr,\n  y     = 100 * tpr,\n  color = method\n)) +\n  geom_line() +\n  facet_grid(np ~ tv, labeller = labeller(\n    np = as_labeller(np.labs, label_parsed),\n    tv = as_labeller(tv.labs, label_parsed)\n  )) +\n  geom_abline(\n    intercept = 0,\n    slope = 1,\n    linetype = \"dashed\",\n    color = \"grey\"\n  ) +\n  xlab(\"False Positive Rate\") +\n  ylab(\"True Positive Rate\") +\n  ggtitle(\"\") +\n  scale_colour_manual(\n    name = \"Method\",\n    labels = c(\"GLasso\", \"MGLasso\"),\n    values = ghibli_palette(\"MarnieMedium1\")[5:6]\n  ) +\n  theme(text = element_text(size = 14))\n```\n\n::: {.cell-output-display}\n![Mean ROC curves for MGLasso and GLasso graph inference in the stochastic block model. We varied the fusion penalty parameter of MGLasso $\\lambda_2 \\in \\{0, 3.33, 10\\}$ alongside the ratio $\\frac{n}{p}\\in \\{0.5,1,2\\}$. Within each panel, the ROC curve shows the True positive rate (y-axis) vs. the False positive rate (x-axis) for both MGLasso (blue) and GLasso (brown). Since GLasso does not have a fusion penalty, its ROC curves were replicated for panels belonging to the same row. We also plot the random classifier (dotted grey line). The results have been averaged over $50$ simulated datasets and suggest that MGLasso performs no worse than GLasso. For $\\lambda_2 = 0$, the MGLasso approach is equivalent to Glasso in its neighborhood selection version.](published-202306-sanou-multiscale_glasso_files/figure-html/fig-roc-sbm-1.svg){#fig-roc-sbm}\n:::\n:::\n\n\nBased on these empirical results, we first observe that, in all the considered\nsimulation models, MGLasso improves over GLasso in terms of support recovery in\nthe high-dimensional setting where $p<n$. In addition, in the absence of\na fusion penalty, i.e., $\\lambda_2 = 0$, MGLasso performs no worse than GLasso in\neach of the $3$ models. However, for $\\lambda_2>0$, increasing penalty value\ndoes not seem to significantly improve the support recovery performances for the\nMGLasso, as we observe similar results for $\\lambda_2=3.3,10$. Preliminary\nanalyses show that, as $\\lambda_2$ increases, the estimates of the regression\nvectors are shrunk towards $0$. This shrinkage effect of group-fused penalty\nterms was also observed in [@chu2021adaptive]. Note that the performance of the\nMGLasso deteriorates comparatively to GLasso when the inter-clusters edge\nconnection probability of the stochastic block model is high.\n\n## Clustering\n\nIn order to study clustering performance, we compared the partitions estimated\nby MGLasso, Hierarchical Agglomerative Clustering (HAC) with Ward's distance and\nK-means to the true partition in a stochastic block model framework. Euclidean\ndistances between variables are used for HAC and K-means. The criterion used for\nthe comparison is the adjusted Rand index (ARI). We studied the influence of the\ncorrelation level inside clusters on the clustering performances through two\ndifferent parameters: $\\rho \\in \\{ 0.1, 0.3 \\}$; the vector of cluster\nproportions is fixed at $\\mathbf \\pi = (1/5, \\dots, 1/5)$. Hundred Gaussian data sets were then simulated for each  configuration ($\\rho$, $n/p$\nfixed).The optimal sparsity penalty for MGLasso was chosen by the Stability\nApproach to Regularization Selection (StARS) method [@Liu2010]. In practice, we\nestimated a stability-like parameter in a sample of graphs simulated via the\nstochastic block model. This estimation of edge variability was then used as the\nthreshold for the StARS method. The parameter $\\lambda_2$ has been varied. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"./data/rand_dt_lower_cor_sbm.RData\")\nplot_res(\n  dt_rand,\n  crit_ = \"rand\",\n  ncluster_ = c(5, 10, 15, 20),\n  cor_ = 0.25,\n  np_ = c(0.5, 1, 2),\n  main = \"\"\n) +\n  theme(text = element_text(size = 14))\n```\n\n::: {.cell-output-display}\n![Boxplots of Adjusted Rand Indices for the stochastic block model with $5$ classes and $p=40$ variables for a correlation level $\\rho=0.1$. The number of estimated clusters $\\{5,10,15,20\\}$ vary alongside the ratio $\\frac{n}{p}\\in \\{0.5,1,2\\}$. Within each panel, the boxplots of ARI between true partition (with $5$ classes) and estimated clustering partitions on $100$ simulated datasets for $k$-means (blue), hierarchical agglomerative clustering (yellow), and MGLasso (brown) methods are plotted against the ratio $\\frac{n}{p}.$  The cluster assignments of MGLasso are computed from a distance between estimated regression vectors for a given value of $\\lambda_2.$ Missing boxplots for MGLasso thus mean computed partitions in the grid of values of $\\lambda_2$ do not yield the fixed number of clusters. The higher the ARI values, the better the estimated clustering partition is.](published-202306-sanou-multiscale_glasso_files/figure-html/fig-ari-low-cor-1.svg){#fig-ari-low-cor}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"./data/rand_dt_higher_cor_sbm.RData\")\nplot_res(\n  dt_rand,\n  crit_ = \"rand\",\n  ncluster_ = c(5, 10, 15, 20),\n  cor_ = 0.95,\n  np_ = c(0.5, 1, 2),\n  main = \"\"\n) +\n  theme(text = element_text(size = 14))\n```\n\n::: {.cell-output-display}\n![Boxplots of Adjusted Rand Indices for the stochastic block model with $5$ classes and $p=40$ variables for a correlation level $\\rho=0.3$. The number of estimated clusters $\\{5,10,15,20\\}$ vary alongside the ratio $\\frac{n}{p}\\in \\{0.5,1,2\\}$. Within each panel, the boxplots of ARI between true partition (with $5$ classes) and estimated clustering partitions on $100$ simulated datasets for $k$-means (blue), hierarchical agglomerative clustering (yellow), and MGLasso (brown) methods are plotted against the ratio $\\frac{n}{p}.$  The cluster assignments of MGLasso are computed from a distance between estimated regression vectors for a given value of $\\lambda_2.$ The higher the ARI values, the better the estimated clustering partition is.](published-202306-sanou-multiscale_glasso_files/figure-html/fig-ari-high-cor-1.svg){#fig-ari-high-cor}\n:::\n:::\n\n\nThe expected empirical evidence that MGLasso would work reasonably well for strongly correlated variables is somehow highlighted in @fig-ari-low-cor and @fig-ari-high-cor. The performances of MGLasso slightly improve when going from @fig-ari-low-cor to @fig-ari-high-cor, which corresponds to correlation levels of 0.1 and 0.3 between variables belonging to the same block, respectively. We observe the same trend for the HAC and the k-means. Compared to these two approaches, the MGLasso presents the lowest values of adjusted Rand indices, thus suggesting a lower quality of clustering. It should be noted that the performance of MGLasso can be sensitive to the selection of the Lasso penalty parameter and the threshold fixed to determine clusters' fusion. In practice, this fusion threshold is varied in a grid of values close to zero and lower than $10^{-3}$. The value leading to the maximum number of intermediate clusters in the clustering path is chosen. Using non-trivial weights could also improve the overall performance of MGLasso.\n\n<revision> \nDuring the revision of this paper, an interesting question was raised regarding the behavior of the algorithm in a phylogenetic-based model. To investigate this, extensive numerical experiments were conducted on a phylogenetic-based model that evaluates only clustering performances. The results showed that the MGLASSO algorithm's performance improves, and the method performs as well as some state-of-the-art clustering approaches, including vanilla convex clustering and spectral clustering. In phylogenetic-based models, adjusted Rand indices can be computed between the estimated partition with $k$ clusters and the true partition in $k$ clusters computed from the tree used for the simulation procedure. This differs from the clustering performance evaluation scheme applied in the stochastic block model, where the true partition is considered fixed.\n</revision>\n\n# Applications \nTo illustrate the proposed simultaneous graphs and clusters inference approach,\nwe present analyses where the MGLasso model is applied to microbial association\ndata for the study of multiscale networks between operational taxonomic units\nand to transcriptomic and methylation genotypes for multi-omics data\nintegration.\n\n## Application to microbial associations in gut data\n\nWe analyze microbial associations in human gut microbiome data acquired from the\nround $1$ of the American Gut Project (AGP, @mcdonald2018american) for $p = 127$\noperational taxonomic units (OTUs) and $n = 289$ individuals samples. The count\nof microbial OTUs is an indicator of the abundance of underlying microbial\npopulations. Here, we investigate the network and clustering structures of the\nOTUs for different levels of granularity on the processed data included in the\nSpiecEasi R package (see @Kurtz2015 for details). The data is first normalized\nto have a unit-sum per sample and to remove biases. Then, a centered log-ratio\n[clr, @aitchison1982statistical] transformation with an added unit pseudo-count\nis applied to come back to an unconstrained Euclidean space. For fitting the\nMGLasso model, we select the Lasso penalty parameter $\\lambda_1$ via the StARS\napproach with threshold $\\upsilon = 0.05$ and vary the fusion penalty\n$\\lambda_2$ in the interval $[0, 20]$ with irregular steps. The CPU time taken\nfor $20$ values of $\\lambda_2$ is about $8$ hours with parallel evaluations on a\ncomputation cluster with as many cores as $\\lambda_2$ values. The maximal number\nof iterations is set to $10000$ and the solver precision to $0.01$.\n\n<old>We finally illustrate our new method of inferring the multiscale\nGaussian graphical model, with an application to the analysis of\nmicrobial associations in the American Gut Project. The data used are\ncount data that have been previously normalized by applying the\nlog-centered ratio technique as used in [@Kurtz2015]. After some\nfiltering steps [@Kurtz2015] on the operational taxonomic units (OTUs)\ncounts (removed if present in less than $37\\%$ of the samples) and the\nsamples (removed if sequencing depth below 2700), the top OTUs are\ngrouped in a dataset composed of $n = 289$ for $127$ OTUs.\n<old>\nAs a preliminary analysis, we perform a hierarchical agglomerative clustering\n(HAC) on the OTUs, which allows us to identify four significant groups.\nThe correlation matrix of the dataset is given in fig-emp-cor;\nvariables have been rearranged according to the HAC partition.\n\nUsing these settings, we compute a clustering path of the solutions and\nestimated graphs for $5$ values of $\\lambda_2$ corresponding to $5$ different\nclusters partitions. The @fig-clusterpath shows how the predicted\n$\\hat{\\boldsymbol X}$ evolves through $\\lambda_2.$ The $\\hat{\\boldsymbol X}$ are\ncomputed from estimated centroids $\\hat{\\boldsymbol \\beta}$ and projected onto\ntwo principal components of the original data. The path is not always\nagglomerative, but the clusters' splits observed ensure optimal solutions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SpiecEasi)\nlibrary(colorspace)\nlibrary(ggrepel)\n\npath_data <- \"./data/\"\nload(paste0(path_data, \"mgl_amgut_rev_l2_seq0to1_20val.RData\"))\nload(paste0(path_data, \"mgl_amgut_rev_l2_seq1to20_20val.RData\"))\nload(paste0(path_data, \"mgl_amgut_rev_l2_seq0to4_20val.RData\"))\nload(paste0(path_data, \"amgut1.filt.phy.rda\")) # Data for the phylum taxonomic classifier loaded from supplementary files of the SpiecEasi package. See https://github.com/zdk123/SpiecEasi/blob/master/data/amgut2.filt.phy.rda\nload(paste0(path_data, \"amgut1.filt.rda\"))\namgut1.filt <- t(clr(amgut1.filt + 1 , 1))\n\ntaxas <- amgut1.filt.phy@tax_table@.Data\nrank2_table <- table(taxas[,\"Rank2\"])\ncol_leaves <- as.factor(rep(rainbow_hcl(6, c=90, l=50), times = rank2_table))\n\nplot_clusterpath <- function(X, mglasso_res, colnames_ = NULL, max.overlaps, cut_k_vars = 5, colors_) {\n  ## Initialisations\n  p <- ncol(X)\n  df.paths <- data.frame(x=c(),y=c(), group=c())\n  nlevel <- length(mglasso_res)\n\n  ## Principal component analysis\n  svdX <- svd(X)                ## singular value decomposition\n  pc <- svdX$u[,3:4,drop=FALSE] ## singular vectors\n\n  for (j in cut_k_vars:nlevel) {\n    Beta <- mglasso_res[[j]]$selected_Theta\n    Xpred <- sapply(1:p, function(i){X %*% Beta[i,]})\n    pcs <- t(pc)%*%Xpred\n    x <- pcs[1,]\n    y <- pcs[2,]\n    df <- data.frame(x=pcs[1,], y=pcs[2,], group=1:p, Rank2 = colors_)\n    df.paths <- rbind(df.paths,df)\n  }\n\n  # X_data <- as.data.frame(t(X) %*% pc) ## PCA projections (scores)\n  X_data <- df.paths[1:p,]\n  #colnames(X_data) <- c(\"x\", \"y\")\n  ifelse(is.null(colnames_),\n         X_data$Name <- colnames(X),\n         X_data$Name <- colnames_)\n  data_plot <- ggplot(data = df.paths, aes(x = x, y = y))\n  data_plot <-\n    data_plot + geom_path(aes(group = group,  colour = Rank2), alpha = 0.5)\n  data_plot <-\n    data_plot + geom_text_repel(data = X_data,\n                                aes(x = x, y = y, label = Name),\n                                max.overlaps = max.overlaps)\n  data_plot <-\n    data_plot + geom_point(data = X_data, aes(x = x, y = y, colour = Rank2), size = 1.5)\n  data_plot <-\n    data_plot + xlab('Principal Component 3') + ylab('Principal Component 4')\n  data_plot + theme_bw()\n}\n\nplot_clusterpath(amgut1.filt, c(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3), max.overlaps = 10, cut_k_vars = 1, colors_ = taxas[,\"Rank2\"])\n```\n\n::: {.cell-output-display}\n![Clustering path of the MGLasso convex clustering solutions on microbiome data with $127$ OTUs. The predicted data are projected onto the two principal components of the original data, while the fusion penalty varies. As $\\lambda_2$ increases, it reaches a value for which all the estimated centroids are equal; thus, the branches of the path converge to a unique point in the center of the graph. OTUs are colored according to their phylum classification. The path displays abrupt merges. The pure cluster on the graph's left side (down) corresponds to the phylum Bacteroidetes.](published-202306-sanou-multiscale_glasso_files/figure-html/fig-clusterpath-1.svg){#fig-clusterpath}\n:::\n:::\n\n\nThe @fig-meta-graphs displays graphs and clusters for different levels of\ngranularity: $127$, $63$, $31$, $15$ and $2$ clusters. For computing the\nclusters' assignment of nodes, the fusion threshold has been set to\n$\\epsilon_{fuse} = 0.001$. Variables that belong to the same cluster share the\nsame neighborhood; thus, the neighboring information is summarized into a single\nvariable representative of the group. The subfigures show graphs at multiple\nlevels of granularity which are built on the meta-variables or representative\nvariables.\n\n\n::: {#fig-meta-graphs .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nlibrary(igraph)\nlibrary(phyloseq)\n\nall_clusters_partition <-\n  lapply(c(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3), function(x)\n    get_clusters_mgl(x$selected_Theta))\nall_num_clusters <-\n  unlist(lapply(all_clusters_partition, function(x)\n    length(unique(x))))\n\nind <- which(all_num_clusters == 127)[1]\nclusters <- as.character(all_clusters_partition[[ind]])\nvec <- extract_meta(clusters = all_clusters_partition[[ind]])\nmetaG <-\n  c(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]$selected_Theta[vec, vec]\n\nmetaG <- symmetrize(metaG)\nmetaG <-\n  adj2igraph(metaG, vertex.attr = list(name = taxa_names(amgut1.filt.phy)[vec]))\nE(metaG)$weight <- abs(E(metaG)$weight)\ntaxas <- amgut1.filt.phy@tax_table@.Data\ntaxas <- cbind(clusters, taxas)\ntaxas <- taxas[vec, ]\nplot_network(\n  metaG,\n  taxas,\n  type = \"taxa\",\n  layout.method = layout_with_fr,\n  color = \"Rank2\"\n)\n\nind <- which(all_num_clusters == 63)[1]\nclusters <- as.character(all_clusters_partition[[ind]])\nvec <- extract_meta(clusters = all_clusters_partition[[ind]])\nmetaG <-\n  c(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]$selected_Theta[vec, vec]\n\nmetaG <- symmetrize(metaG)\nmetaG <-\n  adj2igraph(metaG, vertex.attr = list(name = taxa_names(amgut1.filt.phy)[vec]))\nE(metaG)$weight <- abs(E(metaG)$weight)\ntaxas <- amgut1.filt.phy@tax_table@.Data\ntaxas <- cbind(clusters, taxas)\ntaxas <- taxas[vec, ]\nplot_network(\n  metaG,\n  taxas,\n  type = \"taxa\",\n  layout.method = layout_with_fr,\n  color = \"Rank2\"\n)\n\nind <- which(all_num_clusters == 31)[1]\nclusters <- as.character(all_clusters_partition[[ind]])\nvec <- extract_meta(clusters = all_clusters_partition[[ind]])\nmetaG <-\n  c(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]$selected_Theta[vec, vec]\n\nmetaG <- symmetrize(metaG)\nmetaG <-\n  adj2igraph(metaG, vertex.attr = list(name = taxa_names(amgut1.filt.phy)[vec]))\nE(metaG)$weight <- abs(E(metaG)$weight)\ntaxas <- amgut1.filt.phy@tax_table@.Data\ntaxas <- cbind(clusters, taxas)\ntaxas <- taxas[vec, ]\nplot_network(\n  metaG,\n  taxas,\n  type = \"taxa\",\n  layout.method = layout_with_dh,\n  color = \"Rank2\"\n)\n\nind <- which(all_num_clusters == 15)[1]\nclusters <- as.character(all_clusters_partition[[ind]])\nvec <- extract_meta(clusters = all_clusters_partition[[ind]])\nmetaG <-\n  c(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]$selected_Theta[vec, vec]\n\nmetaG <- symmetrize(metaG)\nmetaG <-\n  adj2igraph(metaG, vertex.attr = list(name = taxa_names(amgut1.filt.phy)[vec]))\nE(metaG)$weight <- abs(E(metaG)$weight)\ntaxas <- amgut1.filt.phy@tax_table@.Data\ntaxas <- cbind(clusters, taxas)\ntaxas <- taxas[vec, ]\nplot_network(\n  metaG,\n  taxas,\n  type = \"taxa\",\n  layout.method = layout_with_dh,\n  color = \"Rank2\"\n)\n\nind <- which(all_num_clusters == 2)[1]\nclusters <- as.character(all_clusters_partition[[ind]])\nvec <- extract_meta(clusters = all_clusters_partition[[ind]])\nmetaG <-\n  c(mgl_amgut_rev, mgl_amgut_rev_set2, mgl_amgut_rev_set3)[[ind]]$selected_Theta[vec, vec]\n\nmetaG <- symmetrize(metaG)\nmetaG <-\n  adj2igraph(metaG, vertex.attr = list(name = taxa_names(amgut1.filt.phy)[vec]))\nE(metaG)$weight <- abs(E(metaG)$weight)\ntaxas <- amgut1.filt.phy@tax_table@.Data\ntaxas <- cbind(clusters, taxas)\ntaxas <- taxas[vec, ]\nplot_network(\n  metaG,\n  taxas,\n  type = \"taxa\",\n  layout.method = layout_with_dh,\n  color = \"Rank2\"\n)\n```\n\n::: {.cell-output-display}\n![127 clusters graph](published-202306-sanou-multiscale_glasso_files/figure-html/fig-meta-graphs-1.svg){#fig-meta-graphs-1}\n:::\n\n::: {.cell-output-display}\n![Meta-variables graph with 63 clusters](published-202306-sanou-multiscale_glasso_files/figure-html/fig-meta-graphs-2.svg){#fig-meta-graphs-2}\n:::\n\n::: {.cell-output-display}\n![Meta-variables graph with 31 clusters](published-202306-sanou-multiscale_glasso_files/figure-html/fig-meta-graphs-3.svg){#fig-meta-graphs-3}\n:::\n\n::: {.cell-output-display}\n![Meta-variables graph with 15 clusters](published-202306-sanou-multiscale_glasso_files/figure-html/fig-meta-graphs-4.svg){#fig-meta-graphs-4}\n:::\n\n::: {.cell-output-display}\n![Meta-variables graph with 2 clusters](published-202306-sanou-multiscale_glasso_files/figure-html/fig-meta-graphs-5.svg){#fig-meta-graphs-5}\n:::\n\nEstimated graphs at multiple levels of granularity. The first graph shows a network inferred when $\\lambda_2=0$.The number of clusters is equal to the number of OTUs. Increasing the fusion penalty makes it possible to uncover graphs built on the representative variable of each cluster. OTUs are colored according to their phylum taxonomic classifier. The number of clusters is computed from the regression vectors with a fixed fusion threshold.\n:::\n\n\nTo assess the relevance of the inferred clusters, they are compared to known\ntaxonomic ranks (phylum, class, order, family, genera, or species). The phylum\nclassification is used. For example, for a clustering partition in $2$ groups,\nthe MGLasso clustering partition is composed of $120$ variables versus $7$\nvariables. The cluster $2$ is exclusively composed of OTUs\nbelonging to the Proteobacteria phylum. The cluster $1$ also contains\nProteobacteria OTUs, so those identified in cluster $2$ might share more\nintimate characteristics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nind <- which(all_num_clusters == 2)[1]\nclusters <- as.character(all_clusters_partition[[ind]])\ntaxas <- amgut1.filt.phy@tax_table@.Data\ntaxonomic.classification <- taxas[,\"Rank2\"]\n\n## remove \"p__\" characters in species names\ntaxonomic.classification <- sub(\"p__\", \"\", taxonomic.classification)\n\ntables::as.tabular(table(clusters, taxonomic.classification))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"Rtable\">\n <thead>\n <tr class=\"center\">\n  <th>&nbsp;</th>\n  <th colspan=\"6\">taxonomic.classification</th>\n</tr>\n <tr class=\"center\">\n  <th>clusters</th>\n  <th>Actinobacteria</th>\n  <th>Bacteroidetes</th>\n  <th>Firmicutes</th>\n  <th>Proteobacteria</th>\n  <th>Tenericutes</th>\n  <th>Verrucomicrobia</th>\n</tr>\n </thead>\n <tbody>\n <tr class=\"center\">\n  <th class=\"left\">1</th>\n  <td>2</td>\n  <td>27</td>\n  <td>76</td>\n  <td>13</td>\n  <td>1</td>\n  <td>1</td>\n</tr>\n <tr class=\"center\">\n  <th class=\"left\">2</th>\n  <td>0</td>\n  <td> 0</td>\n  <td> 0</td>\n  <td> 7</td>\n  <td>0</td>\n  <td>0</td>\n</tr>\n </tbody>\n </table>\n\n`````\n:::\n:::\n\n\nAdjusted Rand indices are not calculated for comparisons as the unitary weights\nin the convex clustering problem can be suboptimal. The abundance of OTUs\nbelonging to cluster $1$, mainly composed of Bacteroidetes and Firmicutes phyla,\nis seemingly dependent on the abundance of OTUS in cluster $2$, i.e.,\nProteobacteria phylum.\n\n## Application to methylation and transcriptomic genotypes in poplar  \n\nNext, we investigate interactions between European poplar genotypes for\ntranscriptomic and DNA methylation data extracted from the Evolutionary and\nfunctional impact of EPIgenetic variation in forest TREEs project [EPITREE,\n@maury2019epigenetics]. The analysis was purposefully applied to the samples and\nnot the genes in order to highlight the MGLasso clustering performance and show\nsome potential relationships between DNA methylation and gene expression levels\nfor some genotypes.\n\n<!-- Classic correlation approaches can lead to spurious relationships between\nvariables. Through the gaussian graphical framework of MGLasso, one can focus on\nthe conditional dependency structure which gets rid of confusion effects. We\nrefer to @akalin2020computational for a broader definition of the central dogma\nof molecular biology (DNA-RNA-proteins). -->\n\nPoplar (_Populus_) is often used as a model tree for the study of drought\nresponse. Natural populations of black poplars (_Populus nigra_) have been\nplanted in common gardens in France, Italy, and Germany (see\n@fig-context-epitree) with control on some environmental variables such as water\navailability [@sow2018narrow]. The poplar has economic importance and is one of\nthe most endangered species as a result of global climate change. The drought\nresponse can be studied via DNA methylation, which is a necessary process in\nplant development and response to environmental variations\n[@amaral2020advances]. It consists of the addition of a Methyl group to a\ncytosine (C) in the genome and occurs in three contexts (CG, CHG, and CHH, where\nH $\\in \\{ A, C, T\\}$). Methylation can be measured on two regions of the gene.\nMethylation in promoters is linked to gene silencing, and methylation in the\nbody of the gene can be related to tissue-specific expression or alternative\nsplicing [@sow2019role].\n\n<!-- Epigenetic is the study of heritable changes which are not the result of a\nmodification in the DNA sequence [@plomion2016forest]. Epigenetic marks in\nforest trees can be studied via  -->\n\n::: {#fig-context-epitree layout-ncol=2}\n\n![Black poplar (C. Fischer Wikimedia)](./figures/peuplier-noir-Christian-Fischer.jpeg)\n\n![Map of genotypes](./figures/carte-genotypes.png)\n:::\n\nThe collected DNA methylation and expression data are counts data. Details on\nthe plant material and experimental design can be found in @sow2019role and\n@chateigner2020gene. The transcriptomic data were measured via RNA-Seq and\nnormalized using Trimmed Mean of M-Values combined with a Best linear unbiased\npredictor (BLUP) correction as described in @chateigner2020gene. The methylation\ndata were measured through whole-genome bisulfite sequencing (WGBS) and are\nnormalized via the read per density approach then passed to a logarithm function\n$log_2(x+1)$ with $x \\in \\mathbb R$. For each one of the $10$\npopulations (see @fig-context-epitree), DNA methylation in CG, CHG, and CHH\ncontexts for promoters and gene-body and RNA sequencing data are observed on\ngenotypes. A mean measure is computed from two replicates per population. The\nanalysis has been restricted to a set of $151$ target genes which explains the\nmost variability in the omics data and the subsequent number of samples from\ndifferent omic variables, which is $70.$\n\nThe MGLasso model is fitted with fusion penalty values chosen in $[0, 30.94]$\nand a Lasso penalty $\\lambda_1$ parameter chosen via the StARS approach with\nthreshold $0.05$. In the resulting clustering path (see\n@fig-clusterpath-poplar), we can identify three distinct and coherent clusters,\nwhich are samples corresponding to gene expression genotypes, gene-body\nmethylation samples, and gene promoter samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmglasso_genot <-\n  readRDS(paste0(path_data, \"mgl_epit_sparse_geno.rds\"))\nepit_sparse <- readRDS(paste0(path_data, \"epit-spca-select.rds\"))\n\n# Shorten columns' names\n# To do: add colors to cluster path for known groups\nnames_epit <- epit_sparse %>% colnames()\n\ncut_names <- names_epit %>%\n  sapply(function(x)\n    gsub(\"log2_rpd.\", \"\", x)) %>%\n  sapply(function(x)\n    gsub(\"new_\", \"\", x)) %>%\n  as.character()\n\n####\norder_omics <- c(\n  grep(\"exp\", cut_names),\n  grep(\"gbM.CG\", cut_names),\n  grep(\"gbM.CHG\", cut_names),\n  grep(\"gbM.CHH\", cut_names),\n  grep(\"prom.CG\", cut_names),\n  grep(\"prom.CHG\", cut_names),\n  grep(\"prom.CHH\", cut_names))\n\ncol_leaves <- as.factor(rep(rainbow_hcl(7, c=90, l=50), each = 10))\n\ncol_leaves <- col_leaves[order(order_omics)]\n\nlevels(col_leaves) <- list(\"RNA-Seq\" = \"#0093A9\",\n                           \"CpG-Body\" = \"#00944F\",\n                           \"CHG-Body\" = \"#4473D7\",\n                           \"CHH-Body\" = \"#5D8400\",\n                           \"CpG-Promoter\" = \"#A86B00\",\n                           \"CHG-Promoter\" = \"#C03FBE\",\n                           \"CHH-Promoter\" = \"#CC476B\")\n####\n\nplot_clusterpath <- function(X, mglasso_res, colnames_ = NULL, max.overlaps, cut_k_vars = 5, colors_) {\n  ## Initialisations\n  p <- ncol(X)\n  df.paths <- data.frame(x=c(),y=c(), group=c())\n  nlevel <- length(mglasso_res)\n\n  ## Principal component analysis\n  svdX <- svd(X)                ## singular value decomposition\n  pc <- svdX$u[,1:2,drop=FALSE] ## singular vectors\n\n  for (j in cut_k_vars:nlevel) {\n    Beta <- mglasso_res[[j]]$selected_Theta\n    Xpred <- sapply(1:p, function(i){X %*% Beta[i,]})\n    pcs <- t(pc)%*%Xpred\n    x <- pcs[1,]\n    y <- pcs[2,]\n    df <- data.frame(x=pcs[1,], y=pcs[2,], group=1:p, Data = colors_)\n    df.paths <- rbind(df.paths,df)\n  }\n\n  # X_data <- as.data.frame(t(X) %*% pc) ## PCA projections (scores)\n  X_data <- df.paths[1:p,]\n  #colnames(X_data) <- c(\"x\", \"y\")\n  ifelse(is.null(colnames_),\n         X_data$Name <- colnames(X),\n         X_data$Name <- colnames_)\n  data_plot <- ggplot(data = df.paths, aes(x = x, y = y))\n  data_plot <-\n    data_plot + geom_path(aes(group = group,  colour = Data), alpha = 0.5)\n  data_plot <-\n    data_plot + geom_text_repel(data = X_data,\n                                aes(x = x, y = y, label = Name),\n                                max.overlaps = max.overlaps)\n  data_plot <-\n    data_plot + geom_point(data = X_data, aes(x = x, y = y, colour = Data), size = 1.5)\n  data_plot <-\n    data_plot + xlab('Principal Component 1') + ylab('Principal Component 2')\n  data_plot + theme_bw()\n}\n\nplot_clusterpath(as.matrix(epit_sparse), mglasso_genot, cut_names, max.overlaps = 20, cut_k_vars = 1, colors_ = col_leaves)\n```\n\n::: {.cell-output-display}\n![Clustering path of solutions on DNA methylation and transcriptomic samples. The figure shows $3$ distinct clusters which correspond to omics data of different natures: transcriptomic (right), methylation on the promoter (bottom), and methylation on gene-body (top left).](published-202306-sanou-multiscale_glasso_files/figure-html/fig-clusterpath-poplar-1.svg){#fig-clusterpath-poplar}\n:::\n:::\n\n\nThe results of the MGLasso can also be represented in the expanded way where\nmeta-variables are not computed from clusters. In @fig-graphpath-poplar, a focus\nis put on the effect of the fusion penalty. Clusters partitions are not\npresented. The higher the fusion penalty, variables are encouraged to share the\nsame neighborhood structure. Note that an equivalent graph over meta-variables\ncan be computed after choosing a fusion threshold as in @fig-meta-graphs.\n\n\n::: {#fig-graphpath-poplar .cell layout-ncol=\"3\"}\n\n```{.r .cell-code}\n# Plot adjacency matrices for some levels \n# Selection based on network interpretability  \n#' symmetrize matrix of regression vectors pxp\nMatrix::image(\n  adj_mat(mglasso_genot$`1`$selected_Theta[order_omics, order_omics]),\n  sub = \"\", xlab = \"\", ylab = \"\"\n)\nMatrix::image(\n  adj_mat(mglasso_genot$`2`$selected_Theta[order_omics, order_omics]),\n  sub = \"\", xlab = \"\", ylab = \"\"\n)\nMatrix::image(\n  adj_mat(mglasso_genot$`3`$selected_Theta[order_omics, order_omics]),\n  sub = \"\", xlab = \"\", ylab = \"\"\n)\nMatrix::image(\n  adj_mat(mglasso_genot$`4`$selected_Theta[order_omics, order_omics]),\n  sub = \"\", xlab = \"\", ylab = \"\"\n)\nMatrix::image(\n  adj_mat(mglasso_genot$`20`$selected_Theta[order_omics, order_omics]),\n  sub = \"\", xlab = \"\", ylab = \"\"\n)\n```\n\n::: {.cell-output-display}\n![Full graph with $\\lambda_2$ = 0](published-202306-sanou-multiscale_glasso_files/figure-html/fig-graphpath-poplar-1.svg){#fig-graphpath-poplar-1}\n:::\n\n::: {.cell-output-display}\n![Full graph with $\\lambda_2$ = 1.63](published-202306-sanou-multiscale_glasso_files/figure-html/fig-graphpath-poplar-2.svg){#fig-graphpath-poplar-2}\n:::\n\n::: {.cell-output-display}\n![Full graph with $\\lambda_2$ = 3.26](published-202306-sanou-multiscale_glasso_files/figure-html/fig-graphpath-poplar-3.svg){#fig-graphpath-poplar-3}\n:::\n\n::: {.cell-output-display}\n![Full graph with $\\lambda_2$ = 4.89](published-202306-sanou-multiscale_glasso_files/figure-html/fig-graphpath-poplar-4.svg){#fig-graphpath-poplar-4}\n:::\n\n::: {.cell-output-display}\n![Full graph with $\\lambda_2$ = 30.94](published-202306-sanou-multiscale_glasso_files/figure-html/fig-graphpath-poplar-5.svg){#fig-graphpath-poplar-5}\n:::\n\nAdjacency matrices for different fusion penalty parameters. The first graph shows the inferred network when no fusion penalty is added to the model. In that graph, the first block of size $10 \\times 10$ variables corresponds to RNA-Seq samples. The second sparser block of size $30 \\times 30$ corresponds to gene-body DNA methylation data in the three methylation contexts. The last sparse block of the same size corresponds to promoter methylation. The edge bands suggest a relationship between DNA methylation measurements that belong to the same context. For example, the Loire methylation sample in the CpG context is likely related to the Loire samples in the CHG and CHH contexts. The graphs also suggest some relationships between expression and methylation for some natural populations. As the merging penalty increases, the blocks corresponding to the three methylation contexts merge first, then follow the upper left block corresponding to the expression data. For $\\lambda_2 = 30.94,$ all natural populations merge into a single cluster and complete graph.\n:::\n\n\n# Conclusion\n\nWe proposed a new technique that combines Gaussian Graphical Model inference and\nhierarchical clustering called MGLasso. The method proceeds via convex\noptimization and minimizes the neighborhood selection objective penalized by a\nhybrid regularization combining a sparsity-inducing norm and a convex clustering\npenalty. We developed a complete numerical scheme to apply MGLasso in practice,\nwith an optimization algorithm based on CONESTA and a model selection procedure.\nOur simulations results over synthetic and real datasets showed that MGLasso can\nperform better than GLasso in network support recovery in the presence of groups\nof correlated variables, and we illustrated the method with the analysis of\nmicrobial associations data and methylation mixed with transcriptomic data. \nThe present work paves the way for future\nimprovements: first, by incorporating prior knowledge through more flexible\nweighted regularization; second, by studying the theoretical properties of the\nmethod in terms of statistical guarantees for the MGLasso estimator. Moreover,\nthe node-wise regression approach on which our method is based can be extended\nto a broader family of non-Gaussian distributions belonging to the exponential\nfamily as outlined by @yang2012graphical.  Our MGLasso approach can be easily\nextended to non-Gaussian distributions belonging to the exponential family and\nmixed graphical models. \n\n# Appendix {.appendix .unnumbered}  \nThe scripts to reproduce the simulations are available at <https://github.com/computorg/published-202306-sanou-multiscale_glasso/tree/main/simulation-experiments>.\n\n# Acknowledgments {.appendix .unnumbered}  \nThe authors would like to thank the Editors and referees for comments that led to substantial improvements in the manuscript.\n\n# Session information {.appendix .unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] igraph_1.3.5      phyloseq_1.40.0   ggrepel_0.9.3     colorspace_2.1-0 \n[5] SpiecEasi_1.1.2   ghibli_0.3.3.9000 ggplot2_3.4.2     mglasso_0.1.3    \n\nloaded via a namespace (and not attached):\n  [1] nlme_3.1-160           simone_1.0-4           bitops_1.0-7          \n  [4] blockmodels_1.1.5      httr_1.4.6             GenomeInfoDb_1.32.4   \n  [7] tools_4.2.1            vegan_2.6-4            utf8_1.2.3            \n [10] R6_2.5.1               mgcv_1.8-41            DBI_1.1.3             \n [13] BiocGenerics_0.42.0    permute_0.9-7          rhdf5filters_1.8.0    \n [16] ade4_1.7-20            withr_2.5.0            tidyselect_1.2.0      \n [19] Exact_3.2              compiler_4.2.1         glmnet_4.1-6          \n [22] cli_3.6.1              Biobase_2.56.0         expm_0.999-7          \n [25] prismatic_1.1.1.9000   labeling_0.4.2         scales_1.2.1          \n [28] mvtnorm_1.1-3          tables_0.9.10          proxy_0.4-27          \n [31] stringr_1.5.0          digest_0.6.30          rmarkdown_2.21        \n [34] XVector_0.36.0         pkgconfig_2.0.3        htmltools_0.5.4       \n [37] fastmap_1.1.0          htmlwidgets_1.5.4      rlang_1.1.1           \n [40] readxl_1.4.2           rstudioapi_0.14        huge_1.3.5            \n [43] VGAM_1.1-7             shape_1.4.6            farver_2.1.1          \n [46] generics_0.1.3         jsonlite_1.8.4         dplyr_1.0.10          \n [49] RCurl_1.98-1.9         magrittr_2.0.3         GenomeInfoDbData_1.2.8\n [52] biomformat_1.24.0      Matrix_1.5-3           Rhdf5lib_1.18.2       \n [55] Rcpp_1.0.10            DescTools_0.99.49      munsell_0.5.0         \n [58] S4Vectors_0.34.0       fansi_1.0.4            ape_5.6-2             \n [61] reticulate_1.28        lifecycle_1.0.3        stringi_1.7.8         \n [64] yaml_2.3.6             MASS_7.3-58.1          rootSolve_1.8.2.3     \n [67] zlibbioc_1.42.0        rhdf5_2.40.0           plyr_1.8.8            \n [70] grid_4.2.1             parallel_4.2.1         crayon_1.5.2          \n [73] lmom_2.9               lattice_0.20-45        Biostrings_2.64.1     \n [76] splines_4.2.1          multtest_2.52.0        capushe_1.1.1         \n [79] knitr_1.41             pillar_1.9.0           boot_1.3-28.1         \n [82] pulsar_0.3.8           gld_2.6.6              reshape2_1.4.4        \n [85] codetools_0.2-18       stats4_4.2.1           glue_1.6.2            \n [88] evaluate_0.18          data.table_1.14.8      png_0.1-8             \n [91] vctrs_0.6.2            foreach_1.5.2          cellranger_1.1.0      \n [94] gtable_0.3.3           assertthat_0.2.1       xfun_0.39             \n [97] e1071_1.7-13           class_7.3-20           survival_3.4-0        \n[100] tibble_3.2.1           iterators_1.0.14       IRanges_2.30.1        \n[103] cluster_2.1.4         \n```\n:::\n:::\n",
    "supporting": [
      "published-202306-sanou-multiscale_glasso_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}